{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # Must be first!\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import (\n",
    "    ReduceLROnPlateau,\n",
    "    CosineAnnealingLR,\n",
    "    CyclicLR,\n",
    "    OneCycleLR,\n",
    "    LambdaLR\n",
    ")\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import polars as pl\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "import optuna\n",
    "\n",
    "# Set seeds and deterministic flags\n",
    "random.seed(69)\n",
    "np.random.seed(69)\n",
    "torch.manual_seed(69)\n",
    "torch.cuda.manual_seed(69)\n",
    "torch.use_deterministic_algorithms(True)  # Enable full determinism\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['event_id',\n",
       " 'orig_marker',\n",
       " 'time',\n",
       " 'Fp1',\n",
       " 'Fpz',\n",
       " 'Fp2',\n",
       " 'F7',\n",
       " 'F3',\n",
       " 'Fz',\n",
       " 'F4',\n",
       " 'F8',\n",
       " 'FC5',\n",
       " 'FC1',\n",
       " 'FC2',\n",
       " 'FC6',\n",
       " 'M1',\n",
       " 'T7',\n",
       " 'C3',\n",
       " 'Cz',\n",
       " 'C4',\n",
       " 'T8',\n",
       " 'M2',\n",
       " 'CP5',\n",
       " 'CP1',\n",
       " 'CP2',\n",
       " 'CP6',\n",
       " 'P7',\n",
       " 'P3',\n",
       " 'Pz',\n",
       " 'P4',\n",
       " 'P8',\n",
       " 'POz',\n",
       " 'O1',\n",
       " 'O2',\n",
       " 'AF7',\n",
       " 'AF3',\n",
       " 'AF4',\n",
       " 'AF8',\n",
       " 'F5',\n",
       " 'F1',\n",
       " 'F2',\n",
       " 'F6',\n",
       " 'FC3',\n",
       " 'FCz',\n",
       " 'FC4',\n",
       " 'C5',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C6',\n",
       " 'CP3',\n",
       " 'CP4',\n",
       " 'P5',\n",
       " 'P1',\n",
       " 'P2',\n",
       " 'P6',\n",
       " 'PO5',\n",
       " 'PO3',\n",
       " 'PO4',\n",
       " 'PO6',\n",
       " 'FT7',\n",
       " 'FT8',\n",
       " 'TP7',\n",
       " 'TP8',\n",
       " 'PO7',\n",
       " 'PO8',\n",
       " 'Oz',\n",
       " 'marker',\n",
       " 'prev_marker']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.read_parquet('/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet')\\\n",
    "    .columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class EEGMobileNet(nn.Module):\n",
    "    def __init__(self, in_channels=64, num_classes=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # Initial Conv\n",
    "            nn.Conv1d(in_channels, 32, kernel_size=15, stride=2, padding=7),\n",
    "            nn.BatchNorm1d(32, track_running_stats=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),  # ← Insert dropout here\n",
    "\n",
    "            # Depthwise\n",
    "            nn.Conv1d(32, 32, kernel_size=3, stride=1, padding=1, groups=32),\n",
    "            nn.BatchNorm1d(32, track_running_stats=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Pointwise\n",
    "            nn.Conv1d(32, 64, kernel_size=1),\n",
    "            nn.BatchNorm1d(64, track_running_stats=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),  # ← Insert dropout here\n",
    "\n",
    "            # Another Depthwise Separable block\n",
    "            nn.Conv1d(64, 64, kernel_size=3, stride=2, padding=1, groups=64),\n",
    "            nn.BatchNorm1d(64, track_running_stats=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(64, 128, kernel_size=1),\n",
    "            nn.BatchNorm1d(128, track_running_stats=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),  # ← Insert dropout here\n",
    "\n",
    "            # Global Average Pool\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # your original transpose\n",
    "        return self.model(x).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tslearn.neighbors import KNeighborsTimeSeries\n",
    "from numba import njit\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def fast_interpolate(original, neighbor, alpha):\n",
    "    \"\"\"Numba-accelerated linear interpolation for numeric columns.\"\"\"\n",
    "    return (1 - alpha) * original + alpha * neighbor\n",
    "\n",
    "class TSMOTE:\n",
    "    def __init__(self, \n",
    "                 n_neighbors=3, \n",
    "                 time_slices=10, \n",
    "                 bool_cols=None):\n",
    "        \"\"\"\n",
    "        :param n_neighbors: Number of neighbors for KNN\n",
    "        :param time_slices: Number of slices to split each time series\n",
    "        :param bool_cols:   List (or array) of indices for boolean columns\n",
    "        \"\"\"\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.time_slices = time_slices\n",
    "        self.slice_size = None  # will be set after seeing data\n",
    "        self.bool_cols = bool_cols if bool_cols is not None else []\n",
    "        # numeric_cols will be determined at fit-time, after we see total channels.\n",
    "\n",
    "    def _slice_time_series(self, X):\n",
    "        \"\"\"Split into time slices: (N, 2000, ch) -> (N, time_slices, slice_size, ch).\"\"\"\n",
    "        return X.reshape(X.shape[0], self.time_slices, self.slice_size, X.shape[2])\n",
    "\n",
    "    def _generate_synthetic(self, minority_samples, bool_probs):\n",
    "        \"\"\"\n",
    "        Generate full-length synthetic samples.\n",
    "        :param minority_samples: Array of shape (N_minority, 2000, ch)\n",
    "        :param bool_probs:       Dict mapping boolean column index -> probability of 1\n",
    "        \"\"\"\n",
    "        # slice_size was computed earlier in fit_resample.\n",
    "        sliced_data = self._slice_time_series(minority_samples)  # shape (N, slices, slice_size, ch)\n",
    "        syn_samples = []\n",
    "\n",
    "        # We'll figure out numeric_cols from total channels\n",
    "        all_cols = list(range(minority_samples.shape[2]))\n",
    "        numeric_cols = [c for c in all_cols if c not in self.bool_cols]\n",
    "\n",
    "        for sample_idx in tqdm(range(sliced_data.shape[0]), desc=\"Generating synthetic\"):\n",
    "            synthetic_slices = []\n",
    "\n",
    "            # For each time slice\n",
    "            for slice_idx in range(self.time_slices):\n",
    "                # Split data into included (numeric) columns vs. excluded (boolean) columns\n",
    "                slice_incl = sliced_data[:, slice_idx, :, :][:, :, numeric_cols]  # (N, slice_size, #numeric)\n",
    "                slice_excl = sliced_data[:, slice_idx, :, :][:, :, self.bool_cols] # (N, slice_size, #bool)\n",
    "\n",
    "                # Fit KNN on included (numeric) data only\n",
    "                knn = KNeighborsTimeSeries(n_neighbors=self.n_neighbors, metric='dtw')\n",
    "                knn.fit(slice_incl)  # each entry is shape (slice_size, #numeric)\n",
    "\n",
    "                # The sample's numeric slice\n",
    "                original_slice_incl = slice_incl[sample_idx]  # shape (slice_size, #numeric)\n",
    "\n",
    "                # Find neighbors for this numeric slice\n",
    "                neighbors = knn.kneighbors(original_slice_incl[np.newaxis], \n",
    "                                           return_distance=False)[0]\n",
    "                neighbor_idx = np.random.choice(neighbors)\n",
    "\n",
    "                neighbor_slice_incl = slice_incl[neighbor_idx]  # shape (slice_size, #numeric)\n",
    "\n",
    "                # Interpolate for numeric columns\n",
    "                alpha = np.random.uniform(0.2, 0.8)\n",
    "                # Using fast_interpolate or direct calculation:\n",
    "                synthetic_slice_incl = fast_interpolate(original_slice_incl, \n",
    "                                                        neighbor_slice_incl, \n",
    "                                                        alpha)\n",
    "\n",
    "                # For boolean columns: sample from distribution\n",
    "                # We'll create an array of shape (slice_size, #bool)\n",
    "                # For each boolean column index b, pick 0/1 based on bool_probs[b].\n",
    "                # If you want different logic (like \"choose original or neighbor\"?),\n",
    "                # you can adapt here.\n",
    "                n_bool_cols = len(self.bool_cols)\n",
    "                synthetic_slice_excl = np.zeros((self.slice_size, n_bool_cols), dtype=np.float32)\n",
    "\n",
    "                for col_idx_in_boolarray, bcol in enumerate(self.bool_cols):\n",
    "                    p = bool_probs[bcol]  # Probability of 1 for that bool column\n",
    "                    # Sample 0/1 for each time step in the slice\n",
    "                    synthetic_slice_excl[:, col_idx_in_boolarray] = \\\n",
    "                        np.random.binomial(n=1, p=p, size=self.slice_size)\n",
    "                \n",
    "                # Combine numeric + boolean columns back in correct order\n",
    "                # We have numeric_cols in synthetic_slice_incl\n",
    "                # We have bool_cols in synthetic_slice_excl\n",
    "                # We need to re-insert them into shape: (slice_size, total_channels)\n",
    "                synthetic_slice = np.zeros((self.slice_size, len(all_cols)), dtype=np.float32)\n",
    "\n",
    "                # Place numeric columns\n",
    "                synthetic_slice[:, numeric_cols] = synthetic_slice_incl\n",
    "                # Place boolean columns\n",
    "                synthetic_slice[:, self.bool_cols] = synthetic_slice_excl\n",
    "\n",
    "                synthetic_slices.append(synthetic_slice)\n",
    "\n",
    "            # Concatenate slices into a full time series (2000, ch)\n",
    "            full_series = np.concatenate(synthetic_slices, axis=0)\n",
    "            syn_samples.append(full_series)\n",
    "\n",
    "        return np.array(syn_samples)\n",
    "\n",
    "    def fit_resample(self, X, y):\n",
    "        \"\"\"\n",
    "        Perform TSMOTE oversampling.\n",
    "        :param X: shape (N, 2000, ch)\n",
    "        :param y: shape (N,)\n",
    "        \"\"\"\n",
    "        y_int = y.astype(int)\n",
    "        class_counts = np.bincount(y_int)\n",
    "        minority_class = np.argmin(class_counts)\n",
    "        majority_class = 1 - minority_class\n",
    "\n",
    "        n_needed = class_counts[majority_class] - class_counts[minority_class]\n",
    "        if n_needed <= 0:\n",
    "            return X, y  # no oversampling needed\n",
    "\n",
    "        # Suppose X has shape (N, 2000, ch). We'll assume 2000 is consistent with time_slices * slice_size.\n",
    "        # We'll deduce slice_size\n",
    "        self.slice_size = X.shape[1] // self.time_slices  # e.g. 2000/10=200\n",
    "\n",
    "        # Get only minority samples\n",
    "        minority_samples = X[y_int == minority_class]\n",
    "\n",
    "        # ----- Compute distribution of booleans in the minority data ------\n",
    "        # For each bool column b, compute fraction of 1s across the entire minority set\n",
    "        bool_probs = {}\n",
    "        if len(self.bool_cols) > 0:\n",
    "            # shape is (N_minority, 2000, ch)\n",
    "            # We'll flatten across time for each column to get overall fraction\n",
    "            for bcol in self.bool_cols:\n",
    "                col_values = minority_samples[:, :, bcol].flatten()\n",
    "                p = col_values.mean()  # fraction of 1's\n",
    "                bool_probs[bcol] = p\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        synthetic = self._generate_synthetic(minority_samples, bool_probs)\n",
    "\n",
    "        # Ensure matching dimensions\n",
    "        assert X.shape[1:] == synthetic.shape[1:], \\\n",
    "            f\"Dimension mismatch: Original {X.shape[1:]}, Synthetic {synthetic.shape[1:]}\"\n",
    "\n",
    "        # Use only as many synthetic as needed\n",
    "        synthetic = synthetic[:n_needed]\n",
    "\n",
    "        # Concatenate\n",
    "        X_resampled = np.concatenate([X, synthetic], axis=0)\n",
    "        y_resampled = np.concatenate([y, [minority_class] * len(synthetic)], axis=0)\n",
    "        return X_resampled, y_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# Enhanced Dataset Class with Proper Encapsulation\n",
    "#============================================================\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, source, max_length=2000):\n",
    "        self.df = pl.read_parquet(source) if isinstance(source, str) else source\n",
    "        if 'orig_marker' in self.df.columns:\n",
    "            self.df = self.df.drop('orig_marker')\n",
    "        \n",
    "        # label_map = {\"Left\": 0, \"Right\": 1}  \n",
    "        # self.df = self.df.with_columns([\n",
    "        #     pl.col(\"marker\").replace(label_map).cast(pl.Int32).alias(\"marker\"),\n",
    "        #     pl.col(\"prev_marker\").replace(label_map).cast(pl.Int32).alias(\"prev_marker\")\n",
    "        # ])\n",
    "        \n",
    "        self.df = self.df.with_columns([\n",
    "            pl.col(\"marker\")\n",
    "            .cast(pl.Utf8)\n",
    "            .str.replace_all(\"Left\", \"0\")      # replace exact string \"Left\" with \"0\"\n",
    "            .str.replace_all(\"Right\", \"1\")     # replace exact string \"Right\" with \"1\"\n",
    "            .cast(pl.Int32)                      # now cast the string \"0\"/\"1\" -> int\n",
    "            .alias(\"marker\"),\n",
    "            \n",
    "            pl.col(\"prev_marker\")\n",
    "            .cast(pl.Utf8)\n",
    "            .str.replace_all(\"Left\", \"0\")\n",
    "            .str.replace_all(\"Right\", \"1\")\n",
    "            .cast(pl.Int32)\n",
    "            .alias(\"prev_marker\"),\n",
    "        ])\n",
    "        \n",
    "        self.event_ids = self.df['event_id'].unique().to_list()\n",
    "        self.max_length = max_length\n",
    "        # Keep time for sorting but exclude from features\n",
    "        self.feature_cols = [c for c in self.df.columns \n",
    "                           if c not in {'event_id', 'marker', 'time'}]\n",
    "        \n",
    "        print(\"Precomputing samples...\")\n",
    "        self._precompute_samples()\n",
    "        print(\"Computing class weights...\")\n",
    "        self._class_weights = self.compute_class_weights()\n",
    "    \n",
    "    @property\n",
    "    def class_weights(self):\n",
    "        # Expose the computed weights as a property.\n",
    "        return self._class_weights \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.event_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "    \n",
    "    def _precompute_samples(self):\n",
    "        self.samples = []\n",
    "        for event_id in tqdm(self.event_ids, desc='precomputing_samples'):\n",
    "            # Sort by time within each event!\n",
    "            event_data = self.df.filter(pl.col(\"event_id\") == event_id).sort(\"time\")\n",
    "            features = torch.tensor(\n",
    "                event_data.select(self.feature_cols).to_numpy(),\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "            features = self._pad_sequence(features)\n",
    "            \n",
    "            label = event_data['marker'][0]\n",
    "            self.samples.append((\n",
    "                torch.tensor(label, dtype=torch.float32), \n",
    "                features\n",
    "            ))\n",
    "    \n",
    "    def compute_class_weights(self):\n",
    "        \"\"\"\n",
    "        Compute inverse frequency weights based on the 'marker' column.\n",
    "        Assumes markers are \"Stimulus/A\" and \"Stimulus/P\".\n",
    "        \"\"\"\n",
    "        # Get unique combinations of event_id and marker.\n",
    "        unique_events = self.df.select([\"event_id\", \"marker\"]).unique()\n",
    "        \n",
    "        # Use value_counts on the \"marker\" column.\n",
    "        counts_df = unique_events[\"marker\"].value_counts()\n",
    "\n",
    "        # We'll use 'values' if it exists, otherwise 'marker'.\n",
    "        d = { (row.get(\"values\") or row.get(\"marker\")): row[\"count\"] \n",
    "            for row in counts_df.to_dicts() }\n",
    "        \n",
    "        weight_L = 1.0 / d.get(0, 1)\n",
    "        weight_R = 1.0 / d.get(1, 1)\n",
    "        return {\"Left\": weight_L, \"Right\": weight_R}\n",
    "   \n",
    "    def split_dataset(self, ratios=(0.7, 0.15, 0.15), seed=None):\n",
    "        \"\"\"\n",
    "        Splits the dataset into three EEGDataset instances for train, val, and test.\n",
    "        This method shuffles the event_ids and then partitions them based on the given ratios.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Copy and shuffle the event_ids\n",
    "        event_ids = self.event_ids.copy()\n",
    "        np.random.shuffle(event_ids)\n",
    "        total = len(event_ids)\n",
    "        \n",
    "        n_train = int(ratios[0] * total)\n",
    "        n_val   = int(ratios[1] * total)\n",
    "        \n",
    "        train_ids = event_ids[:n_train]\n",
    "        val_ids   = event_ids[n_train:n_train+n_val]\n",
    "        test_ids  = event_ids[n_train+n_val:]\n",
    "        \n",
    "        # Filter self.df for the selected event_ids\n",
    "        train_df = self.df.filter(pl.col(\"event_id\").is_in(train_ids))\n",
    "        val_df   = self.df.filter(pl.col(\"event_id\").is_in(val_ids))\n",
    "        test_df  = self.df.filter(pl.col(\"event_id\").is_in(test_ids))\n",
    "        \n",
    "        # Create new EEGDataset instances using the filtered data\n",
    "        train_set = EEGDataset(train_df, self.max_length)\n",
    "        val_set   = EEGDataset(val_df, self.max_length)\n",
    "        test_set  = EEGDataset(test_df, self.max_length)\n",
    "        \n",
    "        return train_set, val_set, test_set\n",
    "\n",
    "    def _pad_sequence(self, tensor):\n",
    "        # Pre-allocate tensor for maximum efficiency\n",
    "        padded = torch.zeros((self.max_length, tensor.size(1)), dtype=tensor.dtype)\n",
    "        length = min(tensor.size(0), self.max_length)\n",
    "        padded[:length] = tensor[:length]\n",
    "        return padded\n",
    "    \n",
    "    def rebalance_by_tsmote(self):\n",
    "        \"\"\"TSMOTE implementation for temporal EEG data\"\"\"\n",
    "        # Extract time-ordered features as 3D array (samples, timesteps, features)\n",
    "        X = np.stack([features.numpy() for _, features in self.samples])\n",
    "        y = np.array([label.item() for label, _ in self.samples])\n",
    "        \n",
    "        # Apply TSMOTE with temporal awareness\n",
    "        \n",
    "        \n",
    "        # Find the index of 'prev_marker' in the feature columns\n",
    "        prev_marker_idx = self.feature_cols.index('prev_marker')\n",
    "        \n",
    "        # Apply TSMOTE with the correct boolean column index\n",
    "        tsmote = TSMOTE(bool_cols=[prev_marker_idx])\n",
    "        X_res, y_res = tsmote.fit_resample(X, y)\n",
    "\n",
    "        # Generate synthetic temporal events\n",
    "        new_events = []\n",
    "        new_event_id = self.df['event_id'].max() + 1\n",
    "        time_base = np.arange(self.max_length)\n",
    "        original_schema = self.df.schema\n",
    "\n",
    "        # Create dtype conversion map\n",
    "        dtype_map = {\n",
    "            pl.Float64: np.float64,\n",
    "            pl.Float32: np.float32,\n",
    "            pl.Int64: np.int64,\n",
    "            pl.Int32: np.int32,\n",
    "            pl.Utf8: str,\n",
    "        }\n",
    "\n",
    "        # Process synthetic samples (original samples come first in X_res)\n",
    "        for features_3d, label in zip(X_res[len(self.samples):], y_res[len(self.samples):]):\n",
    "            event_data = {}\n",
    "\n",
    "            # Ensure columns are added in the original DataFrame's order\n",
    "            for col in self.df.columns:\n",
    "                if col == 'event_id':\n",
    "                    event_data[col] = [new_event_id] * self.max_length\n",
    "                elif col == 'marker':\n",
    "                    event_data[col] = [int(label)] * self.max_length  # Ensure label is integer\n",
    "                elif col == 'time':\n",
    "                    event_data[col] = time_base.copy().astype(np.int32)  # Match original time type\n",
    "                else:\n",
    "                    # Feature columns (excluding event_id, marker, time)\n",
    "                    if col not in self.feature_cols:\n",
    "                        continue  # Shouldn't happen as feature_cols covers all else\n",
    "                    col_idx = self.feature_cols.index(col)\n",
    "                    col_data = features_3d[:, col_idx]\n",
    "                    schema_type = original_schema[col]\n",
    "\n",
    "                    # Handle data types\n",
    "                    if isinstance(schema_type, pl.List):\n",
    "                        base_type = schema_type.inner\n",
    "                        target_type = dtype_map.get(type(base_type), np.float64)\n",
    "                    else:\n",
    "                        target_type = dtype_map.get(type(schema_type), np.float64)\n",
    "                    \n",
    "                    col_data = col_data.astype(target_type)\n",
    "                    \n",
    "                    # Maintain integer precision for Int columns (e.g., prev_marker)\n",
    "                    if schema_type in (pl.Int64, pl.Int32):\n",
    "                        col_data = np.round(col_data).astype(int)\n",
    "                    \n",
    "                    event_data[col] = col_data\n",
    "\n",
    "            # Create DataFrame with strict schema adherence\n",
    "            event_df = pl.DataFrame(event_data).cast(original_schema)\n",
    "            new_events.append(event_df)\n",
    "            new_event_id += 1\n",
    "\n",
    "        # Update dataset with synthetic temporal events\n",
    "        self.df = pl.concat([self.df, *new_events])\n",
    "        self.event_ids = self.df['event_id'].unique().to_list()\n",
    "        self._precompute_samples()\n",
    "        self._class_weights = self.compute_class_weights()\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_optimizer(model, optimizer_config):\n",
    "    \"\"\"\n",
    "    Create and return an optimizer based on optimizer_config.\n",
    "    \"\"\"\n",
    "    opt_type = optimizer_config.get('type', 'AdamW')\n",
    "    lr = optimizer_config.get('lr', 1e-3)\n",
    "    weight_decay = optimizer_config.get('weight_decay', 1e-2)\n",
    "    \n",
    "    if opt_type == 'AdamW':\n",
    "        return optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif opt_type == 'SGD':\n",
    "        momentum = optimizer_config.get('momentum', 0.9)\n",
    "        return optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    elif opt_type == 'RMSProp':\n",
    "        alpha = optimizer_config.get('alpha', 0.99)\n",
    "        return optim.RMSprop(model.parameters(), lr=lr, alpha=alpha, weight_decay=weight_decay)\n",
    "    # Add more optimizers if needed\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer type: {opt_type}\")\n",
    "\n",
    "def create_scheduler(optimizer, scheduler_config, total_epochs=None, steps_per_epoch=None):\n",
    "    \"\"\"\n",
    "    Create and return a scheduler based on scheduler_config.\n",
    "    Return (scheduler, requires_val_loss),\n",
    "    where requires_val_loss indicates if the scheduler needs validation loss (e.g. ReduceLROnPlateau).\n",
    "    \"\"\"\n",
    "    if not scheduler_config or scheduler_config.get('type') is None:\n",
    "        # No scheduler used\n",
    "        return None, False\n",
    "\n",
    "    sched_type = scheduler_config['type']\n",
    "\n",
    "    if sched_type == 'ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=scheduler_config.get('mode', 'min'),\n",
    "            factor=scheduler_config.get('factor', 0.1),\n",
    "            patience=scheduler_config.get('patience', 10),\n",
    "            threshold=scheduler_config.get('threshold', 0.0001),\n",
    "            cooldown=scheduler_config.get('cooldown', 0),\n",
    "            min_lr=scheduler_config.get('min_lr', 0),\n",
    "            # verbose=scheduler_config.get('verbose', False)\n",
    "        )\n",
    "        return scheduler, True\n",
    "\n",
    "    elif sched_type == 'CosineAnnealingLR':\n",
    "        T_max = scheduler_config.get('T_max', 10)\n",
    "        eta_min = scheduler_config.get('eta_min', 0)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)\n",
    "        return scheduler, False\n",
    "\n",
    "    elif sched_type == 'CyclicLR':\n",
    "        base_lr = scheduler_config.get('base_lr', 1e-3)\n",
    "        max_lr = scheduler_config.get('max_lr', 1e-2)\n",
    "        step_size_up = scheduler_config.get('step_size_up', 2000)\n",
    "        mode = scheduler_config.get('mode', 'triangular')\n",
    "        scheduler = CyclicLR(\n",
    "            optimizer,\n",
    "            base_lr=base_lr,\n",
    "            max_lr=max_lr,\n",
    "            step_size_up=step_size_up,\n",
    "            mode=mode\n",
    "        )\n",
    "        return scheduler, False\n",
    "\n",
    "    elif sched_type == 'OneCycleLR':\n",
    "        # OneCycleLR requires total_steps or (epochs * steps_per_epoch).\n",
    "        # If not specified in config, try to compute from total_epochs and steps_per_epoch.\n",
    "        max_lr = scheduler_config.get('max_lr', 1e-2)\n",
    "        if 'total_steps' in scheduler_config:\n",
    "            total_steps = scheduler_config['total_steps']\n",
    "        else:\n",
    "            if total_epochs is None or steps_per_epoch is None:\n",
    "                raise ValueError(\n",
    "                    \"OneCycleLR requires either 'total_steps' in config \"\n",
    "                    \"or (total_epochs and steps_per_epoch) arguments.\"\n",
    "                )\n",
    "            total_steps = total_epochs * steps_per_epoch\n",
    "\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=max_lr,\n",
    "            total_steps=total_steps,\n",
    "            pct_start=scheduler_config.get('pct_start', 0.3),\n",
    "            anneal_strategy=scheduler_config.get('anneal_strategy', 'cos'),\n",
    "            cycle_momentum=scheduler_config.get('cycle_momentum', True),\n",
    "            base_momentum=scheduler_config.get('base_momentum', 0.85),\n",
    "            max_momentum=scheduler_config.get('max_momentum', 0.95),\n",
    "            div_factor=scheduler_config.get('div_factor', 25.0),\n",
    "            final_div_factor=scheduler_config.get('final_div_factor', 1e4)\n",
    "        )\n",
    "        return scheduler, False\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported scheduler type: {sched_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for variable-length EEG feature sequences.\n",
    "\n",
    "    Each sample is expected to be a tuple (label, feature), where:\n",
    "    - label is a scalar tensor (or 1D tensor) representing the class/target.\n",
    "    - feature is a tensor of shape (seq_len, num_channels), where seq_len may vary.\n",
    "\n",
    "    This function stacks labels and pads features along the time dimension so that\n",
    "    all sequences in the batch have the same length.\n",
    "    \"\"\"\n",
    "    # Unzip the batch into labels and features\n",
    "    labels, features = zip(*batch)\n",
    "    \n",
    "    labels = torch.stack(labels)\n",
    "    padded_features = pad_sequence(features, batch_first=True)\n",
    "    \n",
    "    return labels, padded_features\n",
    "\n",
    "\n",
    "def train_model(config, train_set, train_loader, val_loader):\n",
    "    # -------------------- MODEL --------------------\n",
    "    model = EEGMobileNet(\n",
    "        in_channels=64,\n",
    "        num_classes=1,\n",
    "        dropout=config['dropout']\n",
    "    ).to(config['device'])\n",
    "    \n",
    "    # ------------------ LOSS FUNCTION ------------------\n",
    "    pos_weight = torch.tensor([\n",
    "        train_set.class_weights['Left'] / train_set.class_weights['Right']\n",
    "    ]).to(config['device'])\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(weight=pos_weight)\n",
    "    \n",
    "    # ------------------- OPTIMIZER ---------------------\n",
    "    optimizer_config = config['optimizer']\n",
    "    # Inject global lr & weight_decay into optimizer_config\n",
    "    optimizer_config['lr'] = config['lr']\n",
    "    optimizer_config['weight_decay'] = config['weight_decay']\n",
    "    \n",
    "    optimizer = create_optimizer(model, optimizer_config)\n",
    "    \n",
    "    # ------------------- SCHEDULER ---------------------\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    scheduler_config = config.get('scheduler', {})\n",
    "    scheduler, requires_val_loss = create_scheduler(\n",
    "        optimizer,\n",
    "        scheduler_config,\n",
    "        total_epochs=config['epochs'],\n",
    "        steps_per_epoch=steps_per_epoch\n",
    "    )\n",
    "    \n",
    "    # ------------------- WARMUP SCHEDULER ---------------\n",
    "    warmup_epochs = config.get('warmup_epochs', 0)\n",
    "    if warmup_epochs > 0:\n",
    "        warmup_scheduler = LambdaLR(\n",
    "            optimizer,\n",
    "            lambda epoch: min(1.0, (epoch + 1) / warmup_epochs)\n",
    "        )\n",
    "    else:\n",
    "        warmup_scheduler = None\n",
    "    \n",
    "    # -------------------- TRAINING LOOP --------------------\n",
    "    best_metric = -float(\"inf\")\n",
    "    best_model_weights = None  # will hold a copy of state_dict()\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # ---------- TRAIN ----------\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for labels, features in train_loader:\n",
    "            features = features.to(config['device']).float()\n",
    "            labels = labels.to(config['device']).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping (if specified)\n",
    "            if config.get('grad_clip') is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # ---------- VALIDATION ----------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for labels, features in val_loader:\n",
    "                features = features.to(config['device']).float()\n",
    "                labels = labels.to(config['device']).float()\n",
    "                \n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.sigmoid(outputs)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        predictions = (np.array(all_preds) > 0.5).astype(int)\n",
    "        \n",
    "        # ---------- METRICS ----------\n",
    "        accuracy = accuracy_score(all_labels, predictions)\n",
    "        \n",
    "        # ---------- SCHEDULER UPDATE ----------\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if warmup_scheduler is not None and epoch < warmup_epochs:\n",
    "            warmup_scheduler.step()\n",
    "        else:\n",
    "            if scheduler is not None:\n",
    "                if requires_val_loss:\n",
    "                    # e.g. ReduceLROnPlateau\n",
    "                    scheduler.step(val_loss)\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "        \n",
    "\n",
    "        \n",
    "        # ---------- SAVE BEST MODEL ----------\n",
    "        if accuracy > best_metric:\n",
    "            best_metric = accuracy\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # After the loop, restore the best weights\n",
    "    best_model = EEGMobileNet(\n",
    "        in_channels=64,\n",
    "        num_classes=1,\n",
    "        dropout=config['dropout']\n",
    "    ).to(config['device'])\n",
    "\n",
    "    best_model.load_state_dict(best_model_weights)\n",
    "\n",
    "    # Now best_model is truly the best epoch’s model\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = {\n",
    "    'data_path': '/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet',\n",
    "    'split_ratios': (0.7, 0.15, 0.15),\n",
    "    'batch_size': 32,\n",
    "}\n",
    "\n",
    "train_set = torch.load('train_set_smol.pt', weights_only=False)\n",
    "val_set = torch.load('val_set.pt', weights_only=False)\n",
    "test_set = torch.load('test_set.pt', weights_only=False)\n",
    "\n",
    "\n",
    "generator = torch.Generator().manual_seed(69)  # Set seed\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=config_data['batch_size'],\n",
    "    shuffle=True,\n",
    "    generator=generator,  # Add this line\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    # persistent_workers=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(val_set, batch_size=config_data['batch_size'], collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=config_data['batch_size'], collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_adamw_plateau(trial, train_set, train_loader, val_loader, device):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-4, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "    \n",
    "    # Scheduler params\n",
    "    factor = trial.suggest_float(\"factor\", 0.1, 0.8)\n",
    "    patience = trial.suggest_int(\"patience\", 2, 25)\n",
    "    cooldown = trial.suggest_int(\"cooldown\", 10, 25)\n",
    "    \n",
    "    config = {\n",
    "        \"device\": device,\n",
    "        \"dropout\": dropout,\n",
    "        \"epochs\": 300,\n",
    "        \"log_dir\": \"./runs/OptunaTest\",\n",
    "        \"warmup_epochs\": 0,\n",
    "        \"grad_clip\": None,\n",
    "        \"lr\": lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"AdamW\",\n",
    "        },\n",
    "        \n",
    "        # Scheduler config\n",
    "        \"scheduler\": {\n",
    "            \"type\": \"ReduceLROnPlateau\",\n",
    "            \"mode\": \"min\",\n",
    "            \"factor\": factor,\n",
    "            \"patience\": patience,\n",
    "            \"threshold\": 0.0001,\n",
    "            \"cooldown\": cooldown,\n",
    "            \"min_lr\": 1e-8,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    best_model = train_model(config, train_set, train_loader, val_loader)\n",
    "\n",
    "    # Move model to the correct device\n",
    "    best_model = best_model.to(config['device'])\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    best_model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    all_test_markers = []\n",
    "    all_test_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for markers, features in test_loader:\n",
    "            features = features.to(config['device'])\n",
    "            markers = markers.to(config['device'])\n",
    "\n",
    "            outputs = best_model(features)\n",
    "            # Collect markers and predictions for metrics calculation\n",
    "            all_test_markers.extend(markers.cpu().numpy().flatten())\n",
    "            all_test_predictions.extend(torch.sigmoid(outputs).cpu().numpy().flatten())\n",
    "\n",
    "    test_accuracy = accuracy_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "    \n",
    "    return test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34eaca6a5d394780ac3a5a9a00a8a460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Trial: 0.6402877697841727\n",
      "Best Hyperparams: {'lr': 4.60642718866224e-05, 'weight_decay': 9.100662172620129e-05, 'dropout': 0.05810041413216577, 'factor': 0.252755192665368, 'patience': 18, 'cooldown': 19}\n"
     ]
    }
   ],
   "source": [
    "# Example: Tuning AdamW + CyclicLR\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(\n",
    "    lambda trial: objective_adamw_plateau(\n",
    "        trial, train_set, train_loader, val_loader, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    ),\n",
    "    n_trials=1000,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"Best Trial:\", study.best_trial.value)\n",
    "print(\"Best Hyperparams:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle \n",
    "with open(r'study.pkl', 'wb') as f:\n",
    "    pickle.dump(study.best_trial.params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 4.60642718866224e-05,\n",
       " 'weight_decay': 9.100662172620129e-05,\n",
       " 'dropout': 0.05810041413216577,\n",
       " 'factor': 0.252755192665368,\n",
       " 'patience': 18,\n",
       " 'cooldown': 19}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
