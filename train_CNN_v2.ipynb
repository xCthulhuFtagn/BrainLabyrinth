{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # Must be first!\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import (\n",
    "    ReduceLROnPlateau,\n",
    "    LambdaLR\n",
    ")\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import polars as pl\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "###################\n",
    "from model import EEGMobileNet\n",
    "from dataset import EEGDatasetV2\n",
    "# from utils import collate_fn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "###################\n",
    "\n",
    "# Set seeds and deterministic flags\n",
    "random.seed(69)\n",
    "np.random.seed(69)\n",
    "torch.manual_seed(69)\n",
    "torch.cuda.manual_seed(69)\n",
    "torch.use_deterministic_algorithms(True)  # Enable full determinism\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.read_parquet('/home/owner/Documents/DEV/BrainLabyrinth/data/combined_prev_prev_2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, train_set, train_loader, val_loader, writer):\n",
    "    # -------------------- MODEL --------------------\n",
    "    model = EEGMobileNet(\n",
    "        in_channels=65,\n",
    "        num_classes=1,\n",
    "        dropout=config['dropout']\n",
    "    ).to(config['device'])\n",
    "        \n",
    "    for name, layer in model.named_modules():\n",
    "        if isinstance(layer, nn.BatchNorm1d):\n",
    "            assert not layer.track_running_stats, \\\n",
    "                f\"BatchNorm layer {name} should have track_running_stats=False\"\n",
    "    \n",
    "    # Log model architecture and config\n",
    "    writer.add_text(\"Model/Type\", f\"EEGMobileNet with dropout={config['dropout']}\")\n",
    "    writer.add_text(\"Model/Structure\", str(model))\n",
    "    writer.add_text(\"Training Config\", str(config))\n",
    "    \n",
    "    # ------------------ LOSS FUNCTION ------------------\n",
    "    \n",
    "    \n",
    "    class_counts = train_set.class_weights # Fetches {'Left_count': N, 'Right_count': M}\n",
    "\n",
    "    count_L = class_counts.get('Left', 1)  # Default to 1 to prevent division by zero errors later\n",
    "    count_R = class_counts.get('Right', 1)  # Default to 1\n",
    "\n",
    "    # Calculate the pos_weight value = count_negative / count_positive\n",
    "    # Add a check for count_R being zero\n",
    "    if count_R > 0:\n",
    "        pos_weight_value = count_L / count_R\n",
    "    else:\n",
    "        pos_weight_value = 1.0 # Assign a default weight if the positive class has zero samples\n",
    "        print(f\"Warning: Positive class (Right/1) has count {count_R}. Setting pos_weight to 1.0.\")\n",
    "\n",
    "    print(f\"Class counts: Left={count_L}, Right={count_R}\")\n",
    "    print(f\"Calculated pos_weight for BCEWithLogitsLoss: {pos_weight_value:.4f}\")\n",
    "    \n",
    "    pos_weight = torch.tensor([pos_weight_value]).to(config['device'])\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(weight=pos_weight)\n",
    "    \n",
    "    # ------------------- OPTIMIZER ---------------------\n",
    "    lr = config.get('lr', 1e-3)\n",
    "    weight_decay = config.get('weight_decay', 1e-2)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # ------------------- SCHEDULER ---------------------\n",
    "    scheduler_config = config.get('scheduler', {})\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=scheduler_config.get('mode', 'min'),\n",
    "        factor=scheduler_config.get('factor', 0.1),\n",
    "        patience=scheduler_config.get('patience', 10),\n",
    "        threshold=scheduler_config.get('threshold', 0.0001),\n",
    "        cooldown=scheduler_config.get('cooldown', 0),\n",
    "        min_lr=scheduler_config.get('min_lr', 0),\n",
    "    )\n",
    "    \n",
    "    # ------------------- WARMUP SCHEDULER ---------------\n",
    "    warmup_epochs = config.get('warmup_epochs', 0)\n",
    "    if warmup_epochs > 0:\n",
    "        warmup_scheduler = LambdaLR(\n",
    "            optimizer,\n",
    "            lambda epoch: min(1.0, (epoch + 1) / warmup_epochs)\n",
    "        )\n",
    "    else:\n",
    "        warmup_scheduler = None\n",
    "    \n",
    "    # -------------------- TRAINING LOOP --------------------\n",
    "    best_metric = -float('inf')\n",
    "    \n",
    "    for epoch in tqdm(range(config['epochs']), desc=\"Training\"):\n",
    "        # ---------- TRAIN ----------\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for labels, features, _ in train_loader:\n",
    "            features = features.to(config['device']).float()\n",
    "            labels = labels.to(config['device']).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features).squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping (if specified)\n",
    "            if config.get('grad_clip') is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # ---------- VALIDATION ----------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for labels, features, _ in val_loader:\n",
    "                features = features.to(config['device']).float()\n",
    "                labels = labels.to(config['device']).float()\n",
    "                \n",
    "                outputs = model(features).squeeze(1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.sigmoid(outputs)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        predictions = (np.array(all_preds) > 0.5).astype(int)\n",
    "        \n",
    "        # ---------- METRICS ----------\n",
    "        accuracy = accuracy_score(all_labels, predictions)\n",
    "        precision = precision_score(all_labels, predictions)\n",
    "        recall = recall_score(all_labels, predictions)\n",
    "        f1 = f1_score(all_labels, predictions)\n",
    "        \n",
    "        # ---------- SCHEDULER UPDATE ----------\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if warmup_scheduler is not None and epoch < warmup_epochs:\n",
    "            warmup_scheduler.step()\n",
    "        else:\n",
    "            if scheduler is not None:\n",
    "                scheduler.step(val_loss)\n",
    "        \n",
    "        # ---------- LOGGING ----------\n",
    "        writer.add_scalar('LR', current_lr, epoch)\n",
    "        writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Val', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy', accuracy, epoch)\n",
    "        writer.add_scalar('Precision', precision, epoch)\n",
    "        writer.add_scalar('Recall', recall, epoch)\n",
    "        writer.add_scalar('F1', f1, epoch)\n",
    "        \n",
    "        # You can also combine them in a single dictionary\n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "        writer.add_scalars('Metrics', metrics, epoch)\n",
    "        \n",
    "        # ---------- SAVE BEST MODEL ----------\n",
    "        if accuracy > best_metric:\n",
    "            best_metric = accuracy\n",
    "            torch.save(model.state_dict(), f\"{config['log_dir']}/best_model.pth\")\n",
    "    \n",
    "    writer.close()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('baseline_CNN_V2.pkl', 'rb') as f:\n",
    "    config_study = pickle.load(f)\n",
    "    \n",
    "required_keys = ['lr', 'weight_decay', 'dropout', 'factor', 'patience', 'cooldown']\n",
    "assert all(k in config_study for k in required_keys), \\\n",
    "    f\"Missing Optuna parameters: {set(required_keys) - set(config_study.keys())}\"\n",
    "    \n",
    "config_study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data_path': '/home/owner/Documents/DEV/BrainLabyrinth/data/combined_prev_prev_2.parquet',\n",
    "    'split_ratios': (0.7, 0.15, 0.15),\n",
    "    'batch_size': 32,\n",
    "    'dropout': config_study['dropout'],\n",
    "    'epochs': 300,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'log_dir': './runs/CNN',\n",
    "\n",
    "    # <<< Global LR and Weight Decay here >>>\n",
    "    'lr': config_study['lr'],\n",
    "    'weight_decay': config_study['weight_decay'],\n",
    "    'factor': config_study['factor'],\n",
    "    'patience': config_study['patience'],\n",
    "    'cooldown': config_study['cooldown'],\n",
    "    \n",
    "    # Optimizer config (without lr/weight_decay)\n",
    "    'optimizer': {\n",
    "        'mode': 'min',\n",
    "        'factor': config_study['factor'],      # From Optuna\n",
    "        'patience': config_study['patience'],  # From Optuna\n",
    "        'cooldown': config_study['cooldown'],  # From Optuna\n",
    "        'min_lr': 1e-8,\n",
    "        'threshold': 0.0001,\n",
    "    },\n",
    "\n",
    "    # Scheduler config\n",
    "    'scheduler': {\n",
    "        'mode': 'min',\n",
    "        'factor': config_study['factor'],\n",
    "        'patience': config_study['patience'],\n",
    "        'threshold': 0.0001,\n",
    "        'cooldown': config_study['cooldown'],\n",
    "        'min_lr': 1e-8\n",
    "    },\n",
    "\n",
    "    'warmup_epochs': 0,\n",
    "    'grad_clip': None\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# Training Pipeline\n",
    "#============================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Initialize dataset\n",
    "print(\"Creating full dataset...\")\n",
    "full_dataset = EEGDatasetV2(config['data_path'])\n",
    "\n",
    "print(full_dataset.class_weights)\n",
    "\n",
    "print(\"Splitting the dataset...\")\n",
    "# Split dataset\n",
    "train_set, val_set, test_set = full_dataset.split_dataset(\n",
    "    ratios=config['split_ratios']\n",
    ")\n",
    "\n",
    "del full_dataset\n",
    "\n",
    "len_dataset = len(train_set)\n",
    "sample = train_set[0]\n",
    "label_shape = sample[0].shape\n",
    "feature_shape = sample[1].shape\n",
    "\n",
    "print(f\"unbalanced train dataset shape: ({len_dataset}, [labels: {label_shape}, features: {list(feature_shape)}])\")\n",
    "\n",
    "torch.save(train_set, 'train_set.pt')\n",
    "torch.save(val_set, 'val_set.pt')\n",
    "torch.save(test_set, 'test_set.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_set = torch.load('train_set.pt', weights_only=False)\n",
    "val_set = torch.load('val_set.pt', weights_only=False)\n",
    "test_set = torch.load('test_set.pt', weights_only=False)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    labels, features, original_labels = zip(*batch)\n",
    "    labels = torch.stack(labels)\n",
    "    # Pad sequences, resulting shape: (batch_size, max_seq_len, num_channels)\n",
    "    padded_features = pad_sequence(features, batch_first=True)\n",
    "    original_labels = torch.stack(original_labels)\n",
    "\n",
    "    # Transpose features to shape: (batch_size, num_channels, max_seq_len)\n",
    "    transposed_features = padded_features.transpose(1, 2)\n",
    "\n",
    "    return labels, transposed_features, original_labels\n",
    "\n",
    "\n",
    "generator = torch.Generator().manual_seed(69)  # Set seed\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    generator=generator,  # Add this line\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    # persistent_workers=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(val_set, batch_size=config['batch_size'], collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=config['batch_size'], collate_fn=collate_fn)\n",
    "\n",
    "len_dataset = len(train_set)\n",
    "sample = train_set[0]\n",
    "label_shape = sample[0].shape\n",
    "feature_shape = sample[1].shape\n",
    "\n",
    "print(f\"train dataset shape: ({len_dataset}, [labels: {label_shape}, features: {list(feature_shape)}])\")\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "writer = SummaryWriter(log_dir=config['log_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set.df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Applying seeds immediately before training...\")\n",
    "random.seed(69)\n",
    "np.random.seed(69)\n",
    "torch.manual_seed(69)\n",
    "torch.cuda.manual_seed(69)\n",
    "# Ensure deterministic algorithms are still enabled (should be from top)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "trained_model = train_model(config, train_set, train_loader, val_loader, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "best_model = EEGMobileNet(in_channels=65)  # Adjust parameters as needed\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load(f\"{config['log_dir']}/best_model.pth\", map_location=config['device'])\n",
    "best_model.load_state_dict(state_dict)\n",
    "\n",
    "# Move model to the correct device\n",
    "best_model = best_model.to(config['device'])\n",
    "\n",
    "# Set model to evaluation mode\n",
    "best_model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "all_test_markers = []\n",
    "all_test_predictions = []\n",
    "all_test_original_markers = []\n",
    "with torch.no_grad():\n",
    "    for markers, features, original_markers in tqdm(test_loader):\n",
    "        features = features.to(config['device'])\n",
    "        markers = markers.to(config['device'])\n",
    "\n",
    "        outputs = best_model(features)\n",
    "\n",
    "        # Collect markers and predictions for metrics calculation\n",
    "        all_test_markers.extend(markers.cpu().numpy().flatten())\n",
    "        all_test_predictions.extend(torch.sigmoid(outputs).cpu().numpy().flatten())\n",
    "        all_test_original_markers.extend(original_markers.cpu().numpy().flatten())\n",
    "\n",
    "# Calculate test metrics\n",
    "test_accuracy = accuracy_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_precision = precision_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_recall = recall_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_f1 = f1_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_roc_auc = roc_auc_score(all_test_markers, all_test_predictions)\n",
    "\n",
    "# Log test metrics to TensorBoard\n",
    "writer.add_scalar('Metrics/test_accuracy', test_accuracy, 1)\n",
    "writer.add_scalar('Metrics/test_precision', test_precision, 1)\n",
    "writer.add_scalar('Metrics/test_recall', test_recall, 1)\n",
    "writer.add_scalar('Metrics/test_f1', test_f1, 1)\n",
    "writer.add_scalar('Metrics/test_roc_auc', test_roc_auc, 1)\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "{test_accuracy=}\n",
    "{test_precision=}\n",
    "{test_recall=}\n",
    "{test_f1=}\n",
    "{test_roc_auc=}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "best_threshold = 0.0\n",
    "best_f1 = 0.0\n",
    "thresholds = np.arange(0.1, 1.0, 0.01)\n",
    "\n",
    "for threshold in tqdm(thresholds):\n",
    "    binary_predictions = (all_test_predictions > threshold).astype(int)\n",
    "    current_recall = f1_score(all_test_markers, binary_predictions)\n",
    "\n",
    "    if current_recall > best_f1:\n",
    "        best_f1 = current_recall\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"{best_threshold=}\")\n",
    "print(f\"{best_f1=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "best_threshold = 0.1\n",
    "best_accuracy = 0.0\n",
    "thresholds = np.arange(0.005, 1.0, 0.005)\n",
    "\n",
    "for threshold in tqdm(thresholds):\n",
    "    binary_predictions = (all_test_predictions > threshold).astype(int) \n",
    "    current_recall = accuracy_score(all_test_markers, binary_predictions)\n",
    "\n",
    "    if current_recall > best_accuracy:\n",
    "        best_accuracy = current_recall\n",
    "        best_threshold = threshold\n",
    "        precision = precision_score(all_test_markers, [1 if p > threshold else 0 for p in all_test_predictions])\n",
    "        recall = recall_score(all_test_markers, [1 if p > threshold else 0 for p in all_test_predictions])\n",
    "        f1 = f1_score(all_test_markers, [1 if p > threshold else 0 for p in all_test_predictions])\n",
    "        roc_auc = roc_auc_score(all_test_markers, all_test_predictions)\n",
    "\n",
    "print(f\"{best_threshold=}\")\n",
    "print(f\"\"\"\n",
    "{best_accuracy=}\n",
    "{precision=}\n",
    "{recall=}\n",
    "{f1=}\n",
    "{roc_auc=}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "statuses = []\n",
    "test_rightness =  all_test_markers == np.array([1 if p > best_threshold else 0 for p in all_test_predictions])\n",
    "for original_marker, positive_verdict in zip(all_test_original_markers, test_rightness):\n",
    "    if original_marker == 1:\n",
    "        if positive_verdict:\n",
    "            statuses.append('Stimulus/P right')\n",
    "        else:\n",
    "            statuses.append('Stimulus/P wrong')\n",
    "    else:\n",
    "        if positive_verdict:\n",
    "            statuses.append('Stimulus/A right')\n",
    "        else:\n",
    "            statuses.append('Stimulus/A wrong')\n",
    "\n",
    "\n",
    "results_counter = Counter(statuses)\n",
    "\n",
    "\n",
    "\n",
    "# --- Extract counts ---\n",
    "# Use .get() for safety in case a key is missing (though not in this example)\n",
    "stim_A_right = results_counter.get('Stimulus/A right', 0)\n",
    "stim_A_wrong = results_counter.get('Stimulus/A wrong', 0)\n",
    "stim_P_right = results_counter.get('Stimulus/P right', 0)\n",
    "stim_P_wrong = results_counter.get('Stimulus/P wrong', 0)\n",
    "\n",
    "# --- Calculate totals ---\n",
    "total_A_events = stim_A_right + stim_A_wrong\n",
    "total_P_events = stim_P_right + stim_P_wrong\n",
    "\n",
    "total_right_predictions = stim_A_right + stim_P_right\n",
    "total_wrong_predictions = stim_A_wrong + stim_P_wrong\n",
    "total_events = total_right_predictions + total_wrong_predictions\n",
    "\n",
    "# --- Calculate Metrics ---\n",
    "\n",
    "# 1. Overall Accuracy of the Rules\n",
    "# (How often did the rule correctly predict the outcome?)\n",
    "overall_accuracy = total_right_predictions / total_events if total_events > 0 else 0.0\n",
    "\n",
    "# 2. Accuracy of Rule A (Flip)\n",
    "# (When Stimulus/A was shown, how often was flipping the correct strategy?)\n",
    "accuracy_A = stim_A_right / total_A_events if total_A_events > 0 else 0.0\n",
    "\n",
    "# 3. Accuracy of Rule P (Persist)\n",
    "# (When Stimulus/P was shown, how often was persisting the correct strategy?)\n",
    "accuracy_P = stim_P_right / total_P_events if total_P_events > 0 else 0.0\n",
    "\n",
    "# --- Print Results ---\n",
    "print(\"--- Metrics Based on Rule Application Success ---\")\n",
    "print(f\"Total Events Analyzed (A + P): {total_events}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Stimulus/A Events: {total_A_events}\")\n",
    "print(f\"  - Rule 'Flip' Correct: {stim_A_right}\")\n",
    "print(f\"  - Rule 'Flip' Incorrect: {stim_A_wrong}\")\n",
    "print(f\"  - Accuracy of 'Flip' Rule: {accuracy_A:.4f}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Stimulus/P Events: {total_P_events}\")\n",
    "print(f\"  - Rule 'Persist' Correct: {stim_P_right}\")\n",
    "print(f\"  - Rule 'Persist' Incorrect: {stim_P_wrong}\")\n",
    "print(f\"  - Accuracy of 'Persist' Rule: {accuracy_P:.4f}\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Overall Accuracy (Rule matched outcome): {overall_accuracy:.4f}\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(\"\\nNote:\")\n",
    "print(\"These metrics evaluate the success rate of the simple 'Flip'/'Persist' heuristics.\")\n",
    "print(\"They are NOT standard classification metrics like Precision, Recall, or F1-Score \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if test_set is not None:\n",
    "    # Load the trained model\n",
    "    # Ensure EEGMobileNet is defined and accessible (e.g., in model.py)\n",
    "    try:\n",
    "        # Adjust in_channels and num_classes based on your EEGMobileNet definition\n",
    "        best_model = EEGMobileNet(in_channels=65, num_classes=1, dropout=config.get('dropout', 0.5)) # Use dropout from config if available\n",
    "        model_path = f\"{config['log_dir']}/best_model.pth\"\n",
    "\n",
    "        state_dict = torch.load(model_path, map_location=config['device'])\n",
    "        best_model.load_state_dict(state_dict)\n",
    "        best_model = best_model.to(config['device'])\n",
    "        best_model.eval() # Set model to evaluation mode\n",
    "        print(f\"Model loaded successfully from {model_path}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: best_model.pth not found at {model_path}. Please ensure the training notebook was run to save the model.\")\n",
    "        best_model = None\n",
    "    except NameError:\n",
    "            print(\"Error: EEGMobileNet class not found. Make sure model.py is in the same directory or its path is included.\")\n",
    "            best_model = None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        best_model = None\n",
    "\n",
    "# --- Perform Interpretation Analysis ---\n",
    "if best_model is not None and test_loader is not None and test_set is not None:\n",
    "    print(\"\\n--- Starting Model Interpretation Analysis ---\")\n",
    "\n",
    "# --- 1. Analysis of Filter Activations ---\n",
    "print(\"\\nAnalyzing Filter Activations...\")\n",
    "\n",
    "# Get a sample batch from the test loader\n",
    "# Taking the first batch for demonstration\n",
    "# Make sure test_loader provides labels, features, and original_labels\n",
    "sample_labels, sample_features, sample_original_labels = next(iter(test_loader))\n",
    "\n",
    "\n",
    "sample_features = sample_features.to(config['device']).float()\n",
    "\n",
    "# Transpose features before passing to the model for activation analysis\n",
    "# Shape becomes (batch_size, channels, seq_len)\n",
    "\n",
    "\n",
    "# Define a function to get activations from intermediate layers\n",
    "def get_activations(model, layer_name, input_data):\n",
    "    activations = None\n",
    "    def hook_fn(module, input, output):\n",
    "        nonlocal activations\n",
    "        activations = output.detach().cpu()\n",
    "\n",
    "    target_layer = None\n",
    "    for name, module in model.named_modules():\n",
    "        if name == layer_name:\n",
    "            target_layer = module\n",
    "            break\n",
    "\n",
    "    if target_layer is None:\n",
    "        print(f\"Layer '{layer_name}' not found in the model.\")\n",
    "        return None\n",
    "\n",
    "    hook = target_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Use the transposed features here\n",
    "        model(input_data)\n",
    "\n",
    "    hook.remove()\n",
    "\n",
    "    return activations\n",
    "\n",
    "# Choose a convolutional layer to visualize activations from\n",
    "# Corrected layer name: 'model.0' to access the first Conv1d in nn.Sequential\n",
    "layer_to_visualize = 'model.0'\n",
    "\n",
    "print(f\"Attempting to get activations from layer: {layer_to_visualize}\")\n",
    "# Pass the transposed features to the get_activations function\n",
    "activations = get_activations(best_model, layer_to_visualize, sample_features)\n",
    "\n",
    "if activations is not None:\n",
    "    print(f\"Shape of activations from layer '{layer_to_visualize}': {activations.shape}\")\n",
    "\n",
    "    num_samples_to_viz = min(4, activations.shape[0])\n",
    "    num_filters_to_viz = min(8, activations.shape[1])\n",
    "\n",
    "    plt.figure(figsize=(15, num_samples_to_viz * 2))\n",
    "    for i in range(num_samples_to_viz):\n",
    "        for j in range(num_filters_to_viz):\n",
    "            plt.subplot(num_samples_to_viz, num_filters_to_viz, i * num_filters_to_viz + j + 1)\n",
    "            # Activations are typically (batch, filters, sequence_length).\n",
    "            # We are visualizing a single filter's activation across time for a sample\n",
    "            plt.imshow(activations[i, j].unsqueeze(0), aspect='auto', cmap='viridis') # Shape (1, sequence_length)\n",
    "            plt.title(f'Sample {i+1}, Filter {j+1}')\n",
    "            plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Filter Activations from Layer: {layer_to_visualize}', y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(f\"Skipping filter activation visualization.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Assuming test_set, config, and best_model are already defined and available\n",
    "# (as they are used in the original code)\n",
    "\n",
    "print(\"\\nGenerating Saliency Maps...\")\n",
    "\n",
    "# Determine the number of saliency samples you want to generate plots for.\n",
    "# Using a smaller number (e.g., 5 or 10) is recommended for separate plots\n",
    "# to avoid opening too many windows.\n",
    "num_saliency_samples = len(test_set) # Use this if you want all samples\n",
    "# num_saliency_samples = 5 # Example: plot only the first 5 saliency maps\n",
    "\n",
    "# Select indices for saliency mapping.\n",
    "# If num_saliency_samples is less than len(test_set), select randomly.\n",
    "# If num_saliency_samples equals len(test_set), use all indices.\n",
    "if num_saliency_samples > len(test_set):\n",
    "     num_saliency_samples = len(test_set)\n",
    "     saliency_indices = np.arange(len(test_set))\n",
    "else:\n",
    "     saliency_indices = np.random.choice(len(test_set), num_saliency_samples, replace=False)\n",
    "\n",
    "\n",
    "for i, idx in enumerate(saliency_indices):\n",
    "    label, feature, original_label = test_set[idx]\n",
    "\n",
    "    # Add batch dimension and move to device\n",
    "    feature = feature.to(config['device']).float().unsqueeze(0) # Shape: (1, seq_len, channels)\n",
    "    label = torch.tensor([label]).to(config['device']).float()\n",
    "\n",
    "    # Transpose features before passing to the model for saliency calculation\n",
    "    feature_transposed = feature.transpose(1, 2) # Shape becomes (1, channels, seq_len)\n",
    "\n",
    "    # Enable gradient calculation for the transposed input feature tensor\n",
    "    feature_transposed.requires_grad_()\n",
    "\n",
    "    # Perform a forward pass using the transposed features\n",
    "    output = best_model(feature_transposed)\n",
    "    # Assuming the model output is logits for binary classification\n",
    "\n",
    "    output_logit = output.squeeze()\n",
    "\n",
    "    # Calculate the target output to backpropagate through.\n",
    "    # This targets the gradient with respect to the logit of the true class.\n",
    "    # If label is 1, target_output = output_logit * 1 + (-output_logit) * 0 = output_logit\n",
    "    # If label is 0, target_output = output_logit * 0 + (-output_logit) * 1 = -output_logit\n",
    "    # This means we want gradients that increase the logit for class 1 if the label is 1,\n",
    "    # and gradients that decrease the logit for class 1 if the label is 0.\n",
    "    target_output = output_logit * label + (-output_logit) * (1 - label)\n",
    "\n",
    "    # Perform backpropagation to get gradients with respect to the input\n",
    "    best_model.zero_grad() # Clear previous gradients\n",
    "    target_output.backward()\n",
    "\n",
    "    # Get the absolute gradients from the transposed features\n",
    "    # Need to transpose back the gradients to match original (seq_len, channels) for plotting\n",
    "    saliency = feature_transposed.grad.abs().squeeze(0).cpu().numpy().transpose(1, 0) # Transpose gradient back\n",
    "\n",
    "    # --- Plotting the saliency map on a separate figure ---\n",
    "    plt.figure(figsize=(10, 6)) # Create a new figure for each sample\n",
    "\n",
    "    # Plot saliency map with time on x-axis and channels on y-axis\n",
    "    # Transpose saliency for plotting: (seq_len, channels) -> (channels, seq_len)\n",
    "    # origin='lower' places the first row of the array at the bottom of the plot.\n",
    "    plt.imshow(saliency.T, aspect='auto', cmap='hot', origin='lower')\n",
    "    plt.colorbar(label='Absolute Gradient Magnitude')\n",
    "    plt.title(f'Saliency Map for Sample {idx} (True Label: {original_label.item()})')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('EEG Channels')\n",
    "\n",
    "    # Display the current figure\n",
    "    plt.show()\n",
    "\n",
    "    # Zero the gradients after processing the sample\n",
    "    feature_transposed.grad.zero_()\n",
    "\n",
    "print(\"Finished generating Saliency Maps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm # Using tqdm.notebook for progress bar in a notebook environment\n",
    "\n",
    "# Assuming best_model, test_loader, test_set, and config are defined and available\n",
    "# Make sure your best_model is loaded and in evaluation mode (best_model.eval())\n",
    "# and moved to the correct device (best_model.to(config['device']))\n",
    "\n",
    "if best_model is not None and test_loader is not None and test_set is not None:\n",
    "    print(\"\\n--- Generating Average Saliency Map ---\")\n",
    "\n",
    "    # Initialize a tensor to accumulate saliency maps\n",
    "    # The shape should be (sequence_length, number_of_channels)\n",
    "    # Get the shape from the first sample in the test set\n",
    "    _, first_feature, _ = test_set[0]\n",
    "    saliency_shape = first_feature.shape # Assuming shape is (seq_len, channels)\n",
    "    total_saliency = torch.zeros(saliency_shape).to(config['device']) # Accumulate on the same device as the model\n",
    "\n",
    "    total_samples = 0\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    best_model.eval()\n",
    "\n",
    "    # Iterate through the test dataset\n",
    "    # Use test_loader for batch processing\n",
    "    for labels, features, original_labels in tqdm(test_loader, desc=\"Calculating Saliency Maps\"):\n",
    "        # Move data to the correct device\n",
    "        features = features.to(config['device']).float()\n",
    "        labels = labels.to(config['device']).float()\n",
    "\n",
    "\n",
    "        # Enable gradient calculation for the input features\n",
    "        features.requires_grad_()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = best_model(features)\n",
    "\n",
    "        # Assuming binary classification with logits output\n",
    "        # Calculate the loss or target output for backpropagation\n",
    "        # For saliency, we want the gradient of the output w.r.t. the input\n",
    "        # We can use the output logit directly or the loss w.r.t. the true label.\n",
    "        # Let's use the output logit for simplicity to see general feature importance for the predicted class score.\n",
    "        # If you want saliency w.r.t. the correct class score, you would use the target_output method from before.\n",
    "        # For an average map over the whole dataset, the raw output might be more informative of general patterns.\n",
    "        # Let's take the sum of the absolute outputs for simplicity in averaging across the dataset.\n",
    "        # Alternatively, you could sum the gradients of the loss w.r.t. the input.\n",
    "        # Let's stick to the gradient of the output logit as requested by the user's previous code structure.\n",
    "\n",
    "        # Calculate the gradient of the outputs with respect to the input features\n",
    "        # Sum the outputs for the backward pass to get gradients for the batch\n",
    "        outputs.sum().backward()\n",
    "\n",
    "        # Get the absolute gradients from the transposed features\n",
    "        # The gradient shape will be the same as the input shape (batch_size, channels, seq_len)\n",
    "        # Squeeze to remove batch dimension if batch_size is 1, abs for absolute magnitude\n",
    "        # Transpose from (channels, seq_len) to (seq_len, channels) to match original feature shape\n",
    "        saliency_batch = features.grad.abs().transpose(1, 2) # Shape (batch_size, seq_len, channels)\n",
    "\n",
    "        # Accumulate the saliency maps\n",
    "        total_saliency += saliency_batch.sum(dim=0) # Sum across the batch dimension\n",
    "\n",
    "        # Count the number of samples processed\n",
    "        total_samples += features.size(0)\n",
    "\n",
    "        # Zero out gradients after processing the batch\n",
    "        features.grad.zero_()\n",
    "\n",
    "    # Calculate the average saliency map\n",
    "    average_saliency = total_saliency / total_samples\n",
    "\n",
    "    # Move the average saliency map to CPU for plotting\n",
    "    average_saliency_np = average_saliency.cpu().numpy()\n",
    "\n",
    "    # --- Visualize the Average Saliency Map ---\n",
    "    print(\"\\nVisualizing Average Saliency Map...\")\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    # Plot average saliency map with time on x-axis and channels on y-axis\n",
    "    # imshow expects (height, width), so (channels, seq_len) after transpose\n",
    "    plt.imshow(average_saliency_np.T, aspect='auto', cmap='coolwarm', origin='lower')\n",
    "    plt.colorbar(label='Average Absolute Gradient Magnitude')\n",
    "    plt.title('Average Saliency Map Across Test Dataset')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('EEG Channels')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Model or test data not available. Cannot generate average saliency map.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
