{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import (\n",
    "    ReduceLROnPlateau,\n",
    "    CosineAnnealingLR,\n",
    "    CyclicLR,\n",
    "    OneCycleLR,\n",
    "    LambdaLR\n",
    ")\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import polars as pl\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # Fix for CuBLAS determinism\n",
    "\n",
    "# Set seeds and deterministic flags\n",
    "random.seed(69)\n",
    "np.random.seed(69)\n",
    "torch.manual_seed(69)\n",
    "torch.cuda.manual_seed(69)\n",
    "torch.use_deterministic_algorithms(True)  # Enable full determinism\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['event_id',\n",
       " 'orig_marker',\n",
       " 'time',\n",
       " 'Fp1',\n",
       " 'Fpz',\n",
       " 'Fp2',\n",
       " 'F7',\n",
       " 'F3',\n",
       " 'Fz',\n",
       " 'F4',\n",
       " 'F8',\n",
       " 'FC5',\n",
       " 'FC1',\n",
       " 'FC2',\n",
       " 'FC6',\n",
       " 'M1',\n",
       " 'T7',\n",
       " 'C3',\n",
       " 'Cz',\n",
       " 'C4',\n",
       " 'T8',\n",
       " 'M2',\n",
       " 'CP5',\n",
       " 'CP1',\n",
       " 'CP2',\n",
       " 'CP6',\n",
       " 'P7',\n",
       " 'P3',\n",
       " 'Pz',\n",
       " 'P4',\n",
       " 'P8',\n",
       " 'POz',\n",
       " 'O1',\n",
       " 'O2',\n",
       " 'AF7',\n",
       " 'AF3',\n",
       " 'AF4',\n",
       " 'AF8',\n",
       " 'F5',\n",
       " 'F1',\n",
       " 'F2',\n",
       " 'F6',\n",
       " 'FC3',\n",
       " 'FCz',\n",
       " 'FC4',\n",
       " 'C5',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C6',\n",
       " 'CP3',\n",
       " 'CP4',\n",
       " 'P5',\n",
       " 'P1',\n",
       " 'P2',\n",
       " 'P6',\n",
       " 'PO5',\n",
       " 'PO3',\n",
       " 'PO4',\n",
       " 'PO6',\n",
       " 'FT7',\n",
       " 'FT8',\n",
       " 'TP7',\n",
       " 'TP8',\n",
       " 'PO7',\n",
       " 'PO8',\n",
       " 'Oz',\n",
       " 'marker',\n",
       " 'prev_marker']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.read_parquet('/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet')\\\n",
    "    .columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #============================================================\n",
    "# # Model Architecture\n",
    "# #============================================================\n",
    "# class EEGDSConv(nn.Module):\n",
    "#     def __init__(self, dropout=0.5):\n",
    "#         super().__init__()\n",
    "#         self.block = nn.Sequential(\n",
    "#             nn.Conv1d(64, 64, 15, padding='same', groups=64),\n",
    "#             nn.Conv1d(64, 16, 1),\n",
    "#             nn.BatchNorm1d(16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool1d(4),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Conv1d(16, 16, 7, padding='same', groups=16),\n",
    "#             nn.Conv1d(16, 8, 1),\n",
    "#             nn.BatchNorm1d(8),\n",
    "#             nn.ReLU(),\n",
    "#             nn.AdaptiveAvgPool1d(1),\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(8, 1)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = x.permute(0, 2, 1)\n",
    "#         return self.block(x).squeeze(-1)  # Squeeze last dimension to match target shape\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class EEGMobileNet(nn.Module):\n",
    "    def __init__(self, in_channels=64, num_classes=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # Initial Conv\n",
    "            nn.Conv1d(in_channels, 32, kernel_size=15, stride=2, padding=7),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),  # ← Insert dropout here\n",
    "\n",
    "            # Depthwise\n",
    "            nn.Conv1d(32, 32, kernel_size=3, stride=1, padding=1, groups=32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Pointwise\n",
    "            nn.Conv1d(32, 64, kernel_size=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),  # ← Insert dropout here\n",
    "\n",
    "            # Another Depthwise Separable block\n",
    "            nn.Conv1d(64, 64, kernel_size=3, stride=2, padding=1, groups=64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(64, 128, kernel_size=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),  # ← Insert dropout here\n",
    "\n",
    "            # Global Average Pool\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # your original transpose\n",
    "        return self.model(x).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tslearn.neighbors import KNeighborsTimeSeries\n",
    "from numba import njit\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def fast_interpolate(original, neighbor, alpha):\n",
    "    \"\"\"Numba-accelerated linear interpolation for numeric columns.\"\"\"\n",
    "    return (1 - alpha) * original + alpha * neighbor\n",
    "\n",
    "class TSMOTE:\n",
    "    def __init__(self, \n",
    "                 n_neighbors=3, \n",
    "                 time_slices=10, \n",
    "                 bool_cols=None):\n",
    "        \"\"\"\n",
    "        :param n_neighbors: Number of neighbors for KNN\n",
    "        :param time_slices: Number of slices to split each time series\n",
    "        :param bool_cols:   List (or array) of indices for boolean columns\n",
    "        \"\"\"\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.time_slices = time_slices\n",
    "        self.slice_size = None  # will be set after seeing data\n",
    "        self.bool_cols = bool_cols if bool_cols is not None else []\n",
    "        # numeric_cols will be determined at fit-time, after we see total channels.\n",
    "\n",
    "    def _slice_time_series(self, X):\n",
    "        \"\"\"Split into time slices: (N, 2000, ch) -> (N, time_slices, slice_size, ch).\"\"\"\n",
    "        return X.reshape(X.shape[0], self.time_slices, self.slice_size, X.shape[2])\n",
    "\n",
    "    def _generate_synthetic(self, minority_samples, bool_probs):\n",
    "        \"\"\"\n",
    "        Generate full-length synthetic samples.\n",
    "        :param minority_samples: Array of shape (N_minority, 2000, ch)\n",
    "        :param bool_probs:       Dict mapping boolean column index -> probability of 1\n",
    "        \"\"\"\n",
    "        # slice_size was computed earlier in fit_resample.\n",
    "        sliced_data = self._slice_time_series(minority_samples)  # shape (N, slices, slice_size, ch)\n",
    "        syn_samples = []\n",
    "\n",
    "        # We'll figure out numeric_cols from total channels\n",
    "        all_cols = list(range(minority_samples.shape[2]))\n",
    "        numeric_cols = [c for c in all_cols if c not in self.bool_cols]\n",
    "\n",
    "        for sample_idx in tqdm(range(sliced_data.shape[0]), desc=\"Generating synthetic\"):\n",
    "            synthetic_slices = []\n",
    "\n",
    "            # For each time slice\n",
    "            for slice_idx in range(self.time_slices):\n",
    "                # Split data into included (numeric) columns vs. excluded (boolean) columns\n",
    "                slice_incl = sliced_data[:, slice_idx, :, :][:, :, numeric_cols]  # (N, slice_size, #numeric)\n",
    "                slice_excl = sliced_data[:, slice_idx, :, :][:, :, self.bool_cols] # (N, slice_size, #bool)\n",
    "\n",
    "                # Fit KNN on included (numeric) data only\n",
    "                knn = KNeighborsTimeSeries(n_neighbors=self.n_neighbors, metric='dtw')\n",
    "                knn.fit(slice_incl)  # each entry is shape (slice_size, #numeric)\n",
    "\n",
    "                # The sample's numeric slice\n",
    "                original_slice_incl = slice_incl[sample_idx]  # shape (slice_size, #numeric)\n",
    "\n",
    "                # Find neighbors for this numeric slice\n",
    "                neighbors = knn.kneighbors(original_slice_incl[np.newaxis], \n",
    "                                           return_distance=False)[0]\n",
    "                neighbor_idx = np.random.choice(neighbors)\n",
    "\n",
    "                neighbor_slice_incl = slice_incl[neighbor_idx]  # shape (slice_size, #numeric)\n",
    "\n",
    "                # Interpolate for numeric columns\n",
    "                alpha = np.random.uniform(0.2, 0.8)\n",
    "                # Using fast_interpolate or direct calculation:\n",
    "                synthetic_slice_incl = fast_interpolate(original_slice_incl, \n",
    "                                                        neighbor_slice_incl, \n",
    "                                                        alpha)\n",
    "\n",
    "                # For boolean columns: sample from distribution\n",
    "                # We'll create an array of shape (slice_size, #bool)\n",
    "                # For each boolean column index b, pick 0/1 based on bool_probs[b].\n",
    "                # If you want different logic (like \"choose original or neighbor\"?),\n",
    "                # you can adapt here.\n",
    "                n_bool_cols = len(self.bool_cols)\n",
    "                synthetic_slice_excl = np.zeros((self.slice_size, n_bool_cols), dtype=np.float32)\n",
    "\n",
    "                for col_idx_in_boolarray, bcol in enumerate(self.bool_cols):\n",
    "                    p = bool_probs[bcol]  # Probability of 1 for that bool column\n",
    "                    # Sample 0/1 for each time step in the slice\n",
    "                    synthetic_slice_excl[:, col_idx_in_boolarray] = \\\n",
    "                        np.random.binomial(n=1, p=p, size=self.slice_size)\n",
    "                \n",
    "                # Combine numeric + boolean columns back in correct order\n",
    "                # We have numeric_cols in synthetic_slice_incl\n",
    "                # We have bool_cols in synthetic_slice_excl\n",
    "                # We need to re-insert them into shape: (slice_size, total_channels)\n",
    "                synthetic_slice = np.zeros((self.slice_size, len(all_cols)), dtype=np.float32)\n",
    "\n",
    "                # Place numeric columns\n",
    "                synthetic_slice[:, numeric_cols] = synthetic_slice_incl\n",
    "                # Place boolean columns\n",
    "                synthetic_slice[:, self.bool_cols] = synthetic_slice_excl\n",
    "\n",
    "                synthetic_slices.append(synthetic_slice)\n",
    "\n",
    "            # Concatenate slices into a full time series (2000, ch)\n",
    "            full_series = np.concatenate(synthetic_slices, axis=0)\n",
    "            syn_samples.append(full_series)\n",
    "\n",
    "        return np.array(syn_samples)\n",
    "\n",
    "    def fit_resample(self, X, y):\n",
    "        \"\"\"\n",
    "        Perform TSMOTE oversampling.\n",
    "        :param X: shape (N, 2000, ch)\n",
    "        :param y: shape (N,)\n",
    "        \"\"\"\n",
    "        y_int = y.astype(int)\n",
    "        class_counts = np.bincount(y_int)\n",
    "        minority_class = np.argmin(class_counts)\n",
    "        majority_class = 1 - minority_class\n",
    "\n",
    "        n_needed = class_counts[majority_class] - class_counts[minority_class]\n",
    "        if n_needed <= 0:\n",
    "            return X, y  # no oversampling needed\n",
    "\n",
    "        # Suppose X has shape (N, 2000, ch). We'll assume 2000 is consistent with time_slices * slice_size.\n",
    "        # We'll deduce slice_size\n",
    "        self.slice_size = X.shape[1] // self.time_slices  # e.g. 2000/10=200\n",
    "\n",
    "        # Get only minority samples\n",
    "        minority_samples = X[y_int == minority_class]\n",
    "\n",
    "        # ----- Compute distribution of booleans in the minority data ------\n",
    "        # For each bool column b, compute fraction of 1s across the entire minority set\n",
    "        bool_probs = {}\n",
    "        if len(self.bool_cols) > 0:\n",
    "            # shape is (N_minority, 2000, ch)\n",
    "            # We'll flatten across time for each column to get overall fraction\n",
    "            for bcol in self.bool_cols:\n",
    "                col_values = minority_samples[:, :, bcol].flatten()\n",
    "                p = col_values.mean()  # fraction of 1's\n",
    "                bool_probs[bcol] = p\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        synthetic = self._generate_synthetic(minority_samples, bool_probs)\n",
    "\n",
    "        # Ensure matching dimensions\n",
    "        assert X.shape[1:] == synthetic.shape[1:], \\\n",
    "            f\"Dimension mismatch: Original {X.shape[1:]}, Synthetic {synthetic.shape[1:]}\"\n",
    "\n",
    "        # Use only as many synthetic as needed\n",
    "        synthetic = synthetic[:n_needed]\n",
    "\n",
    "        # Concatenate\n",
    "        X_resampled = np.concatenate([X, synthetic], axis=0)\n",
    "        y_resampled = np.concatenate([y, [minority_class] * len(synthetic)], axis=0)\n",
    "        return X_resampled, y_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# Enhanced Dataset Class with Proper Encapsulation\n",
    "#============================================================\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, source, max_length=2000):\n",
    "        self.df = pl.read_parquet(source) if isinstance(source, str) else source\n",
    "        if 'orig_marker' in self.df.columns:\n",
    "            self.df = self.df.drop('orig_marker')\n",
    "        \n",
    "        # label_map = {\"Left\": 0, \"Right\": 1}  \n",
    "        # self.df = self.df.with_columns([\n",
    "        #     pl.col(\"marker\").replace(label_map).cast(pl.Int32).alias(\"marker\"),\n",
    "        #     pl.col(\"prev_marker\").replace(label_map).cast(pl.Int32).alias(\"prev_marker\")\n",
    "        # ])\n",
    "        \n",
    "        self.df = self.df.with_columns([\n",
    "            pl.col(\"marker\")\n",
    "            .cast(pl.Utf8)\n",
    "            .str.replace_all(\"Left\", \"0\")      # replace exact string \"Left\" with \"0\"\n",
    "            .str.replace_all(\"Right\", \"1\")     # replace exact string \"Right\" with \"1\"\n",
    "            .cast(pl.Int32)                      # now cast the string \"0\"/\"1\" -> int\n",
    "            .alias(\"marker\"),\n",
    "            \n",
    "            pl.col(\"prev_marker\")\n",
    "            .cast(pl.Utf8)\n",
    "            .str.replace_all(\"Left\", \"0\")\n",
    "            .str.replace_all(\"Right\", \"1\")\n",
    "            .cast(pl.Int32)\n",
    "            .alias(\"prev_marker\"),\n",
    "        ])\n",
    "        \n",
    "        self.event_ids = self.df['event_id'].unique().to_list()\n",
    "        self.max_length = max_length\n",
    "        # Keep time for sorting but exclude from features\n",
    "        self.feature_cols = [c for c in self.df.columns \n",
    "                           if c not in {'event_id', 'marker', 'time'}]\n",
    "        \n",
    "        print(\"Precomputing samples...\")\n",
    "        self._precompute_samples()\n",
    "        print(\"Computing class weights...\")\n",
    "        self._class_weights = self.compute_class_weights()\n",
    "    \n",
    "    @property\n",
    "    def class_weights(self):\n",
    "        # Expose the computed weights as a property.\n",
    "        return self._class_weights \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.event_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "    \n",
    "    def _precompute_samples(self):\n",
    "        self.samples = []\n",
    "        for event_id in tqdm(self.event_ids, desc='precomputing_samples'):\n",
    "            # Sort by time within each event!\n",
    "            event_data = self.df.filter(pl.col(\"event_id\") == event_id).sort(\"time\")\n",
    "            features = torch.tensor(\n",
    "                event_data.select(self.feature_cols).to_numpy(),\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "            features = self._pad_sequence(features)\n",
    "            \n",
    "            label = event_data['marker'][0]\n",
    "            self.samples.append((\n",
    "                torch.tensor(label, dtype=torch.float32), \n",
    "                features\n",
    "            ))\n",
    "    \n",
    "    def compute_class_weights(self):\n",
    "        \"\"\"\n",
    "        Compute inverse frequency weights based on the 'marker' column.\n",
    "        Assumes markers are \"Stimulus/A\" and \"Stimulus/P\".\n",
    "        \"\"\"\n",
    "        # Get unique combinations of event_id and marker.\n",
    "        unique_events = self.df.select([\"event_id\", \"marker\"]).unique()\n",
    "        \n",
    "        # Use value_counts on the \"marker\" column.\n",
    "        counts_df = unique_events[\"marker\"].value_counts()\n",
    "\n",
    "        # We'll use 'values' if it exists, otherwise 'marker'.\n",
    "        d = { (row.get(\"values\") or row.get(\"marker\")): row[\"count\"] \n",
    "            for row in counts_df.to_dicts() }\n",
    "        \n",
    "        weight_L = 1.0 / d.get(0, 1)\n",
    "        weight_R = 1.0 / d.get(1, 1)\n",
    "        return {\"Left\": weight_L, \"Right\": weight_R}\n",
    "   \n",
    "    def split_dataset(self, ratios=(0.7, 0.15, 0.15), seed=None):\n",
    "        \"\"\"\n",
    "        Splits the dataset into three EEGDataset instances for train, val, and test.\n",
    "        This method shuffles the event_ids and then partitions them based on the given ratios.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Copy and shuffle the event_ids\n",
    "        event_ids = self.event_ids.copy()\n",
    "        np.random.shuffle(event_ids)\n",
    "        total = len(event_ids)\n",
    "        \n",
    "        n_train = int(ratios[0] * total)\n",
    "        n_val   = int(ratios[1] * total)\n",
    "        \n",
    "        train_ids = event_ids[:n_train]\n",
    "        val_ids   = event_ids[n_train:n_train+n_val]\n",
    "        test_ids  = event_ids[n_train+n_val:]\n",
    "        \n",
    "        # Filter self.df for the selected event_ids\n",
    "        train_df = self.df.filter(pl.col(\"event_id\").is_in(train_ids))\n",
    "        val_df   = self.df.filter(pl.col(\"event_id\").is_in(val_ids))\n",
    "        test_df  = self.df.filter(pl.col(\"event_id\").is_in(test_ids))\n",
    "        \n",
    "        # Create new EEGDataset instances using the filtered data\n",
    "        train_set = EEGDataset(train_df, self.max_length)\n",
    "        val_set   = EEGDataset(val_df, self.max_length)\n",
    "        test_set  = EEGDataset(test_df, self.max_length)\n",
    "        \n",
    "        return train_set, val_set, test_set\n",
    "\n",
    "    def _pad_sequence(self, tensor):\n",
    "        # Pre-allocate tensor for maximum efficiency\n",
    "        padded = torch.zeros((self.max_length, tensor.size(1)), dtype=tensor.dtype)\n",
    "        length = min(tensor.size(0), self.max_length)\n",
    "        padded[:length] = tensor[:length]\n",
    "        return padded\n",
    "    \n",
    "    def rebalance_by_tsmote(self):\n",
    "        \"\"\"TSMOTE implementation for temporal EEG data\"\"\"\n",
    "        # Extract time-ordered features as 3D array (samples, timesteps, features)\n",
    "        X = np.stack([features.numpy() for _, features in self.samples])\n",
    "        y = np.array([label.item() for label, _ in self.samples])\n",
    "        \n",
    "        # Apply TSMOTE with temporal awareness\n",
    "        \n",
    "        \n",
    "        # Find the index of 'prev_marker' in the feature columns\n",
    "        prev_marker_idx = self.feature_cols.index('prev_marker')\n",
    "        \n",
    "        # Apply TSMOTE with the correct boolean column index\n",
    "        tsmote = TSMOTE(bool_cols=[prev_marker_idx])\n",
    "        X_res, y_res = tsmote.fit_resample(X, y)\n",
    "\n",
    "        # Generate synthetic temporal events\n",
    "        new_events = []\n",
    "        new_event_id = self.df['event_id'].max() + 1\n",
    "        time_base = np.arange(self.max_length)\n",
    "        original_schema = self.df.schema\n",
    "\n",
    "        # Create dtype conversion map\n",
    "        dtype_map = {\n",
    "            pl.Float64: np.float64,\n",
    "            pl.Float32: np.float32,\n",
    "            pl.Int64: np.int64,\n",
    "            pl.Int32: np.int32,\n",
    "            pl.Utf8: str,\n",
    "        }\n",
    "\n",
    "        # Process synthetic samples (original samples come first in X_res)\n",
    "        for features_3d, label in zip(X_res[len(self.samples):], y_res[len(self.samples):]):\n",
    "            event_data = {}\n",
    "\n",
    "            # Ensure columns are added in the original DataFrame's order\n",
    "            for col in self.df.columns:\n",
    "                if col == 'event_id':\n",
    "                    event_data[col] = [new_event_id] * self.max_length\n",
    "                elif col == 'marker':\n",
    "                    event_data[col] = [int(label)] * self.max_length  # Ensure label is integer\n",
    "                elif col == 'time':\n",
    "                    event_data[col] = time_base.copy().astype(np.int32)  # Match original time type\n",
    "                else:\n",
    "                    # Feature columns (excluding event_id, marker, time)\n",
    "                    if col not in self.feature_cols:\n",
    "                        continue  # Shouldn't happen as feature_cols covers all else\n",
    "                    col_idx = self.feature_cols.index(col)\n",
    "                    col_data = features_3d[:, col_idx]\n",
    "                    schema_type = original_schema[col]\n",
    "\n",
    "                    # Handle data types\n",
    "                    if isinstance(schema_type, pl.List):\n",
    "                        base_type = schema_type.inner\n",
    "                        target_type = dtype_map.get(type(base_type), np.float64)\n",
    "                    else:\n",
    "                        target_type = dtype_map.get(type(schema_type), np.float64)\n",
    "                    \n",
    "                    col_data = col_data.astype(target_type)\n",
    "                    \n",
    "                    # Maintain integer precision for Int columns (e.g., prev_marker)\n",
    "                    if schema_type in (pl.Int64, pl.Int32):\n",
    "                        col_data = np.round(col_data).astype(int)\n",
    "                    \n",
    "                    event_data[col] = col_data\n",
    "\n",
    "            # Create DataFrame with strict schema adherence\n",
    "            event_df = pl.DataFrame(event_data).cast(original_schema)\n",
    "            new_events.append(event_df)\n",
    "            new_event_id += 1\n",
    "\n",
    "        # Update dataset with synthetic temporal events\n",
    "        self.df = pl.concat([self.df, *new_events])\n",
    "        self.event_ids = self.df['event_id'].unique().to_list()\n",
    "        self._precompute_samples()\n",
    "        self._class_weights = self.compute_class_weights()\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_optimizer(model, optimizer_config):\n",
    "    \"\"\"\n",
    "    Create and return an optimizer based on optimizer_config.\n",
    "    \"\"\"\n",
    "    opt_type = optimizer_config.get('type', 'AdamW')\n",
    "    lr = optimizer_config.get('lr', 1e-3)\n",
    "    weight_decay = optimizer_config.get('weight_decay', 1e-2)\n",
    "    \n",
    "    if opt_type == 'AdamW':\n",
    "        return optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=lr, \n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "    elif opt_type == 'SGD':\n",
    "        momentum = optimizer_config.get('momentum', 0.9)\n",
    "        return optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    elif opt_type == 'RMSProp':\n",
    "        alpha = optimizer_config.get('alpha', 0.99)\n",
    "        return optim.RMSprop(model.parameters(), lr=lr, alpha=alpha, weight_decay=weight_decay)\n",
    "    # Add more optimizers if needed\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer type: {opt_type}\")\n",
    "\n",
    "def create_scheduler(optimizer, scheduler_config, total_epochs=None, steps_per_epoch=None):\n",
    "    \"\"\"\n",
    "    Create and return a scheduler based on scheduler_config.\n",
    "    Return (scheduler, requires_val_loss),\n",
    "    where requires_val_loss indicates if the scheduler needs validation loss (e.g. ReduceLROnPlateau).\n",
    "    \"\"\"\n",
    "    if not scheduler_config or scheduler_config.get('type') is None:\n",
    "        # No scheduler used\n",
    "        return None, False\n",
    "\n",
    "    sched_type = scheduler_config['type']\n",
    "\n",
    "    if sched_type == 'ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=scheduler_config.get('mode', 'min'),\n",
    "            factor=scheduler_config.get('factor', 0.1),\n",
    "            patience=scheduler_config.get('patience', 10),\n",
    "            threshold=scheduler_config.get('threshold', 0.0001),\n",
    "            cooldown=scheduler_config.get('cooldown', 0),\n",
    "            min_lr=scheduler_config.get('min_lr', 0),\n",
    "            verbose=scheduler_config.get('verbose', False)\n",
    "        )\n",
    "        return scheduler, True\n",
    "\n",
    "    elif sched_type == 'CosineAnnealingLR':\n",
    "        T_max = scheduler_config.get('T_max', 10)\n",
    "        eta_min = scheduler_config.get('eta_min', 0)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)\n",
    "        return scheduler, False\n",
    "\n",
    "    elif sched_type == 'CyclicLR':\n",
    "        base_lr = scheduler_config.get('base_lr', 1e-3)\n",
    "        max_lr = scheduler_config.get('max_lr', 1e-2)\n",
    "        step_size_up = scheduler_config.get('step_size_up', 2000)\n",
    "        mode = scheduler_config.get('mode', 'triangular')\n",
    "        scheduler = CyclicLR(\n",
    "            optimizer,\n",
    "            base_lr=base_lr,\n",
    "            max_lr=max_lr,\n",
    "            step_size_up=step_size_up,\n",
    "            mode=mode\n",
    "        )\n",
    "        return scheduler, False\n",
    "\n",
    "    elif sched_type == 'OneCycleLR':\n",
    "        # OneCycleLR requires total_steps or (epochs * steps_per_epoch).\n",
    "        # If not specified in config, try to compute from total_epochs and steps_per_epoch.\n",
    "        max_lr = scheduler_config.get('max_lr', 1e-2)\n",
    "        if 'total_steps' in scheduler_config:\n",
    "            total_steps = scheduler_config['total_steps']\n",
    "        else:\n",
    "            if total_epochs is None or steps_per_epoch is None:\n",
    "                raise ValueError(\n",
    "                    \"OneCycleLR requires either 'total_steps' in config \"\n",
    "                    \"or (total_epochs and steps_per_epoch) arguments.\"\n",
    "                )\n",
    "            total_steps = total_epochs * steps_per_epoch\n",
    "\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=max_lr,\n",
    "            total_steps=total_steps,\n",
    "            pct_start=scheduler_config.get('pct_start', 0.3),\n",
    "            anneal_strategy=scheduler_config.get('anneal_strategy', 'cos'),\n",
    "            cycle_momentum=scheduler_config.get('cycle_momentum', True),\n",
    "            base_momentum=scheduler_config.get('base_momentum', 0.85),\n",
    "            max_momentum=scheduler_config.get('max_momentum', 0.95),\n",
    "            div_factor=scheduler_config.get('div_factor', 25.0),\n",
    "            final_div_factor=scheduler_config.get('final_div_factor', 1e4)\n",
    "        )\n",
    "        return scheduler, False\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported scheduler type: {sched_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for variable-length EEG feature sequences.\n",
    "\n",
    "    Each sample is expected to be a tuple (label, feature), where:\n",
    "    - label is a scalar tensor (or 1D tensor) representing the class/target.\n",
    "    - feature is a tensor of shape (seq_len, num_channels), where seq_len may vary.\n",
    "\n",
    "    This function stacks labels and pads features along the time dimension so that\n",
    "    all sequences in the batch have the same length.\n",
    "    \"\"\"\n",
    "    # Unzip the batch into labels and features\n",
    "    labels, features = zip(*batch)\n",
    "    \n",
    "    labels = torch.stack(labels)\n",
    "    padded_features = pad_sequence(features, batch_first=True)\n",
    "    \n",
    "    return labels, padded_features\n",
    "\n",
    "\n",
    "def train_model(config, train_set, train_loader, val_loader, writer):\n",
    "    # -------------------- MODEL --------------------\n",
    "    model = EEGMobileNet(\n",
    "        in_channels=64,\n",
    "        num_classes=1,\n",
    "        dropout=config['dropout']\n",
    "    ).to(config['device'])\n",
    "    \n",
    "    # Log model architecture and config\n",
    "    writer.add_text(\"Model/Type\", f\"EEGMobileNet with dropout={config['dropout']}\")\n",
    "    writer.add_text(\"Model/Structure\", str(model))\n",
    "    writer.add_text(\"Training Config\", str(config))\n",
    "    \n",
    "    # ------------------ LOSS FUNCTION ------------------\n",
    "    pos_weight = torch.tensor([\n",
    "        train_set.class_weights['Left'] / train_set.class_weights['Right']\n",
    "    ]).to(config['device'])\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(weight=pos_weight)\n",
    "    \n",
    "    # ------------------- OPTIMIZER ---------------------\n",
    "    optimizer_config = config['optimizer']\n",
    "    # Inject global lr & weight_decay into optimizer_config\n",
    "    optimizer_config['lr'] = config['lr']\n",
    "    optimizer_config['weight_decay'] = config['weight_decay']\n",
    "    \n",
    "    optimizer = create_optimizer(model, optimizer_config)\n",
    "    \n",
    "    # ------------------- SCHEDULER ---------------------\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    scheduler_config = config.get('scheduler', {})\n",
    "    scheduler_config['factor'] = config['factor']\n",
    "    scheduler_config['patience'] = config['patience']\n",
    "    scheduler_config['cooldown'] = config['cooldown']\n",
    "    \n",
    "    scheduler, requires_val_loss = create_scheduler(\n",
    "        optimizer,\n",
    "        scheduler_config,\n",
    "        total_epochs=config['epochs'],\n",
    "        steps_per_epoch=steps_per_epoch\n",
    "    )\n",
    "    \n",
    "    # ------------------- WARMUP SCHEDULER ---------------\n",
    "    warmup_epochs = config.get('warmup_epochs', 0)\n",
    "    if warmup_epochs > 0:\n",
    "        warmup_scheduler = LambdaLR(\n",
    "            optimizer,\n",
    "            lambda epoch: min(1.0, (epoch + 1) / warmup_epochs)\n",
    "        )\n",
    "    else:\n",
    "        warmup_scheduler = None\n",
    "    \n",
    "    # -------------------- TRAINING LOOP --------------------\n",
    "    best_metric = -float('inf')\n",
    "    \n",
    "    for epoch in tqdm(range(config['epochs']), desc=\"Training\"):\n",
    "        # ---------- TRAIN ----------\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for labels, features in train_loader:\n",
    "            features = features.to(config['device']).float()\n",
    "            labels = labels.to(config['device']).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping (if specified)\n",
    "            if config.get('grad_clip') is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # ---------- VALIDATION ----------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for labels, features in val_loader:\n",
    "                features = features.to(config['device']).float()\n",
    "                labels = labels.to(config['device']).float()\n",
    "                \n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.sigmoid(outputs)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        predictions = (np.array(all_preds) > 0.5).astype(int)\n",
    "        \n",
    "        # ---------- METRICS ----------\n",
    "        accuracy = accuracy_score(all_labels, predictions)\n",
    "        precision = precision_score(all_labels, predictions)\n",
    "        recall = recall_score(all_labels, predictions)\n",
    "        f1 = f1_score(all_labels, predictions)\n",
    "        \n",
    "        # ---------- SCHEDULER UPDATE ----------\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if warmup_scheduler is not None and epoch < warmup_epochs:\n",
    "            warmup_scheduler.step()\n",
    "        else:\n",
    "            if scheduler is not None:\n",
    "                if requires_val_loss:\n",
    "                    # e.g. ReduceLROnPlateau\n",
    "                    scheduler.step(val_loss)\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "        \n",
    "        # ---------- LOGGING ----------\n",
    "        writer.add_scalar('LR', current_lr, epoch)\n",
    "        writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Val', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy', accuracy, epoch)\n",
    "        writer.add_scalar('Precision', precision, epoch)\n",
    "        writer.add_scalar('Recall', recall, epoch)\n",
    "        writer.add_scalar('F1', f1, epoch)\n",
    "        \n",
    "        # You can also combine them in a single dictionary\n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "        writer.add_scalars('Metrics', metrics, epoch)\n",
    "        \n",
    "        # ---------- SAVE BEST MODEL ----------\n",
    "        if accuracy > best_metric:\n",
    "            best_metric = accuracy\n",
    "            torch.save(model.state_dict(), f\"{config['log_dir']}/best_model.pth\")\n",
    "    \n",
    "    writer.close()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- OPTIMIZERS CONFIGS ----------\n",
    "\n",
    "adamw_config = {\n",
    "    'type': 'AdamW',\n",
    "    'weight_decay': 1e-4\n",
    "}\n",
    "\n",
    "\n",
    "sgd_config = {\n",
    "    'type': 'SGD',\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 1e-4\n",
    "}\n",
    "\n",
    "rmsprop_config = {\n",
    "    'type': 'RMSProp',\n",
    "    'alpha': 0.99,\n",
    "    'weight_decay': 1e-5\n",
    "}\n",
    "\n",
    "# ---------- SCHEDULERS CONFIGS ----------\n",
    "\n",
    "reduce_on_plateau_config = {\n",
    "    'type': 'ReduceLROnPlateau',\n",
    "    'mode': 'min',\n",
    "    'factor': 0.1,\n",
    "    'patience': 10,\n",
    "    'threshold': 0.0001,\n",
    "    'cooldown': 10,\n",
    "    'min_lr': 1e-8\n",
    "}\n",
    "\n",
    "cosine_annealing_config = {\n",
    "    'type': 'CosineAnnealingLR',\n",
    "    'T_max': 10,       # Number of iterations (e.g., epochs) to restart from max LR\n",
    "    'eta_min': 0       # Minimum learning rate\n",
    "}\n",
    "\n",
    "cycliclr_config = {\n",
    "    'type': 'CyclicLR',\n",
    "    'base_lr': 1e-4,     # Lower learning rate bound\n",
    "    'max_lr': 1e-3,      # Upper learning rate bound\n",
    "    'step_size_up': 2000,# Number of training iterations (batches) in the increasing half of a cycle\n",
    "    'mode': 'triangular' # 'triangular', 'triangular2', or 'exp_range'\n",
    "}\n",
    "\n",
    "onecyclelr_config = {\n",
    "    'type': 'OneCycleLR',\n",
    "    'max_lr': 1e-3,\n",
    "    'pct_start': 0.3,\n",
    "    'anneal_strategy': 'cos',   # 'cos' or 'linear'\n",
    "    'cycle_momentum': True,\n",
    "    'base_momentum': 0.85,\n",
    "    'max_momentum': 0.95,\n",
    "    'div_factor': 25.0,\n",
    "    'final_div_factor': 1e4\n",
    "    # 'total_steps':  ...  # Provide explicitly OR use (epochs * steps_per_epoch) if left out\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('study.pkl', 'rb') as f:\n",
    "    config_study = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.00014782765350118218,\n",
       " 'weight_decay': 4.39923324740438e-06,\n",
       " 'dropout': 0.44201278804845473,\n",
       " 'factor': 0.7470511809208503,\n",
       " 'patience': 5,\n",
       " 'cooldown': 17}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data_path': '/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet',\n",
    "    'split_ratios': (0.7, 0.15, 0.15),\n",
    "    'batch_size': 32,\n",
    "    'dropout': config_study['dropout'],\n",
    "    'epochs': 300,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'log_dir': './runs/CNN',\n",
    "\n",
    "    # <<< Global LR and Weight Decay here >>>\n",
    "    'lr': config_study['lr'],\n",
    "    'weight_decay': config_study['weight_decay'],\n",
    "    'factor': config_study['factor'],\n",
    "    'patience': config_study['patience'],\n",
    "    'cooldown': config_study['cooldown'],\n",
    "    \n",
    "    # Optimizer config (without lr/weight_decay)\n",
    "    'optimizer': adamw_config,\n",
    "\n",
    "    # Scheduler config\n",
    "    'scheduler': reduce_on_plateau_config,\n",
    "\n",
    "    'warmup_epochs': 10,\n",
    "    'grad_clip': None\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Replace the hardcoded scheduler config with Optuna parameters\n",
    "config['scheduler'] = {\n",
    "    'type': 'ReduceLROnPlateau',\n",
    "    'mode': 'min',\n",
    "    'factor': config_study['factor'],      # From Optuna\n",
    "    'patience': config_study['patience'],  # From Optuna\n",
    "    'cooldown': config_study['cooldown'],  # From Optuna\n",
    "    'min_lr': 1e-8,\n",
    "    'threshold': 0.0001,\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #============================================================\n",
    "# # Training Pipeline\n",
    "# #============================================================\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# # Initialize dataset\n",
    "# print(\"Creating full dataset...\")\n",
    "# full_dataset = EEGDataset(config['data_path'])\n",
    "\n",
    "# print(\"Splitting the dataset...\")\n",
    "# # Split dataset\n",
    "# train_set, val_set, test_set = full_dataset.split_dataset(\n",
    "#     ratios=config['split_ratios']\n",
    "# )\n",
    "\n",
    "# del full_dataset\n",
    "\n",
    "# len_dataset = len(train_set)\n",
    "# sample = train_set[0]\n",
    "# label_shape = sample[0].shape\n",
    "# feature_shape = sample[1].shape\n",
    "\n",
    "# print(f\"unbalanced train dataset shape: ({len_dataset}, [labels: {label_shape}, features: {list(feature_shape)}])\")\n",
    "\n",
    "# # # Balance training set\n",
    "# # print(\"Applying SMOTE to train dataset...\")\n",
    "# # train_set.rebalance_by_tsmote()\n",
    "\n",
    "# len_dataset = len(train_set)\n",
    "# sample = train_set[0]\n",
    "# label_shape = sample[0].shape\n",
    "# feature_shape = sample[1].shape\n",
    "\n",
    "# print(f\"balanced train dataset shape: ({len_dataset}, [labels: {label_shape}, features: {list(feature_shape)}])\")\n",
    "\n",
    "# torch.save(train_set, 'train_set.pt')\n",
    "# torch.save(val_set, 'val_set.pt')\n",
    "# torch.save(test_set, 'test_set.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1788725/2155104737.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_set_smol.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val_set.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_set.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m                             \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m                         )\n\u001b[1;32m   1358\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m                 return _load(\n\u001b[0m\u001b[1;32m   1361\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m                     \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m                     \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1844\u001b[0m     \u001b[0;31m# Needed for tensors where storage device and rebuild tensor device are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;31m# not connected (wrapper subclasses and tensors rebuilt using numpy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/polars/dataframe/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, state)\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_set = torch.load('train_set_smol.pt', weights_only=False)\n",
    "val_set = torch.load('val_set.pt', weights_only=False)\n",
    "test_set = torch.load('test_set.pt', weights_only=False)\n",
    "\n",
    "\n",
    "generator = torch.Generator().manual_seed(69)  # Set seed\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    generator=generator,  # Add this line\n",
    "    num_workers=12,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(val_set, batch_size=config['batch_size'], collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=config['batch_size'], collate_fn=collate_fn)\n",
    "\n",
    "len_dataset = len(train_set)\n",
    "sample = train_set[0]\n",
    "label_shape = sample[0].shape\n",
    "feature_shape = sample[1].shape\n",
    "\n",
    "print(f\"train dataset shape: ({len_dataset}, [labels: {label_shape}, features: {list(feature_shape)}])\")\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "writer = SummaryWriter(log_dir=config['log_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owner/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac63d81b8b0443ca3278628ad27f2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training\n",
    "trained_model = train_model(config, train_set, train_loader, val_loader, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1788725/1960808482.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f\"{config['log_dir']}/best_model.pth\", map_location=config['device'])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330529caeb8c4d4c9fb1a49ba4df6640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "pos_weight = torch.tensor([\n",
    "    train_set.class_weights['Left'] / train_set.class_weights['Right']\n",
    "]).to(config['device'])\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "epoch = 1\n",
    "# Assuming model, criterion, test_loader, device, writer, and epoch are already defined\n",
    "# Instantiate your model\n",
    "best_model = EEGMobileNet()  # Adjust parameters as needed\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load(f\"{config['log_dir']}/best_model.pth\", map_location=config['device'])\n",
    "best_model.load_state_dict(state_dict)\n",
    "\n",
    "# Move model to the correct device\n",
    "best_model = best_model.to(config['device'])\n",
    "\n",
    "# Set model to evaluation mode\n",
    "best_model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "all_test_markers = []\n",
    "all_test_predictions = []\n",
    "with torch.no_grad():\n",
    "    for markers, features in tqdm(test_loader):\n",
    "        features = features.to(config['device'])\n",
    "        markers = markers.to(config['device'])\n",
    "\n",
    "        outputs = best_model(features)\n",
    "        loss = criterion(outputs, markers)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Collect markers and predictions for metrics calculation\n",
    "        all_test_markers.extend(markers.cpu().numpy().flatten())\n",
    "        all_test_predictions.extend(torch.sigmoid(outputs).cpu().numpy().flatten())\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "# Calculate test metrics\n",
    "test_accuracy = accuracy_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_precision = precision_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_recall = recall_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_f1 = f1_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_roc_auc = roc_auc_score(all_test_markers, all_test_predictions)\n",
    "\n",
    "# Log test metrics to TensorBoard\n",
    "writer.add_scalar('Metrics/test_accuracy', test_accuracy, epoch)\n",
    "writer.add_scalar('Metrics/test_precision', test_precision, epoch)\n",
    "writer.add_scalar('Metrics/test_recall', test_recall, epoch)\n",
    "writer.add_scalar('Metrics/test_f1', test_f1, epoch)\n",
    "writer.add_scalar('Metrics/test_roc_auc', test_roc_auc, epoch)\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test_accuracy=0.5323741007194245\n",
      "test_precision=0.5176991150442478\n",
      "test_recall=0.5763546798029556\n",
      "test_f1=0.5454545454545454\n",
      "test_roc_auc=np.float64(0.5681368261129782)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "{test_accuracy=}\n",
    "{test_precision=}\n",
    "{test_recall=}\n",
    "{test_f1=}\n",
    "{test_roc_auc=}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649a32786ba34a848b843592e61d1afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_threshold=np.float64(0.2599999999999999)\n",
      "best_f1=0.657439446366782\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "best_threshold = 0.0\n",
    "best_f1 = 0.0\n",
    "thresholds = np.arange(0.1, 1.0, 0.01)\n",
    "\n",
    "for threshold in tqdm(thresholds):\n",
    "    binary_predictions = (all_test_predictions > threshold).astype(int)\n",
    "    current_recall = f1_score(all_test_markers, binary_predictions)\n",
    "\n",
    "    if current_recall > best_f1:\n",
    "        best_f1 = current_recall\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"{best_threshold=}\")\n",
    "print(f\"{best_f1=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e32beefde04a5bb6e72e93cfe7f28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_threshold=np.float64(0.1)\n",
      "best_recall=0.9852216748768473\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "import numpy as np\n",
    "best_threshold = 0.1\n",
    "best_recall = 0.0\n",
    "thresholds = np.arange(0.1, 1.0, 0.01)\n",
    "\n",
    "for threshold in tqdm(thresholds):\n",
    "    binary_predictions = (all_test_predictions > threshold).astype(int)\n",
    "    current_recall = recall_score(all_test_markers, binary_predictions)\n",
    "\n",
    "    if current_recall > best_recall:\n",
    "        best_recall = current_recall\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"{best_threshold=}\")\n",
    "print(f\"{best_recall=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
