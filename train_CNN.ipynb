{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import polars as pl\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "# Set the seed for Python's built-in random module\n",
    "random.seed(69)\n",
    "\n",
    "# Set the seed for NumPy's random number generator\n",
    "np.random.seed(69)\n",
    "\n",
    "# Set the seed for PyTorch's random number generators\n",
    "torch.manual_seed(69)\n",
    "torch.cuda.manual_seed(69)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5_544_008, 68)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>event_id</th><th>orig_marker</th><th>time</th><th>Fp1</th><th>Fpz</th><th>Fp2</th><th>F7</th><th>F3</th><th>Fz</th><th>F4</th><th>F8</th><th>FC5</th><th>FC1</th><th>FC2</th><th>FC6</th><th>M1</th><th>T7</th><th>C3</th><th>Cz</th><th>C4</th><th>T8</th><th>M2</th><th>CP5</th><th>CP1</th><th>CP2</th><th>CP6</th><th>P7</th><th>P3</th><th>Pz</th><th>P4</th><th>P8</th><th>POz</th><th>O1</th><th>O2</th><th>AF7</th><th>AF3</th><th>AF4</th><th>AF8</th><th>F5</th><th>F1</th><th>F2</th><th>F6</th><th>FC3</th><th>FCz</th><th>FC4</th><th>C5</th><th>C1</th><th>C2</th><th>C6</th><th>CP3</th><th>CP4</th><th>P5</th><th>P1</th><th>P2</th><th>P6</th><th>PO5</th><th>PO3</th><th>PO4</th><th>PO6</th><th>FT7</th><th>FT8</th><th>TP7</th><th>TP8</th><th>PO7</th><th>PO8</th><th>Oz</th><th>marker</th><th>prev_marker</th></tr><tr><td>i64</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>&quot;Stimulus/A&quot;</td><td>16.458</td><td>17.170373</td><td>13.906436</td><td>21.251943</td><td>0.04914</td><td>6.744807</td><td>-11.924581</td><td>-0.963751</td><td>0.169659</td><td>5.941923</td><td>10.144309</td><td>-5.712161</td><td>0.856205</td><td>-13.592378</td><td>5.55773</td><td>5.277309</td><td>4.191759</td><td>11.859419</td><td>-5.135765</td><td>-22.437368</td><td>-15.228711</td><td>-12.038605</td><td>-0.75959</td><td>-1.466066</td><td>-21.842841</td><td>13.373856</td><td>-9.149643</td><td>0.330315</td><td>4.335297</td><td>-5.321761</td><td>16.798603</td><td>12.262668</td><td>30.010249</td><td>16.320185</td><td>7.076928</td><td>21.956538</td><td>11.408192</td><td>-0.31748</td><td>2.813686</td><td>6.930768</td><td>4.998787</td><td>0.411273</td><td>4.038051</td><td>10.464517</td><td>5.762318</td><td>1.730282</td><td>-0.464392</td><td>-5.006052</td><td>-8.962569</td><td>13.681179</td><td>-9.582485</td><td>-17.106364</td><td>13.691397</td><td>12.681116</td><td>11.514871</td><td>25.205208</td><td>14.079291</td><td>11.501145</td><td>1.636257</td><td>5.387385</td><td>6.44589</td><td>14.401535</td><td>13.353615</td><td>8.559186</td><td>&quot;Right&quot;</td><td>&quot;Left&quot;</td></tr><tr><td>0</td><td>&quot;Stimulus/A&quot;</td><td>16.46</td><td>18.195809</td><td>14.819681</td><td>21.679231</td><td>1.756323</td><td>7.864059</td><td>-11.064782</td><td>-0.080928</td><td>0.735955</td><td>6.969142</td><td>10.664451</td><td>-4.989593</td><td>1.752117</td><td>-12.19725</td><td>7.671375</td><td>5.210584</td><td>4.463182</td><td>12.858593</td><td>-2.887153</td><td>-20.226918</td><td>-15.193608</td><td>-11.675535</td><td>-0.408616</td><td>-0.500061</td><td>-21.86162</td><td>12.080171</td><td>-9.111754</td><td>0.608741</td><td>5.29489</td><td>-6.487817</td><td>14.594936</td><td>11.257213</td><td>30.903666</td><td>16.686429</td><td>7.859665</td><td>21.840979</td><td>13.346964</td><td>0.369015</td><td>3.588863</td><td>7.620327</td><td>5.759861</td><td>1.229387</td><td>4.697295</td><td>10.818292</td><td>6.213349</td><td>2.547987</td><td>0.577626</td><td>-4.975261</td><td>-8.451353</td><td>12.374722</td><td>-9.802303</td><td>-17.080294</td><td>13.566208</td><td>10.967429</td><td>9.950173</td><td>24.596992</td><td>13.09233</td><td>12.683138</td><td>3.16226</td><td>6.085818</td><td>8.821207</td><td>12.746989</td><td>12.518362</td><td>5.916264</td><td>&quot;Right&quot;</td><td>&quot;Left&quot;</td></tr><tr><td>0</td><td>&quot;Stimulus/A&quot;</td><td>16.462</td><td>19.208807</td><td>15.713978</td><td>22.097658</td><td>3.589434</td><td>8.999892</td><td>-10.254515</td><td>0.710523</td><td>1.346892</td><td>7.978441</td><td>11.145332</td><td>-4.357971</td><td>2.793821</td><td>-10.682037</td><td>10.741887</td><td>5.124484</td><td>4.718824</td><td>13.820216</td><td>-1.052706</td><td>-18.077094</td><td>-15.224916</td><td>-11.403306</td><td>-0.094469</td><td>0.433875</td><td>-22.097145</td><td>10.625836</td><td>-9.156111</td><td>0.810305</td><td>6.464841</td><td>-7.659126</td><td>12.052806</td><td>10.762715</td><td>31.842351</td><td>17.043632</td><td>8.56405</td><td>22.021347</td><td>15.409027</td><td>1.007149</td><td>4.277586</td><td>8.126715</td><td>6.484071</td><td>2.037813</td><td>5.304125</td><td>11.000511</td><td>6.634963</td><td>3.363777</td><td>1.715886</td><td>-4.994935</td><td>-7.956395</td><td>10.896297</td><td>-10.095891</td><td>-17.036856</td><td>13.747517</td><td>8.995882</td><td>8.208103</td><td>24.074887</td><td>12.511693</td><td>13.800554</td><td>4.745254</td><td>6.843331</td><td>11.128809</td><td>10.785815</td><td>12.08669</td><td>3.017775</td><td>&quot;Right&quot;</td><td>&quot;Left&quot;</td></tr><tr><td>0</td><td>&quot;Stimulus/A&quot;</td><td>16.464</td><td>20.14623</td><td>16.545905</td><td>22.489029</td><td>5.424029</td><td>10.097336</td><td>-9.551693</td><td>1.345182</td><td>1.98236</td><td>8.906685</td><td>11.551465</td><td>-3.870977</td><td>3.916049</td><td>-9.132715</td><td>14.534485</td><td>5.035828</td><td>4.941408</td><td>14.682348</td><td>0.284907</td><td>-16.081929</td><td>-15.290949</td><td>-11.246743</td><td>0.171751</td><td>1.3134</td><td>-22.523215</td><td>9.146861</td><td>-9.263138</td><td>0.941635</td><td>7.810634</td><td>-8.737122</td><td>9.412096</td><td>10.861548</td><td>32.818187</td><td>17.386197</td><td>9.129862</td><td>22.509357</td><td>17.445482</td><td>1.540144</td><td>4.819262</td><td>8.413686</td><td>7.13795</td><td>2.780287</td><td>5.815162</td><td>11.045499</td><td>6.992499</td><td>4.122729</td><td>2.89388</td><td>-5.057835</td><td>-7.497601</td><td>9.387137</td><td>-10.433307</td><td>-16.954144</td><td>14.257214</td><td>6.958701</td><td>6.45094</td><td>23.693334</td><td>12.429921</td><td>14.822983</td><td>6.309243</td><td>7.631156</td><td>13.27648</td><td>8.708844</td><td>12.141982</td><td>0.15928</td><td>&quot;Right&quot;</td><td>&quot;Left&quot;</td></tr><tr><td>0</td><td>&quot;Stimulus/A&quot;</td><td>16.466</td><td>20.946355</td><td>17.271059</td><td>22.827998</td><td>7.140907</td><td>11.096112</td><td>-9.006329</td><td>1.771387</td><td>2.620716</td><td>9.695722</td><td>11.849742</td><td>-3.56821</td><td>5.041098</td><td>-7.624744</td><td>18.712668</td><td>4.959592</td><td>5.110745</td><td>15.390717</td><td>1.107496</td><td>-14.316858</td><td>-15.355903</td><td>-11.212635</td><td>0.38525</td><td>2.12328</td><td>-23.090999</td><td>7.784682</td><td>-9.401254</td><td>1.018174</td><td>9.261804</td><td>-9.633229</td><td>6.924365</td><td>11.55411</td><td>33.814531</td><td>17.705811</td><td>9.512561</td><td>23.27303</td><td>19.314331</td><td>1.917181</td><td>5.166991</td><td>8.46995</td><td>7.688362</td><td>3.401579</td><td>6.191096</td><td>11.003568</td><td>7.254085</td><td>4.772919</td><td>4.046689</td><td>-5.150528</td><td>-7.09336</td><td>7.995443</td><td>-10.775587</td><td>-16.813751</td><td>15.07551</td><td>5.057434</td><td>4.844601</td><td>23.494136</td><td>12.871215</td><td>15.72843</td><td>7.780457</td><td>8.422234</td><td>15.192707</td><td>6.723113</td><td>12.701932</td><td>-2.368745</td><td>&quot;Right&quot;</td><td>&quot;Left&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2771</td><td>&quot;Stimulus/A&quot;</td><td>956.836</td><td>-18.873559</td><td>-24.227101</td><td>-13.486943</td><td>-0.403397</td><td>-11.98354</td><td>-16.557347</td><td>-17.518873</td><td>-14.536315</td><td>-6.105563</td><td>-10.793705</td><td>-4.853747</td><td>-7.456738</td><td>10.414136</td><td>-2.954412</td><td>-5.026366</td><td>5.687796</td><td>-2.821944</td><td>3.946126</td><td>4.366952</td><td>-0.86041</td><td>-2.708516</td><td>3.370458</td><td>7.267404</td><td>2.82558</td><td>2.815112</td><td>-1.849568</td><td>9.221112</td><td>7.698055</td><td>2.700318</td><td>3.765467</td><td>4.554788</td><td>6.173636</td><td>-7.883982</td><td>-18.15556</td><td>-1.624224</td><td>-10.617139</td><td>-18.317587</td><td>-17.6747</td><td>-16.561863</td><td>-4.705041</td><td>-4.589931</td><td>1.813413</td><td>-4.839258</td><td>-4.689196</td><td>0.565952</td><td>-2.825098</td><td>-4.87889</td><td>4.368522</td><td>-1.258749</td><td>4.727739</td><td>7.158111</td><td>5.224539</td><td>2.009805</td><td>2.215446</td><td>9.938263</td><td>10.332691</td><td>0.287628</td><td>-6.543679</td><td>5.453187</td><td>0.753634</td><td>3.273086</td><td>10.787134</td><td>5.02536</td><td>&quot;Left&quot;</td><td>&quot;Right&quot;</td></tr><tr><td>2771</td><td>&quot;Stimulus/A&quot;</td><td>956.838</td><td>-18.704103</td><td>-24.969565</td><td>-14.585048</td><td>-1.047141</td><td>-11.943701</td><td>-16.45773</td><td>-17.959561</td><td>-15.664664</td><td>-7.082881</td><td>-11.007107</td><td>-5.073525</td><td>-8.186207</td><td>8.59455</td><td>-4.468591</td><td>-5.374392</td><td>5.653907</td><td>-2.543513</td><td>2.916711</td><td>1.86508</td><td>-2.226537</td><td>-2.766756</td><td>3.701948</td><td>7.189953</td><td>0.664609</td><td>2.01553</td><td>-1.72148</td><td>9.666301</td><td>7.382395</td><td>2.65323</td><td>2.507406</td><td>4.086716</td><td>5.953678</td><td>-7.897042</td><td>-18.654489</td><td>-2.907517</td><td>-10.716993</td><td>-18.135785</td><td>-18.222283</td><td>-17.620759</td><td>-5.103536</td><td>-4.95171</td><td>1.615306</td><td>-5.864539</td><td>-4.802275</td><td>0.860669</td><td>-3.010969</td><td>-5.317478</td><td>4.972083</td><td>-2.620815</td><td>4.514949</td><td>7.682226</td><td>5.731373</td><td>0.435993</td><td>0.715036</td><td>10.155613</td><td>10.351784</td><td>-0.944356</td><td>-7.749362</td><td>3.527842</td><td>-0.227262</td><td>1.671753</td><td>10.779771</td><td>4.207392</td><td>&quot;Left&quot;</td><td>&quot;Right&quot;</td></tr><tr><td>2771</td><td>&quot;Stimulus/A&quot;</td><td>956.84</td><td>-18.061667</td><td>-25.302962</td><td>-15.126122</td><td>-1.543149</td><td>-11.638041</td><td>-16.168372</td><td>-18.234343</td><td>-16.689475</td><td>-7.844465</td><td>-11.080084</td><td>-5.22795</td><td>-8.864504</td><td>6.983161</td><td>-5.717354</td><td>-5.553369</td><td>5.590639</td><td>-2.268625</td><td>1.711394</td><td>-0.604882</td><td>-3.276609</td><td>-2.707251</td><td>3.97878</td><td>6.988798</td><td>-1.132993</td><td>1.500356</td><td>-1.638895</td><td>10.013091</td><td>6.84536</td><td>2.529954</td><td>1.345095</td><td>3.459218</td><td>5.952675</td><td>-7.653053</td><td>-18.907577</td><td>-3.972871</td><td>-10.577918</td><td>-17.735399</td><td>-18.616241</td><td>-18.516636</td><td>-5.395328</td><td>-5.275623</td><td>1.45044</td><td>-6.564501</td><td>-4.839488</td><td>1.135505</td><td>-3.189045</td><td>-5.473588</td><td>5.565337</td><td>-3.532208</td><td>4.463409</td><td>8.272403</td><td>6.121259</td><td>-0.84287</td><td>-0.486312</td><td>10.264764</td><td>10.23355</td><td>-2.014752</td><td>-9.047787</td><td>1.966788</td><td>-1.368454</td><td>0.333598</td><td>10.611117</td><td>3.436481</td><td>&quot;Left&quot;</td><td>&quot;Right&quot;</td></tr><tr><td>2771</td><td>&quot;Stimulus/A&quot;</td><td>956.842</td><td>-17.023192</td><td>-25.220666</td><td>-15.079919</td><td>-1.852834</td><td>-11.10314</td><td>-15.743369</td><td>-18.338312</td><td>-17.531294</td><td>-8.309149</td><td>-11.006394</td><td>-5.293303</td><td>-9.428791</td><td>5.715629</td><td>-6.592615</td><td>-5.547356</td><td>5.501853</td><td>-2.008694</td><td>0.434555</td><td>-2.88224</td><td>-3.923841</td><td>-2.530573</td><td>4.190703</td><td>6.70415</td><td>-2.430261</td><td>1.293626</td><td>-1.613443</td><td>10.266805</td><td>6.137341</td><td>2.336787</td><td>0.363123</td><td>2.739203</td><td>6.169374</td><td>-7.181344</td><td>-18.903561</td><td>-4.734289</td><td>-10.219783</td><td>-17.170485</td><td>-18.840451</td><td>-19.18934</td><td>-5.546836</td><td>-5.528552</td><td>1.357458</td><td>-6.879141</td><td>-4.784823</td><td>1.373481</td><td>-3.355288</td><td>-5.336791</td><td>6.114987</td><td>-3.940727</td><td>4.575404</td><td>8.896693</td><td>6.400383</td><td>-1.731706</td><td>-1.306744</td><td>10.309981</td><td>10.048173</td><td>-2.819302</td><td>-10.318089</td><td>0.896298</td><td>-2.548241</td><td>-0.638066</td><td>10.358187</td><td>2.779875</td><td>&quot;Left&quot;</td><td>&quot;Right&quot;</td></tr><tr><td>2771</td><td>&quot;Stimulus/A&quot;</td><td>956.844</td><td>-15.701089</td><td>-24.752977</td><td>-14.476695</td><td>-1.950724</td><td>-10.3944</td><td>-15.239857</td><td>-18.279968</td><td>-18.132181</td><td>-8.422661</td><td>-10.790637</td><td>-5.252124</td><td>-9.825349</td><td>4.897406</td><td>-7.017004</td><td>-5.353847</td><td>5.392925</td><td>-1.770672</td><td>-0.797842</td><td>-4.821561</td><td>-4.115736</td><td>-2.2452</td><td>4.333632</td><td>6.381522</td><td>-3.137293</td><td>1.397751</td><td>-1.646783</td><td>10.438867</td><td>5.327027</td><td>2.093779</td><td>-0.366215</td><td>2.004391</td><td>6.583987</td><td>-6.526273</td><td>-18.653626</td><td>-5.137994</td><td>-9.679311</td><td>-16.501842</td><td>-18.890446</td><td>-19.601342</td><td>-5.534951</td><td>-5.680605</td><td>1.371231</td><td>-6.782775</td><td>-4.629448</td><td>1.566671</td><td>-3.503027</td><td>-4.91867</td><td>6.590366</td><td>-3.842274</td><td>4.84218</td><td>9.515511</td><td>6.577467</td><td>-2.173789</td><td>-1.7023</td><td>10.333644</td><td>9.864492</td><td>-3.275397</td><td>-11.438203</td><td>0.399305</td><td>-3.641922</td><td>-1.177381</td><td>10.09657</td><td>2.291098</td><td>&quot;Left&quot;</td><td>&quot;Right&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5_544_008, 68)\n",
       "┌──────────┬─────────────┬─────────┬────────────┬───┬───────────┬───────────┬────────┬─────────────┐\n",
       "│ event_id ┆ orig_marker ┆ time    ┆ Fp1        ┆ … ┆ PO8       ┆ Oz        ┆ marker ┆ prev_marker │\n",
       "│ ---      ┆ ---         ┆ ---     ┆ ---        ┆   ┆ ---       ┆ ---       ┆ ---    ┆ ---         │\n",
       "│ i64      ┆ str         ┆ f64     ┆ f64        ┆   ┆ f64       ┆ f64       ┆ str    ┆ str         │\n",
       "╞══════════╪═════════════╪═════════╪════════════╪═══╪═══════════╪═══════════╪════════╪═════════════╡\n",
       "│ 0        ┆ Stimulus/A  ┆ 16.458  ┆ 17.170373  ┆ … ┆ 13.353615 ┆ 8.559186  ┆ Right  ┆ Left        │\n",
       "│ 0        ┆ Stimulus/A  ┆ 16.46   ┆ 18.195809  ┆ … ┆ 12.518362 ┆ 5.916264  ┆ Right  ┆ Left        │\n",
       "│ 0        ┆ Stimulus/A  ┆ 16.462  ┆ 19.208807  ┆ … ┆ 12.08669  ┆ 3.017775  ┆ Right  ┆ Left        │\n",
       "│ 0        ┆ Stimulus/A  ┆ 16.464  ┆ 20.14623   ┆ … ┆ 12.141982 ┆ 0.15928   ┆ Right  ┆ Left        │\n",
       "│ 0        ┆ Stimulus/A  ┆ 16.466  ┆ 20.946355  ┆ … ┆ 12.701932 ┆ -2.368745 ┆ Right  ┆ Left        │\n",
       "│ …        ┆ …           ┆ …       ┆ …          ┆ … ┆ …         ┆ …         ┆ …      ┆ …           │\n",
       "│ 2771     ┆ Stimulus/A  ┆ 956.836 ┆ -18.873559 ┆ … ┆ 10.787134 ┆ 5.02536   ┆ Left   ┆ Right       │\n",
       "│ 2771     ┆ Stimulus/A  ┆ 956.838 ┆ -18.704103 ┆ … ┆ 10.779771 ┆ 4.207392  ┆ Left   ┆ Right       │\n",
       "│ 2771     ┆ Stimulus/A  ┆ 956.84  ┆ -18.061667 ┆ … ┆ 10.611117 ┆ 3.436481  ┆ Left   ┆ Right       │\n",
       "│ 2771     ┆ Stimulus/A  ┆ 956.842 ┆ -17.023192 ┆ … ┆ 10.358187 ┆ 2.779875  ┆ Left   ┆ Right       │\n",
       "│ 2771     ┆ Stimulus/A  ┆ 956.844 ┆ -15.701089 ┆ … ┆ 10.09657  ┆ 2.291098  ┆ Left   ┆ Right       │\n",
       "└──────────┴─────────────┴─────────┴────────────┴───┴───────────┴───────────┴────────┴─────────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.read_parquet('/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet')\\\n",
    "    # ['marker'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #============================================================\n",
    "# # Model Architecture\n",
    "# #============================================================\n",
    "# class EEGDSConv(nn.Module):\n",
    "#     def __init__(self, dropout=0.5):\n",
    "#         super().__init__()\n",
    "#         self.block = nn.Sequential(\n",
    "#             nn.Conv1d(64, 64, 15, padding='same', groups=64),\n",
    "#             nn.Conv1d(64, 16, 1),\n",
    "#             nn.BatchNorm1d(16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool1d(4),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Conv1d(16, 16, 7, padding='same', groups=16),\n",
    "#             nn.Conv1d(16, 8, 1),\n",
    "#             nn.BatchNorm1d(8),\n",
    "#             nn.ReLU(),\n",
    "#             nn.AdaptiveAvgPool1d(1),\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(8, 1)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = x.permute(0, 2, 1)\n",
    "#         return self.block(x).squeeze(-1)  # Squeeze last dimension to match target shape\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class EEGMobileNet(nn.Module):\n",
    "    def __init__(self, in_channels=64, num_classes=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # Initial Conv\n",
    "            nn.Conv1d(in_channels, 32, kernel_size=15, stride=2, padding=7),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),  # ← Insert dropout here\n",
    "\n",
    "            # Depthwise\n",
    "            nn.Conv1d(32, 32, kernel_size=3, stride=1, padding=1, groups=32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Pointwise\n",
    "            nn.Conv1d(32, 64, kernel_size=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),  # ← Insert dropout here\n",
    "\n",
    "            # Another Depthwise Separable block\n",
    "            nn.Conv1d(64, 64, kernel_size=3, stride=2, padding=1, groups=64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(64, 128, kernel_size=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),  # ← Insert dropout here\n",
    "\n",
    "            # Global Average Pool\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # your original transpose\n",
    "        return self.model(x).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tslearn.neighbors import KNeighborsTimeSeries\n",
    "from numba import njit\n",
    "\n",
    "# Standalone JIT-compiled function outside the class\n",
    "@njit(fastmath=True)\n",
    "def fast_interpolate(original, neighbor, alpha):\n",
    "    \"\"\"Numba-accelerated linear interpolation\"\"\"\n",
    "    return (1 - alpha) * original + alpha * neighbor\n",
    "\n",
    "class TSMOTE:\n",
    "    def __init__(self, n_neighbors=3, time_slices=10):  # Keep original 2000 timesteps\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.time_slices = time_slices\n",
    "        self.slice_size = 200  # 2000/10=200\n",
    "\n",
    "    def _slice_time_series(self, X):\n",
    "        \"\"\"Split into time slices while maintaining full series structure\"\"\"\n",
    "        return X.reshape(X.shape[0], self.time_slices, self.slice_size, X.shape[2])\n",
    "\n",
    "    def _reconstruct_full_series(self, synthetic_slices):\n",
    "        \"\"\"Combine synthetic slices into full-length time series\"\"\"\n",
    "        return synthetic_slices.reshape(-1, self.time_slices * self.slice_size, synthetic_slices.shape[-1])\n",
    "\n",
    "    def _generate_synthetic(self, minority_samples):\n",
    "        \"\"\"Generate full-length synthetic samples\"\"\"\n",
    "        sliced_data = self._slice_time_series(minority_samples)  # (N, slices, 200, ch)\n",
    "        syn_samples = []\n",
    "        \n",
    "        # Generate 1 full synthetic sample per minority sample\n",
    "        for sample_idx in tqdm(range(sliced_data.shape[0])):\n",
    "            synthetic_slices = []\n",
    "            \n",
    "            # Process each time slice\n",
    "            for slice_idx in range(self.time_slices):\n",
    "                knn = KNeighborsTimeSeries(n_neighbors=self.n_neighbors, metric='dtw')\n",
    "                knn.fit(sliced_data[:, slice_idx, :, :])\n",
    "                \n",
    "                # Find neighbors for this slice\n",
    "                neighbors = knn.kneighbors(sliced_data[sample_idx, slice_idx][np.newaxis], \n",
    "                                         return_distance=False)[0]\n",
    "                neighbor_idx = np.random.choice(neighbors)\n",
    "                \n",
    "                # Interpolate within slice\n",
    "                alpha = np.random.uniform(0.2, 0.8)\n",
    "                synthetic_slice = (1 - alpha) * sliced_data[sample_idx, slice_idx] + \\\n",
    "                                 alpha * sliced_data[neighbor_idx, slice_idx]\n",
    "                synthetic_slices.append(synthetic_slice)\n",
    "            \n",
    "            # Combine slices into full series\n",
    "            full_series = np.concatenate(synthetic_slices, axis=0)\n",
    "            syn_samples.append(full_series)\n",
    "        \n",
    "        return np.array(syn_samples)\n",
    "\n",
    "    def fit_resample(self, X, y):\n",
    "        y_int = y.astype(int)\n",
    "        class_counts = np.bincount(y_int)\n",
    "        minority_class = np.argmin(class_counts)\n",
    "        n_needed = class_counts[1 - minority_class] - class_counts[minority_class]\n",
    "        \n",
    "        if n_needed <= 0:\n",
    "            return X, y\n",
    "        \n",
    "        minority_samples = X[y_int == minority_class]\n",
    "        synthetic = self._generate_synthetic(minority_samples)\n",
    "        \n",
    "        # Ensure matching dimensions\n",
    "        assert X.shape[1:] == synthetic.shape[1:], \\\n",
    "            f\"Dimension mismatch: Original {X.shape[1:]}, Synthetic {synthetic.shape[1:]}\"\n",
    "        \n",
    "        return (np.concatenate([X, synthetic[:n_needed]], axis=0),\n",
    "                np.concatenate([y, [minority_class] * n_needed]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# Enhanced Dataset Class with Proper Encapsulation\n",
    "#============================================================\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, source, max_length=2000):\n",
    "        self.df = pl.read_parquet(source) if isinstance(source, str) else source\n",
    "        if 'orig_marker' in self.df.columns:\n",
    "            self.df = self.df.drop('orig_marker')\n",
    "        \n",
    "        # label_map = {\"Left\": 0, \"Right\": 1}  \n",
    "        # self.df = self.df.with_columns([\n",
    "        #     pl.col(\"marker\").replace(label_map).cast(pl.Int32).alias(\"marker\"),\n",
    "        #     pl.col(\"prev_marker\").replace(label_map).cast(pl.Int32).alias(\"prev_marker\")\n",
    "        # ])\n",
    "        \n",
    "        self.df = self.df.with_columns([\n",
    "            pl.col(\"marker\")\n",
    "            .cast(pl.Utf8)\n",
    "            .str.replace_all(\"Left\", \"0\")      # replace exact string \"Left\" with \"0\"\n",
    "            .str.replace_all(\"Right\", \"1\")     # replace exact string \"Right\" with \"1\"\n",
    "            .cast(pl.Int32)                      # now cast the string \"0\"/\"1\" -> int\n",
    "            .alias(\"marker\"),\n",
    "            \n",
    "            pl.col(\"prev_marker\")\n",
    "            .cast(pl.Utf8)\n",
    "            .str.replace_all(\"Left\", \"0\")\n",
    "            .str.replace_all(\"Right\", \"1\")\n",
    "            .cast(pl.Int32)\n",
    "            .alias(\"prev_marker\"),\n",
    "        ])\n",
    "        \n",
    "        self.event_ids = self.df['event_id'].unique().to_list()\n",
    "        self.max_length = max_length\n",
    "        # Keep time for sorting but exclude from features\n",
    "        self.feature_cols = [c for c in self.df.columns \n",
    "                           if c not in {'event_id', 'marker', 'time'}]\n",
    "        \n",
    "        print(\"Precomputing samples...\")\n",
    "        self._precompute_samples()\n",
    "        print(\"Computing class weights...\")\n",
    "        self._class_weights = self.compute_class_weights()\n",
    "    \n",
    "    @property\n",
    "    def class_weights(self):\n",
    "        # Expose the computed weights as a property.\n",
    "        return self._class_weights \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.event_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "    \n",
    "    def _precompute_samples(self):\n",
    "        self.samples = []\n",
    "        for event_id in tqdm(self.event_ids, desc='precomputing_samples'):\n",
    "            # Sort by time within each event!\n",
    "            event_data = self.df.filter(pl.col(\"event_id\") == event_id).sort(\"time\")\n",
    "            features = torch.tensor(\n",
    "                event_data.select(self.feature_cols).to_numpy(),\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "            features = self._pad_sequence(features)\n",
    "            \n",
    "            label = event_data['marker'][0]\n",
    "            self.samples.append((\n",
    "                torch.tensor(label, dtype=torch.float32), \n",
    "                features\n",
    "            ))\n",
    "    \n",
    "    def compute_class_weights(self):\n",
    "        \"\"\"\n",
    "        Compute inverse frequency weights based on the 'marker' column.\n",
    "        Assumes markers are \"Stimulus/A\" and \"Stimulus/P\".\n",
    "        \"\"\"\n",
    "        # Get unique combinations of event_id and marker.\n",
    "        unique_events = self.df.select([\"event_id\", \"marker\"]).unique()\n",
    "        \n",
    "        # Use value_counts on the \"marker\" column.\n",
    "        counts_df = unique_events[\"marker\"].value_counts()\n",
    "\n",
    "        # We'll use 'values' if it exists, otherwise 'marker'.\n",
    "        d = { (row.get(\"values\") or row.get(\"marker\")): row[\"count\"] \n",
    "            for row in counts_df.to_dicts() }\n",
    "        \n",
    "        weight_L = 1.0 / d.get(0, 1)\n",
    "        weight_R = 1.0 / d.get(1, 1)\n",
    "        return {\"Left\": weight_L, \"Right\": weight_R}\n",
    "   \n",
    "    def split_dataset(self, ratios=(0.7, 0.15, 0.15), seed=None):\n",
    "        \"\"\"\n",
    "        Splits the dataset into three EEGDataset instances for train, val, and test.\n",
    "        This method shuffles the event_ids and then partitions them based on the given ratios.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Copy and shuffle the event_ids\n",
    "        event_ids = self.event_ids.copy()\n",
    "        np.random.shuffle(event_ids)\n",
    "        total = len(event_ids)\n",
    "        \n",
    "        n_train = int(ratios[0] * total)\n",
    "        n_val   = int(ratios[1] * total)\n",
    "        \n",
    "        train_ids = event_ids[:n_train]\n",
    "        val_ids   = event_ids[n_train:n_train+n_val]\n",
    "        test_ids  = event_ids[n_train+n_val:]\n",
    "        \n",
    "        # Filter self.df for the selected event_ids\n",
    "        train_df = self.df.filter(pl.col(\"event_id\").is_in(train_ids))\n",
    "        val_df   = self.df.filter(pl.col(\"event_id\").is_in(val_ids))\n",
    "        test_df  = self.df.filter(pl.col(\"event_id\").is_in(test_ids))\n",
    "        \n",
    "        # Create new EEGDataset instances using the filtered data\n",
    "        train_set = EEGDataset(train_df, self.max_length)\n",
    "        val_set   = EEGDataset(val_df, self.max_length)\n",
    "        test_set  = EEGDataset(test_df, self.max_length)\n",
    "        \n",
    "        return train_set, val_set, test_set\n",
    "\n",
    "    def _pad_sequence(self, tensor):\n",
    "        # Pre-allocate tensor for maximum efficiency\n",
    "        padded = torch.zeros((self.max_length, tensor.size(1)), dtype=tensor.dtype)\n",
    "        length = min(tensor.size(0), self.max_length)\n",
    "        padded[:length] = tensor[:length]\n",
    "        return padded\n",
    "    \n",
    "    def rebalance_by_tsmote(self):\n",
    "        \"\"\"TSMOTE implementation for temporal EEG data\"\"\"\n",
    "        # Extract time-ordered features as 3D array (samples, timesteps, features)\n",
    "        X = np.stack([features.numpy() for _, features in self.samples])\n",
    "        y = np.array([label.item() for label, _ in self.samples])\n",
    "        \n",
    "        # Apply TSMOTE with temporal awareness\n",
    "        tsmote = TSMOTE()\n",
    "        X_res, y_res = tsmote.fit_resample(X, y)\n",
    "\n",
    "        # Generate synthetic temporal events\n",
    "        new_events = []\n",
    "        new_event_id = self.df['event_id'].max() + 1\n",
    "        time_base = np.arange(self.max_length)\n",
    "        original_schema = self.df.schema\n",
    "\n",
    "        # Create dtype conversion map\n",
    "        dtype_map = {\n",
    "            pl.Float64: np.float64,\n",
    "            pl.Float32: np.float32,\n",
    "            pl.Int64: np.int64,\n",
    "            pl.Int32: np.int32,\n",
    "            pl.Utf8: str,\n",
    "        }\n",
    "\n",
    "        # Process synthetic samples (original samples come first in X_res)\n",
    "        for features_3d, label in zip(X_res[len(self.samples):], y_res[len(self.samples):]):\n",
    "            event_data = {\n",
    "                \"event_id\": [new_event_id] * self.max_length,\n",
    "                \"marker\": [label] * self.max_length,\n",
    "                \"time\": time_base.copy()\n",
    "            }\n",
    "            \n",
    "            # Add features with proper temporal structure\n",
    "            for col_idx, col in enumerate(self.feature_cols):\n",
    "                col_data = features_3d[:, col_idx]\n",
    "                schema_type = original_schema[col]\n",
    "                \n",
    "                # Handle data types\n",
    "                if isinstance(schema_type, pl.List):\n",
    "                    base_type = schema_type.inner\n",
    "                    target_type = dtype_map.get(type(base_type), np.float64)\n",
    "                else:\n",
    "                    target_type = dtype_map.get(type(schema_type), np.float64)\n",
    "                \n",
    "                col_data = col_data.astype(target_type)\n",
    "                \n",
    "                # Maintain integer precision\n",
    "                if schema_type in (pl.Int64, pl.Int32):\n",
    "                    col_data = np.round(col_data).astype(int)\n",
    "                \n",
    "                event_data[col] = col_data\n",
    "\n",
    "            # Create DataFrame with strict schema adherence\n",
    "            event_df = pl.DataFrame(event_data).cast(original_schema)\n",
    "            new_events.append(event_df)\n",
    "            new_event_id += 1\n",
    "\n",
    "        # Update dataset with synthetic temporal events\n",
    "        self.df = pl.concat([self.df, *new_events])\n",
    "        self.event_ids = self.df['event_id'].unique().to_list()\n",
    "        self._precompute_samples()\n",
    "        self._class_weights = self.compute_class_weights()\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for variable-length EEG feature sequences.\n",
    "\n",
    "    Each sample is expected to be a tuple (label, feature), where:\n",
    "    - label is a scalar tensor (or 1D tensor) representing the class/target.\n",
    "    - feature is a tensor of shape (seq_len, num_channels), where seq_len may vary.\n",
    "\n",
    "    This function stacks labels and pads features along the time dimension so that\n",
    "    all sequences in the batch have the same length.\n",
    "    \"\"\"\n",
    "    # Unzip the batch into labels and features\n",
    "    labels, features = zip(*batch)\n",
    "    \n",
    "    labels = torch.stack(labels)\n",
    "    # Optionally: labels = labels.unsqueeze(1)  # Uncomment if required by your loss function\n",
    "    padded_features = pad_sequence(features, batch_first=True)\n",
    "    \n",
    "    return labels, padded_features\n",
    "\n",
    "\n",
    "def train_model(config, train_set, train_loader, val_loader, writer):    \n",
    "    # Model initialization\n",
    "    model = EEGMobileNet(\n",
    "        in_channels=64,  # Or from config if you wish\n",
    "        num_classes=1,\n",
    "        dropout=config['dropout']  # Use the dropout from config\n",
    "    ).to(config['device'])\n",
    "    #EEGDSConv(dropout=config['dropout']).to(config['device'])\n",
    "    \n",
    "    # Log model architecture and config\n",
    "    writer.add_text(\"Model/Type\", f\"EEGDSConv with dropout={config['dropout']}\")\n",
    "    writer.add_text(\"Model/Structure\", str(model))\n",
    "    writer.add_text(\"Training Config\", str(config))\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    pos_weight = torch.tensor([\n",
    "        train_set.class_weights['Left'] / train_set.class_weights['Right']\n",
    "    ]).to(config['device'])\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(weight=pos_weight)\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['lr'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Learning rate schedulers\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lambda epoch: min(1.0, (epoch + 1) / config['warmup_epochs'])\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_metric = float('inf')\n",
    "    \n",
    "    for epoch in tqdm(range(config['epochs'])):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for labels, features in train_loader:\n",
    "            features = features.to(config['device']).float()\n",
    "            labels = labels.to(config['device']).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), \n",
    "                config['grad_clip']\n",
    "            )\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for labels, features in val_loader:\n",
    "                features = features.to(config['device']).float()\n",
    "                labels = labels.to(config['device']).float()\n",
    "                \n",
    "                outputs = model(features)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                \n",
    "                preds = torch.sigmoid(outputs)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        predictions = (np.array(all_preds) > 0.5).astype(int)\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Update schedulers\n",
    "        if epoch < config['warmup_epochs']:\n",
    "            warmup_scheduler.step()\n",
    "        else:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        # Log metrics and learning rate\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(all_labels, predictions),\n",
    "            'precision': precision_score(all_labels, predictions),\n",
    "            'recall': recall_score(all_labels, predictions),\n",
    "            'f1': f1_score(all_labels, predictions)\n",
    "        }\n",
    "        \n",
    "        writer.add_scalar('LR', current_lr, epoch)\n",
    "        writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Val', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy', metrics['accuracy'], epoch)\n",
    "        writer.add_scalar('Precision', metrics['precision'], epoch)\n",
    "        writer.add_scalar('Recall', metrics['recall'], epoch)\n",
    "        writer.add_scalar('F1', metrics['f1'], epoch)\n",
    "        writer.add_scalars('Metrics', metrics, epoch)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_metric:\n",
    "            best_metric = val_loss\n",
    "            torch.save(model.state_dict(), f\"{config['log_dir']}/best_model.pth\")\n",
    "    \n",
    "    writer.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    'data_path': '/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet',\n",
    "    'split_ratios': (0.7, 0.15, 0.15),\n",
    "    'batch_size': 32,          # Increased for better generalization\n",
    "    'dropout': 0.3,            # Reduced from 0.6 for better information flow\n",
    "    'lr': 1e-4,                # Base learning rate (sweet spot between 1e-5 and 3e-3)\n",
    "    'weight_decay': 0,      # Increased regularization\n",
    "    'epochs': 200,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'log_dir': './runs/CNN',\n",
    "    'lr_scheduler': {\n",
    "        'mode': 'min',\n",
    "        'factor': 0.1,         # More aggressive LR reduction\n",
    "        'patience': 5,         # Faster response to plateaus\n",
    "        'threshold': 0.001,\n",
    "        'cooldown': 3\n",
    "    },\n",
    "    'grad_clip': 1.0,          # Add gradient clipping\n",
    "    'warmup_epochs': 10        # Linear LR warmup\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating full dataset...\n",
      "Precomputing samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33bdf0823c224fc7b7bc6c5769641114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "precomputing_samples:   0%|          | 0/2772 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing class weights...\n",
      "Splitting the dataset...\n",
      "Precomputing samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435dae3d9c7444638016ae824eea362a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "precomputing_samples:   0%|          | 0/1940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing class weights...\n",
      "Precomputing samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d7ba54524049b6a23f26dd5f6bc0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "precomputing_samples:   0%|          | 0/415 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing class weights...\n",
      "Precomputing samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e7c1df1bed4a8a9bac16f5fe32d051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "precomputing_samples:   0%|          | 0/417 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing class weights...\n",
      "unbalanced train dataset shape: (1940, [labels: torch.Size([]), features: [2000, 64]])\n",
      "Applying SMOTE to train dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b66bbbc440549b49a49ac448bf1396a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/952 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "InvalidOperationError",
     "evalue": "conversion from `str` to `i32` failed in column 'marker' for 2000 out of 2000 values: [\"Stimulus/A\", \"Stimulus/A\", … \"Stimulus/A\"]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidOperationError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Balance training set\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplying SMOTE to train dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrebalance_by_tsmote\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m len_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_set)\n\u001b[1;32m     31\u001b[0m sample \u001b[38;5;241m=\u001b[39m train_set[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[6], line 183\u001b[0m, in \u001b[0;36mEEGDataset.rebalance_by_tsmote\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m     event_data[col] \u001b[38;5;241m=\u001b[39m col_data\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Create DataFrame with strict schema adherence\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m event_df \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_data\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m new_events\u001b[38;5;241m.\u001b[39mappend(event_df)\n\u001b[1;32m    185\u001b[0m new_event_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/polars/dataframe/frame.py:8045\u001b[0m, in \u001b[0;36mDataFrame.cast\u001b[0;34m(self, dtypes, strict)\u001b[0m\n\u001b[1;32m   7961\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcast\u001b[39m(\n\u001b[1;32m   7962\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   7963\u001b[0m     dtypes: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   7970\u001b[0m     strict: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   7971\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   7972\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   7973\u001b[0m \u001b[38;5;124;03m    Cast DataFrame column(s) to the specified dtype(s).\u001b[39;00m\n\u001b[1;32m   7974\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8043\u001b[0m \u001b[38;5;124;03m     'ham': ['2020-01-02', '2021-03-04', '2022-05-06']}\u001b[39;00m\n\u001b[1;32m   8044\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 8045\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_eager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/polars/lazyframe/frame.py:2056\u001b[0m, in \u001b[0;36mLazyFrame.collect\u001b[0;34m(self, type_coercion, _type_check, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, streaming, engine, background, _check_order, _eager, **_kwargs)\u001b[0m\n\u001b[1;32m   2054\u001b[0m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m callback \u001b[38;5;241m=\u001b[39m _kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_opt_callback\u001b[39m\u001b[38;5;124m\"\u001b[39m, callback)\n\u001b[0;32m-> 2056\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mInvalidOperationError\u001b[0m: conversion from `str` to `i32` failed in column 'marker' for 2000 out of 2000 values: [\"Stimulus/A\", \"Stimulus/A\", … \"Stimulus/A\"]"
     ]
    }
   ],
   "source": [
    "#============================================================\n",
    "# Training Pipeline\n",
    "#============================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Initialize dataset\n",
    "print(\"Creating full dataset...\")\n",
    "full_dataset = EEGDataset(config['data_path'])\n",
    "\n",
    "print(\"Splitting the dataset...\")\n",
    "# Split dataset\n",
    "train_set, val_set, test_set = full_dataset.split_dataset(\n",
    "    ratios=config['split_ratios']\n",
    ")\n",
    "\n",
    "del full_dataset\n",
    "\n",
    "len_dataset = len(train_set)\n",
    "sample = train_set[0]\n",
    "label_shape = sample[0].shape\n",
    "feature_shape = sample[1].shape\n",
    "\n",
    "print(f\"unbalanced train dataset shape: ({len_dataset}, [labels: {label_shape}, features: {list(feature_shape)}])\")\n",
    "\n",
    "# Balance training set\n",
    "print(\"Applying SMOTE to train dataset...\")\n",
    "train_set.rebalance_by_tsmote()\n",
    "\n",
    "len_dataset = len(train_set)\n",
    "sample = train_set[0]\n",
    "label_shape = sample[0].shape\n",
    "feature_shape = sample[1].shape\n",
    "\n",
    "print(f\"balanced train dataset shape: ({len_dataset}, [labels: {label_shape}, features: {list(feature_shape)}])\")\n",
    "\n",
    "# print(\"Augmenting train dataset...\")\n",
    "# train_set.augment_dataset(\n",
    "#     n_times=3,              # Nx dataset expansion\n",
    "#     noise_std=0.15,         # Moderate noise\n",
    "#     scale_range=(0.7, 1.3), # ±30% amplitude variation\n",
    "#     warp_range=(0.85, 1.15),# ±15% time warping\n",
    "#     max_shift=15,           # 150ms temporal shifts\n",
    "#     freq_shift=3,           # ±3Hz frequency shifts\n",
    "#     mask_size=75,           # 750ms masking\n",
    "#     drop_prob=0.15,         # 15% channel dropout\n",
    "#     mixup_alpha=0.2         # Mixup\n",
    "# )\n",
    "\n",
    "torch.save(train_set, 'train_set.pt')\n",
    "torch.save(val_set, 'val_set.pt')\n",
    "torch.save(test_set, 'test_set.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset shape: (1940, [labels: torch.Size([]), features: [2000, 64]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_set = torch.load('train_set.pt', weights_only=False)\n",
    "val_set = torch.load('val_set.pt', weights_only=False)\n",
    "test_set = torch.load('test_set.pt', weights_only=False)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=12,\n",
    "    pin_memory=True,      # For GPU acceleration\n",
    "    persistent_workers=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(val_set, batch_size=config['batch_size'], collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=config['batch_size'], collate_fn=collate_fn)\n",
    "\n",
    "len_dataset = len(train_set)\n",
    "sample = train_set[0]\n",
    "label_shape = sample[0].shape\n",
    "feature_shape = sample[1].shape\n",
    "\n",
    "print(f\"train dataset shape: ({len_dataset}, [labels: {label_shape}, features: {list(feature_shape)}])\")\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "writer = SummaryWriter(log_dir=config['log_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owner/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11d4e1d6b404541a32d79ba1951bd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training\n",
    "trained_model = train_model(config, train_set, train_loader, val_loader, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f455d1802f42ccbe8da9f645154018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# pos_weight = torch.tensor([\n",
    "#     train_set.class_weights['A'] / train_set.class_weights['P']\n",
    "# ]).to(config['device'])\n",
    "criterion = nn.BCEWithLogitsLoss()#pos_weight=pos_weight)\n",
    "\n",
    "epoch = 1\n",
    "# Assuming model, criterion, test_loader, device, writer, and epoch are already defined\n",
    "# Instantiate your model\n",
    "best_model = EEGMobileNet()  # Adjust parameters as needed\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load(f\"{config['log_dir']}/best_model.pth\", map_location=config['device'])\n",
    "best_model.load_state_dict(state_dict)\n",
    "\n",
    "# Move model to the correct device\n",
    "best_model = best_model.to(config['device'])\n",
    "\n",
    "# Set model to evaluation mode\n",
    "best_model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "all_test_markers = []\n",
    "all_test_predictions = []\n",
    "with torch.no_grad():\n",
    "    for markers, features in tqdm(test_loader):\n",
    "        features = features.to(config['device'])\n",
    "        markers = markers.to(config['device'])\n",
    "\n",
    "        outputs = best_model(features)\n",
    "        loss = criterion(outputs, markers)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Collect markers and predictions for metrics calculation\n",
    "        all_test_markers.extend(markers.cpu().numpy().flatten())\n",
    "        all_test_predictions.extend(torch.sigmoid(outputs).cpu().numpy().flatten())\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "# Calculate test metrics\n",
    "test_accuracy = accuracy_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_precision = precision_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_recall = recall_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_f1 = f1_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_roc_auc = roc_auc_score(all_test_markers, all_test_predictions)\n",
    "\n",
    "# Log test metrics to TensorBoard\n",
    "writer.add_scalar('Metrics/test_accuracy', test_accuracy, epoch)\n",
    "writer.add_scalar('Metrics/test_precision', test_precision, epoch)\n",
    "writer.add_scalar('Metrics/test_recall', test_recall, epoch)\n",
    "writer.add_scalar('Metrics/test_f1', test_f1, epoch)\n",
    "writer.add_scalar('Metrics/test_roc_auc', test_roc_auc, epoch)\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test_accuracy=0.5563549160671463\n",
      "test_precision=0.5362903225806451\n",
      "test_recall=0.6551724137931034\n",
      "test_f1=0.5898004434589801\n",
      "test_roc_auc=np.float64(0.6026656231296901)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "{test_accuracy=}\n",
    "{test_precision=}\n",
    "{test_recall=}\n",
    "{test_f1=}\n",
    "{test_roc_auc=}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0890ba80b37442eca3a73652c72e0d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_threshold=np.float64(0.34999999999999987)\n",
      "best_f1=0.6568265682656826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "best_threshold = 0.0\n",
    "best_f1 = 0.0\n",
    "thresholds = np.arange(0.1, 1.0, 0.01)\n",
    "\n",
    "for threshold in tqdm(thresholds):\n",
    "    binary_predictions = (all_test_predictions > threshold).astype(int)\n",
    "    current_recall = f1_score(all_test_markers, binary_predictions)\n",
    "\n",
    "    if current_recall > best_f1:\n",
    "        best_f1 = current_recall\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"{best_threshold=}\")\n",
    "print(f\"{best_f1=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084a35c9c3314d2cb68c3d03fea2e5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_threshold=np.float64(0.1)\n",
      "best_recall=0.9852216748768473\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "import numpy as np\n",
    "best_threshold = 0.1\n",
    "best_recall = 0.0\n",
    "thresholds = np.arange(0.1, 1.0, 0.01)\n",
    "\n",
    "for threshold in tqdm(thresholds):\n",
    "    binary_predictions = (all_test_predictions > threshold).astype(int)\n",
    "    current_recall = recall_score(all_test_markers, binary_predictions)\n",
    "\n",
    "    if current_recall > best_recall:\n",
    "        best_recall = current_recall\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"{best_threshold=}\")\n",
    "print(f\"{best_recall=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
