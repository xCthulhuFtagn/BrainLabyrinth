{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpolars\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/imblearn/__init__.py:52\u001b[0m\n\u001b[1;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m         combine,\n\u001b[1;32m     54\u001b[0m         ensemble,\n\u001b[1;32m     55\u001b[0m         exceptions,\n\u001b[1;32m     56\u001b[0m         metrics,\n\u001b[1;32m     57\u001b[0m         over_sampling,\n\u001b[1;32m     58\u001b[0m         pipeline,\n\u001b[1;32m     59\u001b[0m         tensorflow,\n\u001b[1;32m     60\u001b[0m         under_sampling,\n\u001b[1;32m     61\u001b[0m         utils,\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/imblearn/combine/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/imblearn/combine/_smote_enn.py:9\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#          Christos Aridas\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# License: MIT\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumbers\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/sklearn/__init__.py:73\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     __check_build,\n\u001b[1;32m     71\u001b[0m     _distributor_init,\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     76\u001b[0m _submodules \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m ]\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/sklearn/base.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/sklearn/utils/__init__.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/sklearn/utils/_chunking.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _deprecate_force_all_finite\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspecial\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[1;32m     19\u001b[0m _NUMPY_NAMESPACE_NAMES \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray_api_compat.numpy\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21myield_namespaces\u001b[39m(include_numpy_namespaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/sklearn/utils/fixes.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/scipy/stats/__init__.py:610\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    605\u001b[0m \n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    609\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 610\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/scipy/stats/_stats_py.py:56\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _kendall_dis, _toint64, _weightedrankedtau\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass, field\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_hypotests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _all_partitions\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_pythran\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _compute_outer_prob_inside_method\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_resampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (MonteCarloMethod, PermutationMethod, BootstrapMethod,\n\u001b[1;32m     59\u001b[0m                           monte_carlo_test, permutation_test, bootstrap,\n\u001b[1;32m     60\u001b[0m                           _batch_generator)\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/scipy/stats/_hypotests.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_continuous_distns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m norm\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gamma, kv, gammaln\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ifft\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_pythran\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _a_ij_Aij_Dij2\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_pythran\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     _concordant_pairs \u001b[38;5;28;01mas\u001b[39;00m _P, _discordant_pairs \u001b[38;5;28;01mas\u001b[39;00m _Q\n\u001b[1;32m     17\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/scipy/fft/__init__.py:92\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_realtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dct, idct, dst, idst, dctn, idctn, dstn, idstn\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fftlog\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fht, ifht, fhtoffset\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_helper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     93\u001b[0m     next_fast_len, prev_fast_len, fftfreq,\n\u001b[1;32m     94\u001b[0m     rfftfreq, fftshift, ifftshift)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_backend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (set_backend, skip_backend, set_global_backend,\n\u001b[1;32m     96\u001b[0m                        register_backend)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pocketfft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_workers, get_workers\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/scipy/fft/_helper.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m update_wrapper, lru_cache\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pocketfft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m helper \u001b[38;5;28;01mas\u001b[39;00m _helper\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_namespace\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1024\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:170\u001b[0m, in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:185\u001b[0m, in \u001b[0;36m_get_module_lock\u001b[0;34m(name)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import polars as pl\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "# Set the seed for Python's built-in random module\n",
    "random.seed(69)\n",
    "\n",
    "# Set the seed for NumPy's random number generator\n",
    "np.random.seed(69)\n",
    "\n",
    "# Set the seed for PyTorch's random number generators\n",
    "torch.manual_seed(69)\n",
    "torch.cuda.manual_seed(69)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5_828_008, 68)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>event_id</th><th>marker</th><th>time</th><th>Fp1</th><th>Fpz</th><th>Fp2</th><th>F7</th><th>F3</th><th>Fz</th><th>F4</th><th>F8</th><th>FC5</th><th>FC1</th><th>FC2</th><th>FC6</th><th>M1</th><th>T7</th><th>C3</th><th>Cz</th><th>C4</th><th>T8</th><th>M2</th><th>CP5</th><th>CP1</th><th>CP2</th><th>CP6</th><th>P7</th><th>P3</th><th>Pz</th><th>P4</th><th>P8</th><th>POz</th><th>O1</th><th>O2</th><th>EOG</th><th>AF7</th><th>AF3</th><th>AF4</th><th>AF8</th><th>F5</th><th>F1</th><th>F2</th><th>F6</th><th>FC3</th><th>FCz</th><th>FC4</th><th>C5</th><th>C1</th><th>C2</th><th>C6</th><th>CP3</th><th>CP4</th><th>P5</th><th>P1</th><th>P2</th><th>P6</th><th>PO5</th><th>PO3</th><th>PO4</th><th>PO6</th><th>FT7</th><th>FT8</th><th>TP7</th><th>TP8</th><th>PO7</th><th>PO8</th><th>Oz</th><th>__null_dask_index__</th></tr><tr><td>i64</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td></tr></thead><tbody><tr><td>0</td><td>&quot;Stimulus/1&quot;</td><td>9.438</td><td>9.53724</td><td>7.803913</td><td>-6.957125</td><td>-3.662101</td><td>-1.831665</td><td>6.678428</td><td>0.643271</td><td>-28.855688</td><td>17.219198</td><td>10.443791</td><td>8.557194</td><td>5.307324</td><td>-41.916786</td><td>-20.510318</td><td>20.105794</td><td>7.447749</td><td>18.204764</td><td>-21.123294</td><td>-54.043086</td><td>-0.779434</td><td>9.156651</td><td>3.35639</td><td>7.050375</td><td>8.437749</td><td>10.035689</td><td>18.781002</td><td>12.663772</td><td>-1.516957</td><td>5.923493</td><td>9.755382</td><td>5.507086</td><td>56.784839</td><td>-16.524688</td><td>0.995008</td><td>1.210651</td><td>-16.334868</td><td>5.347515</td><td>7.849705</td><td>4.708359</td><td>-21.316867</td><td>2.927581</td><td>9.449216</td><td>5.787111</td><td>3.851742</td><td>5.561468</td><td>1.623157</td><td>0.986564</td><td>13.195735</td><td>5.790764</td><td>9.944564</td><td>14.604585</td><td>21.713373</td><td>4.75506</td><td>10.76817</td><td>11.458283</td><td>-8.927917</td><td>3.754084</td><td>0.357387</td><td>-4.394041</td><td>-8.618822</td><td>-5.172546</td><td>11.513064</td><td>4.785936</td><td>5.749947</td><td>0</td></tr><tr><td>0</td><td>&quot;Stimulus/1&quot;</td><td>9.44</td><td>13.396619</td><td>13.868635</td><td>-2.922562</td><td>3.933463</td><td>1.559054</td><td>9.798244</td><td>3.592579</td><td>-24.953035</td><td>15.061343</td><td>8.709912</td><td>9.965191</td><td>5.720247</td><td>-70.75158</td><td>-41.867714</td><td>15.554878</td><td>6.088562</td><td>21.294248</td><td>-0.120773</td><td>-50.053209</td><td>-8.458973</td><td>6.922121</td><td>3.178088</td><td>16.021653</td><td>-9.844435</td><td>-2.435621</td><td>18.337207</td><td>18.720653</td><td>21.930048</td><td>5.166919</td><td>-11.485745</td><td>27.682245</td><td>62.681269</td><td>-15.060192</td><td>0.757932</td><td>5.598742</td><td>-17.990556</td><td>2.588855</td><td>7.099604</td><td>5.117679</td><td>-16.808443</td><td>5.047086</td><td>12.311436</td><td>11.37285</td><td>3.025393</td><td>8.060817</td><td>5.687988</td><td>5.582753</td><td>10.034344</td><td>11.733741</td><td>-3.211322</td><td>11.257885</td><td>25.065433</td><td>21.465142</td><td>-6.313013</td><td>-5.096768</td><td>-1.385707</td><td>27.073425</td><td>-1.288571</td><td>-4.394788</td><td>-19.987022</td><td>14.889966</td><td>-10.593543</td><td>27.311613</td><td>-6.368204</td><td>1</td></tr><tr><td>0</td><td>&quot;Stimulus/1&quot;</td><td>9.442</td><td>14.850883</td><td>14.096507</td><td>-0.078051</td><td>-0.246867</td><td>1.38473</td><td>11.079935</td><td>4.932076</td><td>-24.924988</td><td>14.634668</td><td>9.740824</td><td>11.113594</td><td>5.932303</td><td>-66.520317</td><td>-25.757618</td><td>14.687831</td><td>6.78713</td><td>22.50476</td><td>-9.39821</td><td>-44.082786</td><td>-6.577221</td><td>8.424037</td><td>4.776816</td><td>11.782222</td><td>-2.206932</td><td>-3.447845</td><td>17.317284</td><td>15.465552</td><td>7.507261</td><td>4.749701</td><td>-4.053521</td><td>20.096318</td><td>56.423504</td><td>-16.052469</td><td>0.542124</td><td>3.936092</td><td>-14.449495</td><td>2.010495</td><td>9.100914</td><td>5.373109</td><td>-16.629689</td><td>4.217228</td><td>12.391046</td><td>10.807673</td><td>2.673265</td><td>6.832248</td><td>5.108662</td><td>5.212829</td><td>9.382513</td><td>9.761762</td><td>-3.64864</td><td>9.981714</td><td>22.28212</td><td>13.84102</td><td>-1.795933</td><td>-1.438725</td><td>-4.064985</td><td>22.191193</td><td>-9.739858</td><td>-5.565701</td><td>-9.834637</td><td>8.252451</td><td>-1.853507</td><td>20.793088</td><td>3.175448</td><td>2</td></tr><tr><td>0</td><td>&quot;Stimulus/1&quot;</td><td>9.444</td><td>21.293703</td><td>18.676068</td><td>2.396635</td><td>5.01313</td><td>5.984928</td><td>13.828105</td><td>6.36971</td><td>-23.503992</td><td>20.354059</td><td>13.549383</td><td>12.007385</td><td>2.86629</td><td>-32.600562</td><td>6.407807</td><td>19.207374</td><td>8.063451</td><td>18.753907</td><td>-36.384114</td><td>-46.5438</td><td>0.755845</td><td>9.151167</td><td>1.696862</td><td>-2.183605</td><td>9.926201</td><td>2.112486</td><td>14.258738</td><td>2.697055</td><td>-30.135514</td><td>-6.710183</td><td>0.557059</td><td>-28.83586</td><td>66.307082</td><td>-10.671446</td><td>5.90164</td><td>6.894815</td><td>-11.230897</td><td>8.280238</td><td>14.231695</td><td>8.181461</td><td>-18.193031</td><td>10.290102</td><td>16.717396</td><td>9.37912</td><td>4.499651</td><td>9.509004</td><td>5.286035</td><td>-0.039267</td><td>11.469463</td><td>0.325132</td><td>2.030729</td><td>9.968108</td><td>16.758211</td><td>-15.392703</td><td>2.525013</td><td>3.591888</td><td>-23.756455</td><td>-26.819381</td><td>-14.263371</td><td>-7.832686</td><td>1.443246</td><td>-6.503959</td><td>3.155976</td><td>-28.110124</td><td>-16.510437</td><td>3</td></tr><tr><td>0</td><td>&quot;Stimulus/1&quot;</td><td>9.446</td><td>20.054007</td><td>18.219546</td><td>3.120409</td><td>0.285677</td><td>6.550377</td><td>17.138199</td><td>15.234075</td><td>-13.355245</td><td>18.774967</td><td>14.254335</td><td>17.074288</td><td>12.947393</td><td>-34.977934</td><td>5.612512</td><td>15.83888</td><td>9.753912</td><td>22.196893</td><td>-37.742914</td><td>-36.697359</td><td>-3.596704</td><td>5.289391</td><td>1.301041</td><td>4.294134</td><td>1.056083</td><td>-4.711626</td><td>13.018475</td><td>5.3802</td><td>-11.755816</td><td>-9.147353</td><td>-10.122114</td><td>-23.32428</td><td>64.653867</td><td>-5.254813</td><td>8.025885</td><td>10.051126</td><td>-3.915474</td><td>10.896383</td><td>16.240127</td><td>13.117371</td><td>-7.493371</td><td>10.223294</td><td>18.24204</td><td>15.672042</td><td>-0.109563</td><td>9.109484</td><td>6.398258</td><td>10.450412</td><td>8.586929</td><td>4.823086</td><td>-5.016421</td><td>7.561401</td><td>18.00294</td><td>-12.701589</td><td>-6.145933</td><td>-6.488521</td><td>-21.213362</td><td>-21.120105</td><td>-17.366758</td><td>9.093167</td><td>-11.152638</td><td>1.806184</td><td>-7.890925</td><td>-21.705784</td><td>-25.354465</td><td>4</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1381</td><td>&quot;Stimulus/A&quot;</td><td>928.476</td><td>9.591813</td><td>-41.557977</td><td>-60.77412</td><td>23.760604</td><td>-41.33637</td><td>-66.987031</td><td>-11.20374</td><td>-90.704749</td><td>3.790776</td><td>-30.919723</td><td>-37.216313</td><td>-61.566037</td><td>-52.382623</td><td>-9.286577</td><td>-42.072383</td><td>-38.344749</td><td>-53.684958</td><td>-44.898705</td><td>-49.312139</td><td>-0.965291</td><td>-33.180112</td><td>-17.891398</td><td>-10.890542</td><td>-19.449326</td><td>-9.946805</td><td>-33.991378</td><td>-89.135265</td><td>-85.339557</td><td>-19.949215</td><td>-13.598605</td><td>-35.072145</td><td>379.205543</td><td>10.027611</td><td>-13.869941</td><td>-38.946625</td><td>-80.264638</td><td>-1.0685</td><td>-43.047086</td><td>-10.514064</td><td>-57.963668</td><td>31.12568</td><td>-46.874566</td><td>-32.352284</td><td>18.198025</td><td>-51.127463</td><td>-36.473882</td><td>-43.249089</td><td>-26.761444</td><td>-42.808292</td><td>-27.528111</td><td>-29.581585</td><td>-52.231929</td><td>-52.319126</td><td>-18.26109</td><td>-19.034752</td><td>-54.394882</td><td>-40.090876</td><td>1.734976</td><td>-61.119441</td><td>6.031929</td><td>-18.942257</td><td>-19.640234</td><td>-39.629149</td><td>-28.380572</td><td>275996</td></tr><tr><td>1381</td><td>&quot;Stimulus/A&quot;</td><td>928.478</td><td>10.05211</td><td>-39.285851</td><td>-57.304729</td><td>24.489219</td><td>-41.405448</td><td>-65.223565</td><td>-7.581582</td><td>-85.183478</td><td>7.284242</td><td>-32.467265</td><td>-36.44456</td><td>-58.760747</td><td>-51.05454</td><td>-0.406084</td><td>-41.466138</td><td>-37.820387</td><td>-53.660458</td><td>-51.282983</td><td>-47.950155</td><td>-3.073491</td><td>-34.703956</td><td>-18.293676</td><td>-11.464762</td><td>-24.24428</td><td>-11.731681</td><td>-33.641051</td><td>-89.751296</td><td>-86.665381</td><td>-21.44362</td><td>-17.000163</td><td>-35.684019</td><td>385.31099</td><td>13.91835</td><td>-10.738223</td><td>-30.392338</td><td>-74.628701</td><td>2.910838</td><td>-41.721126</td><td>-7.360083</td><td>-49.079759</td><td>30.052943</td><td>-45.777716</td><td>-31.916799</td><td>19.00681</td><td>-50.908402</td><td>-34.346295</td><td>-36.57473</td><td>-26.347318</td><td>-41.537574</td><td>-29.77699</td><td>-30.339979</td><td>-52.22948</td><td>-53.980062</td><td>-21.867465</td><td>-21.754061</td><td>-57.051582</td><td>-40.913533</td><td>3.709452</td><td>-60.064794</td><td>4.821594</td><td>-19.524317</td><td>-20.815015</td><td>-40.568466</td><td>-29.987083</td><td>275997</td></tr><tr><td>1381</td><td>&quot;Stimulus/A&quot;</td><td>928.48</td><td>12.065095</td><td>-40.534576</td><td>-57.783633</td><td>29.010773</td><td>-43.466559</td><td>-65.123134</td><td>-6.594105</td><td>-84.364808</td><td>11.379481</td><td>-32.393301</td><td>-35.587802</td><td>-55.748424</td><td>-54.481948</td><td>9.565403</td><td>-42.304131</td><td>-38.425967</td><td>-52.91369</td><td>-47.393221</td><td>-47.164644</td><td>-2.847837</td><td>-34.77862</td><td>-17.350552</td><td>-9.44111</td><td>-21.020738</td><td>-8.33714</td><td>-32.512395</td><td>-87.232313</td><td>-83.327347</td><td>-20.088512</td><td>-11.905468</td><td>-34.854007</td><td>402.957563</td><td>16.298075</td><td>-10.369621</td><td>-30.212791</td><td>-73.874753</td><td>5.971585</td><td>-39.426622</td><td>-7.338018</td><td>-49.622007</td><td>30.432534</td><td>-45.860797</td><td>-31.460015</td><td>18.195945</td><td>-51.894871</td><td>-34.708639</td><td>-30.659673</td><td>-27.710193</td><td>-41.63766</td><td>-27.134769</td><td>-30.320361</td><td>-51.321614</td><td>-52.401906</td><td>-19.546804</td><td>-20.653188</td><td>-55.703298</td><td>-39.288035</td><td>12.232324</td><td>-56.132915</td><td>10.525082</td><td>-15.37765</td><td>-18.393694</td><td>-40.166692</td><td>-28.50722</td><td>275998</td></tr><tr><td>1381</td><td>&quot;Stimulus/A&quot;</td><td>928.482</td><td>14.72505</td><td>-38.596546</td><td>-53.248571</td><td>31.472895</td><td>-39.738697</td><td>-64.958224</td><td>-6.417419</td><td>-83.385095</td><td>11.147532</td><td>-30.443848</td><td>-35.688861</td><td>-55.982359</td><td>-46.464062</td><td>18.694285</td><td>-45.26478</td><td>-38.533711</td><td>-50.081167</td><td>-42.353148</td><td>-42.931054</td><td>-3.614683</td><td>-32.516321</td><td>-16.236222</td><td>-6.832051</td><td>-17.149998</td><td>-11.510808</td><td>-35.3496</td><td>-88.705435</td><td>-85.313381</td><td>-21.334178</td><td>-14.573576</td><td>-37.156088</td><td>394.311109</td><td>17.552649</td><td>-8.430964</td><td>-33.06964</td><td>-70.96307</td><td>3.752907</td><td>-39.039754</td><td>-5.491022</td><td>-53.773884</td><td>29.962905</td><td>-46.757481</td><td>-30.543767</td><td>14.668657</td><td>-51.72058</td><td>-34.076067</td><td>-32.300195</td><td>-29.405876</td><td>-39.132014</td><td>-25.195138</td><td>-29.335414</td><td>-51.595348</td><td>-49.798575</td><td>-17.942612</td><td>-18.328749</td><td>-55.256379</td><td>-41.800521</td><td>12.451331</td><td>-55.866528</td><td>9.126756</td><td>-17.300153</td><td>-20.074211</td><td>-42.179936</td><td>-31.332407</td><td>275999</td></tr><tr><td>1381</td><td>&quot;Stimulus/A&quot;</td><td>928.484</td><td>15.808375</td><td>-36.266224</td><td>-53.110752</td><td>32.924337</td><td>-37.36348</td><td>-64.090116</td><td>-5.231033</td><td>-82.415774</td><td>13.220155</td><td>-30.305264</td><td>-33.865274</td><td>-54.286609</td><td>-46.264271</td><td>17.130059</td><td>-43.916018</td><td>-36.523518</td><td>-50.787514</td><td>-40.772671</td><td>-41.895675</td><td>-2.720411</td><td>-34.069455</td><td>-14.845942</td><td>-6.451268</td><td>-17.932889</td><td>-10.319097</td><td>-35.023904</td><td>-88.797509</td><td>-85.17417</td><td>-21.106819</td><td>-13.415075</td><td>-34.868671</td><td>405.91546</td><td>19.267502</td><td>-6.658455</td><td>-34.919759</td><td>-70.498651</td><td>3.814254</td><td>-37.306107</td><td>-4.04167</td><td>-51.11805</td><td>30.894601</td><td>-45.19893</td><td>-30.159779</td><td>13.853158</td><td>-51.240877</td><td>-34.016793</td><td>-30.1696</td><td>-29.843134</td><td>-39.165263</td><td>-27.946339</td><td>-29.823856</td><td>-51.218703</td><td>-50.641914</td><td>-18.087194</td><td>-19.294669</td><td>-53.564813</td><td>-40.085109</td><td>12.634602</td><td>-55.407809</td><td>9.125513</td><td>-17.057724</td><td>-19.120281</td><td>-39.526945</td><td>-30.902496</td><td>276000</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5_828_008, 68)\n",
       "┌──────────┬────────────┬─────────┬───────────┬───┬────────────┬───────────┬───────────┬───────────┐\n",
       "│ event_id ┆ marker     ┆ time    ┆ Fp1       ┆ … ┆ PO7        ┆ PO8       ┆ Oz        ┆ __null_da │\n",
       "│ ---      ┆ ---        ┆ ---     ┆ ---       ┆   ┆ ---        ┆ ---       ┆ ---       ┆ sk_index_ │\n",
       "│ i64      ┆ str        ┆ f64     ┆ f64       ┆   ┆ f64        ┆ f64       ┆ f64       ┆ _         │\n",
       "│          ┆            ┆         ┆           ┆   ┆            ┆           ┆           ┆ ---       │\n",
       "│          ┆            ┆         ┆           ┆   ┆            ┆           ┆           ┆ i64       │\n",
       "╞══════════╪════════════╪═════════╪═══════════╪═══╪════════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 0        ┆ Stimulus/1 ┆ 9.438   ┆ 9.53724   ┆ … ┆ 11.513064  ┆ 4.785936  ┆ 5.749947  ┆ 0         │\n",
       "│ 0        ┆ Stimulus/1 ┆ 9.44    ┆ 13.396619 ┆ … ┆ -10.593543 ┆ 27.311613 ┆ -6.368204 ┆ 1         │\n",
       "│ 0        ┆ Stimulus/1 ┆ 9.442   ┆ 14.850883 ┆ … ┆ -1.853507  ┆ 20.793088 ┆ 3.175448  ┆ 2         │\n",
       "│ 0        ┆ Stimulus/1 ┆ 9.444   ┆ 21.293703 ┆ … ┆ 3.155976   ┆ -28.11012 ┆ -16.51043 ┆ 3         │\n",
       "│          ┆            ┆         ┆           ┆   ┆            ┆ 4         ┆ 7         ┆           │\n",
       "│ 0        ┆ Stimulus/1 ┆ 9.446   ┆ 20.054007 ┆ … ┆ -7.890925  ┆ -21.70578 ┆ -25.35446 ┆ 4         │\n",
       "│          ┆            ┆         ┆           ┆   ┆            ┆ 4         ┆ 5         ┆           │\n",
       "│ …        ┆ …          ┆ …       ┆ …         ┆ … ┆ …          ┆ …         ┆ …         ┆ …         │\n",
       "│ 1381     ┆ Stimulus/A ┆ 928.476 ┆ 9.591813  ┆ … ┆ -19.640234 ┆ -39.62914 ┆ -28.38057 ┆ 275996    │\n",
       "│          ┆            ┆         ┆           ┆   ┆            ┆ 9         ┆ 2         ┆           │\n",
       "│ 1381     ┆ Stimulus/A ┆ 928.478 ┆ 10.05211  ┆ … ┆ -20.815015 ┆ -40.56846 ┆ -29.98708 ┆ 275997    │\n",
       "│          ┆            ┆         ┆           ┆   ┆            ┆ 6         ┆ 3         ┆           │\n",
       "│ 1381     ┆ Stimulus/A ┆ 928.48  ┆ 12.065095 ┆ … ┆ -18.393694 ┆ -40.16669 ┆ -28.50722 ┆ 275998    │\n",
       "│          ┆            ┆         ┆           ┆   ┆            ┆ 2         ┆           ┆           │\n",
       "│ 1381     ┆ Stimulus/A ┆ 928.482 ┆ 14.72505  ┆ … ┆ -20.074211 ┆ -42.17993 ┆ -31.33240 ┆ 275999    │\n",
       "│          ┆            ┆         ┆           ┆   ┆            ┆ 6         ┆ 7         ┆           │\n",
       "│ 1381     ┆ Stimulus/A ┆ 928.484 ┆ 15.808375 ┆ … ┆ -19.120281 ┆ -39.52694 ┆ -30.90249 ┆ 276000    │\n",
       "│          ┆            ┆         ┆           ┆   ┆            ┆ 5         ┆ 6         ┆           │\n",
       "└──────────┴────────────┴─────────┴───────────┴───┴────────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.read_parquet('/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# Model Architecture\n",
    "#============================================================\n",
    "class EEGDSConv(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(64, 64, 15, padding='same', groups=64),\n",
    "            nn.Conv1d(64, 16, 1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(4),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(16, 16, 7, padding='same', groups=16),\n",
    "            nn.Conv1d(16, 8, 1),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return self.block(x).squeeze(-1)  # Squeeze last dimension to match target shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# Enhanced Dataset Class with Proper Encapsulation\n",
    "#============================================================\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, source, max_length=2000):\n",
    "        self.df = self._load_and_filter(source)\n",
    "        self.event_ids = self.df['event_id'].unique().to_list()\n",
    "        self.max_length = max_length\n",
    "        self._class_weights = self.compute_class_weights()\n",
    "        # Keep time for sorting but exclude from features\n",
    "        self.feature_cols = [c for c in self.df.columns \n",
    "                           if c not in {'event_id', 'marker', 'time'}]\n",
    "        self._precompute_samples()\n",
    "    \n",
    "    @property\n",
    "    def class_weights(self):\n",
    "        # Expose the computed weights as a property.\n",
    "        return self._class_weights \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.event_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "    \n",
    "    def _precompute_samples(self):\n",
    "        \"\"\"Cache time-ordered samples with revolutionary discipline\"\"\"\n",
    "        self.samples = []\n",
    "        for event_id in self.event_ids:\n",
    "            # Sort by time within each event!\n",
    "            event_data = self.df.filter(pl.col(\"event_id\") == event_id).sort(\"time\")\n",
    "            \n",
    "            features = torch.tensor(\n",
    "                event_data.select(self.feature_cols).to_numpy(),\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "            features = self._pad_sequence(features)\n",
    "            \n",
    "            label = 1.0 if event_data['marker'][0] == \"Stimulus/P\" else 0.0\n",
    "            self.samples.append((\n",
    "                torch.tensor(label, dtype=torch.float32), \n",
    "                features\n",
    "            ))\n",
    "    \n",
    "    def augment_dataset(self, n_times=5, **kwargs):\n",
    "        new_event_id = self.df[\"event_id\"].max() + 1\n",
    "        original_count = len(self.event_ids)\n",
    "        \n",
    "        # Store original samples for mixup\n",
    "        original_samples = [s[1].numpy() for s in self.samples]\n",
    "        original_labels = [s[0].item() for s in self.samples]\n",
    "\n",
    "        new_events = []\n",
    "        for idx in range(original_count):\n",
    "            base_features = original_samples[idx]\n",
    "            base_label = original_labels[idx]\n",
    "            \n",
    "            # Generate N-1 augmented versions\n",
    "            for _ in range(n_times-1):\n",
    "                # Apply augmentation\n",
    "                aug_features = self._apply_random_augmentation(\n",
    "                    base_features, \n",
    "                    mixup_samples=original_samples,\n",
    "                    mixup_labels=original_labels,\n",
    "                    **kwargs\n",
    "                )\n",
    "                \n",
    "                # Create event with unique ID\n",
    "                event_data = self._create_augmented_event(\n",
    "                    aug_features, new_event_id, base_label\n",
    "                )\n",
    "                new_events.append(event_data)\n",
    "                new_event_id += 1 \n",
    "\n",
    "        # Add mixup combinations\n",
    "        mixup_events = self._generate_mixup_combinations(\n",
    "            original_samples, original_labels,\n",
    "            n_combinations=original_count//2,\n",
    "            n_times=n_times,\n",
    "            **kwargs\n",
    "        )\n",
    "        new_events += mixup_events\n",
    "\n",
    "        self.df = pl.concat([self.df, *new_events])\n",
    "        self.event_ids = self.df['event_id'].unique().to_list()\n",
    "        self._precompute_samples()\n",
    "        return self\n",
    "\n",
    "    def _apply_random_augmentation(self, features, **kwargs):\n",
    "        aug_type = np.random.choice([\n",
    "            lambda x: self._gaussian_noise(x, kwargs['noise_std']),\n",
    "            lambda x: self._amplitude_scale(x, kwargs['scale_range']),\n",
    "            lambda x: self._time_warp(x, kwargs['warp_range']),\n",
    "            lambda x: self._channel_shift(x, kwargs['max_shift']),\n",
    "            lambda x: self._frequency_warp(x, kwargs['freq_shift']),\n",
    "            lambda x: self._time_mask(x, kwargs['mask_size']),\n",
    "            lambda x: self._channel_dropout(x, kwargs['drop_prob']),\n",
    "            lambda x: self._mixup(x, kwargs['mixup_samples'], \n",
    "                                kwargs['mixup_labels'], \n",
    "                                kwargs['mixup_alpha'])\n",
    "        ])\n",
    "        return aug_type(features)\n",
    "\n",
    "    # === Core Augmentations ===\n",
    "    def _gaussian_noise(self, features, noise_std=0.1, **kwargs):\n",
    "        noise = np.random.normal(0, noise_std*np.std(features), features.shape)\n",
    "        return features + noise\n",
    "\n",
    "    def _amplitude_scale(self, features, scale_range=(0.8, 1.2), **kwargs):\n",
    "        return features * np.random.uniform(*scale_range)\n",
    "\n",
    "    def _time_warp(self, features, warp_range=(0.8, 1.2)):\n",
    "\n",
    "        orig_length = features.shape[0]\n",
    "        warp_factor = np.random.uniform(*warp_range)\n",
    "        \n",
    "        # People's interpolation ensuring max_length compliance\n",
    "        x_original = np.linspace(0, 1, orig_length)\n",
    "        x_warped = np.linspace(0, 1, self.max_length)  # Always output max_length\n",
    "        \n",
    "        # Proletarian cubic spline interpolation\n",
    "        warped_features = np.array([\n",
    "            CubicSpline(x_original, channel)(x_warped)\n",
    "            for channel in features.T\n",
    "        ]).T\n",
    "        \n",
    "        return warped_features\n",
    "\n",
    "    def _channel_shift(self, features, max_shift=10, **kwargs):\n",
    "        return np.roll(features, np.random.randint(-max_shift, max_shift), axis=0)\n",
    "\n",
    "    def _frequency_warp(self, features, freq_shift=2, **kwargs):\n",
    "        f = np.fft.fft(features, axis=0)\n",
    "        shifted = np.roll(f, np.random.randint(-freq_shift, freq_shift), axis=0)\n",
    "        return np.real(np.fft.ifft(shifted))\n",
    "\n",
    "    def _time_mask(self, features, mask_size=50, **kwargs):\n",
    "        start = np.random.randint(0, len(features)-mask_size)\n",
    "        features[start:start+mask_size] *= np.hanning(mask_size)[:,None]\n",
    "        return features\n",
    "\n",
    "    def _channel_dropout(self, features, drop_prob=0.1, **kwargs):\n",
    "        mask = np.random.rand(features.shape[1]) > drop_prob\n",
    "        return features * mask\n",
    "\n",
    "    def _mixup(self, features, all_samples, all_labels, alpha=0.4, **kwargs):\n",
    "        idx = np.random.randint(0, len(all_samples))\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        return lam*features + (1-lam)*all_samples[idx]\n",
    "\n",
    "    def _generate_mixup_combinations(self, samples, labels, n_times, n_combinations=1000, alpha=0.4,**kwargs):\n",
    "        mixup_events = []\n",
    "        new_event_id = self.df['event_id'].max() + 1 + len(samples)*(n_times-1)\n",
    "        \n",
    "        for _ in range(n_combinations):\n",
    "            idx1, idx2 = np.random.choice(len(samples), 2, replace=False)\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "            \n",
    "            mixed_features = lam*samples[idx1] + (1-lam)*samples[idx2]\n",
    "            mixed_label = lam*labels[idx1] + (1-lam)*labels[idx2]\n",
    "            \n",
    "            event_data = self._create_augmented_event(\n",
    "                mixed_features,\n",
    "                new_event_id,\n",
    "                mixed_label\n",
    "            )\n",
    "            mixup_events.append(event_data)\n",
    "            new_event_id += 1\n",
    "            \n",
    "        return mixup_events\n",
    "\n",
    "    def _create_augmented_event(self, features, event_id, label):\n",
    "        # Ensure features match max_length\n",
    "        features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "        padded_features = self._pad_sequence(features_tensor).numpy()\n",
    "\n",
    "        event_data = {\n",
    "            \"event_id\": [event_id] * self.max_length,\n",
    "            \"marker\": [\"Stimulus/P\" if label else \"Stimulus/A\"] * self.max_length,\n",
    "            \"time\": np.arange(self.max_length)  # This creates Int64 values by default\n",
    "        }\n",
    "\n",
    "        # Revolutionary dtype conversion map\n",
    "        dtype_map = {\n",
    "            pl.Float64: np.float64,\n",
    "            pl.Float32: np.float32,\n",
    "            pl.Int64: np.int64,\n",
    "            pl.Int32: np.int32,\n",
    "            pl.Utf8: str\n",
    "        }\n",
    "\n",
    "        # Convert non-feature columns to the expected types\n",
    "        for key in [\"event_id\", \"marker\", \"time\"]:\n",
    "            expected_dtype = self.df.schema[key]\n",
    "            # Look up the target type based on the expected polars dtype.\n",
    "            target_type = dtype_map.get(type(expected_dtype), np.float64)\n",
    "            event_data[key] = np.array(event_data[key]).astype(target_type)\n",
    "\n",
    "        # Now handle the feature columns using the same logic as before.\n",
    "        for col_idx, col in enumerate(self.feature_cols):\n",
    "            dtype = self.df.schema[col]\n",
    "            if isinstance(dtype, pl.List):\n",
    "                base_type = dtype.inner\n",
    "                target_type = dtype_map.get(type(base_type), np.float64)\n",
    "            else:\n",
    "                target_type = dtype_map.get(type(dtype), np.float64)\n",
    "            event_data[col] = padded_features[:, col_idx].astype(target_type)\n",
    "\n",
    "        return pl.DataFrame(event_data)\n",
    "\n",
    "    \n",
    "    def compute_class_weights(self):\n",
    "        \"\"\"\n",
    "        Compute inverse frequency weights based on the 'marker' column.\n",
    "        Assumes markers are \"Stimulus/A\" and \"Stimulus/P\".\n",
    "        \"\"\"\n",
    "        # Get unique combinations of event_id and marker.\n",
    "        unique_events = self.df.select([\"event_id\", \"marker\"]).unique()\n",
    "        \n",
    "        # Use value_counts on the \"marker\" column.\n",
    "        counts_df = unique_events[\"marker\"].value_counts()\n",
    "\n",
    "        # We'll use 'values' if it exists, otherwise 'marker'.\n",
    "        d = { (row.get(\"values\") or row.get(\"marker\")): row[\"count\"] \n",
    "            for row in counts_df.to_dicts() }\n",
    "        \n",
    "        weight_A = 1.0 / d.get(\"Stimulus/A\", 1)\n",
    "        weight_P = 1.0 / d.get(\"Stimulus/P\", 1)\n",
    "        return {\"A\": weight_A, \"P\": weight_P}\n",
    "   \n",
    "    def split_dataset(self, ratios=(0.7, 0.15, 0.15), seed=None):\n",
    "        \"\"\"\n",
    "        Splits the dataset into three EEGDataset instances for train, val, and test.\n",
    "        This method shuffles the event_ids and then partitions them based on the given ratios.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Copy and shuffle the event_ids\n",
    "        event_ids = self.event_ids.copy()\n",
    "        np.random.shuffle(event_ids)\n",
    "        total = len(event_ids)\n",
    "        \n",
    "        n_train = int(ratios[0] * total)\n",
    "        n_val   = int(ratios[1] * total)\n",
    "        \n",
    "        train_ids = event_ids[:n_train]\n",
    "        val_ids   = event_ids[n_train:n_train+n_val]\n",
    "        test_ids  = event_ids[n_train+n_val:]\n",
    "        \n",
    "        # Filter self.df for the selected event_ids\n",
    "        train_df = self.df.filter(pl.col(\"event_id\").is_in(train_ids))\n",
    "        val_df   = self.df.filter(pl.col(\"event_id\").is_in(val_ids))\n",
    "        test_df  = self.df.filter(pl.col(\"event_id\").is_in(test_ids))\n",
    "        \n",
    "        # Create new EEGDataset instances using the filtered data\n",
    "        train_set = EEGDataset(train_df, self.max_length)\n",
    "        val_set   = EEGDataset(val_df, self.max_length)\n",
    "        test_set  = EEGDataset(test_df, self.max_length)\n",
    "        \n",
    "        return train_set, val_set, test_set\n",
    "    \n",
    "    def _load_and_filter(self, source):\n",
    "        df = pl.read_parquet(source) if isinstance(source, str) else source\n",
    "        df = df.filter(pl.col('marker').is_in([\"Stimulus/A\", \"Stimulus/P\"]))\n",
    "        if '__null_dask_index__' in df.columns:\n",
    "            df = df.drop('__null_dask_index__')\n",
    "        return df\n",
    "\n",
    "    def _pad_sequence(self, tensor):\n",
    "        # Pre-allocate tensor for maximum efficiency\n",
    "        padded = torch.zeros((self.max_length, tensor.size(1)), dtype=tensor.dtype)\n",
    "        length = min(tensor.size(0), self.max_length)\n",
    "        padded[:length] = tensor[:length]\n",
    "        return padded\n",
    "    \n",
    "    def apply_smote(self):\n",
    "        # Extract time-ordered features\n",
    "        X = np.stack([features.numpy().flatten() for _, features in self.samples])\n",
    "        y = np.array([label.item() for label, _ in self.samples])\n",
    "        \n",
    "        # Apply SMOTE\n",
    "        smote = SMOTE()\n",
    "        X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "        # Generate synthetic events\n",
    "        new_events = []\n",
    "        new_event_id = self.df['event_id'].max() + 1\n",
    "        time_base = np.arange(self.max_length)\n",
    "        original_schema = self.df.schema\n",
    "\n",
    "        # Create revolutionary dtype conversion map\n",
    "        dtype_map = {\n",
    "            pl.Float64: np.float64,\n",
    "            pl.Float32: np.float32,\n",
    "            pl.Int64: np.int64,\n",
    "            pl.Int32: np.int32,\n",
    "            pl.Utf8: str,\n",
    "        }\n",
    "\n",
    "        for features_flat, label in zip(X_res[len(self):], y_res[len(self):]):\n",
    "            features_2d = features_flat.reshape(self.max_length, -1)\n",
    "            event_data = {\n",
    "                \"event_id\": [new_event_id] * self.max_length,\n",
    "                \"marker\": [\"Stimulus/P\" if label else \"Stimulus/A\"] * self.max_length,\n",
    "                \"time\": time_base.copy()\n",
    "            }\n",
    "            \n",
    "            for col_idx, col in enumerate(self.feature_cols):\n",
    "                col_data = features_2d[:, col_idx]\n",
    "                schema_type = original_schema[col]\n",
    "                \n",
    "                # Handle nested types with dialectical precision\n",
    "                if isinstance(schema_type, pl.List):\n",
    "                    base_type = schema_type.inner\n",
    "                    col_data = col_data.astype(dtype_map.get(type(base_type), np.float64))\n",
    "                else:\n",
    "                    col_data = col_data.astype(dtype_map.get(type(schema_type), np.float64))\n",
    "                \n",
    "                # Enforce integer discipline\n",
    "                if schema_type in (pl.Int64, pl.Int32):\n",
    "                    col_data = np.round(col_data).astype(int)\n",
    "                \n",
    "                event_data[col] = col_data\n",
    "\n",
    "            # Cast to original schema with iron discipline\n",
    "            event_df = pl.DataFrame(event_data).cast(original_schema)\n",
    "            new_events.append(event_df)\n",
    "            new_event_id += 1\n",
    "\n",
    "        # Merge datasets\n",
    "        self.df = pl.concat([self.df, *new_events])\n",
    "        self.event_ids = self.df['event_id'].unique().to_list()\n",
    "        self._precompute_samples()\n",
    "        self._class_weights = self.compute_class_weights()\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for variable-length EEG feature sequences.\n",
    "\n",
    "    Each sample is expected to be a tuple (label, feature), where:\n",
    "    - label is a scalar tensor (or 1D tensor) representing the class/target.\n",
    "    - feature is a tensor of shape (seq_len, num_channels), where seq_len may vary.\n",
    "\n",
    "    This function stacks labels and pads features along the time dimension so that\n",
    "    all sequences in the batch have the same length.\n",
    "    \"\"\"\n",
    "    # Unzip the batch into labels and features\n",
    "    labels, features = zip(*batch)\n",
    "    \n",
    "    labels = torch.stack(labels)\n",
    "    # Optionally: labels = labels.unsqueeze(1)  # Uncomment if required by your loss function\n",
    "    padded_features = pad_sequence(features, batch_first=True)\n",
    "    \n",
    "    return labels, padded_features\n",
    "\n",
    "\n",
    "def train_model(config, train_set, train_loader, val_loader, writer):    \n",
    "    # Model initialization\n",
    "    model = EEGDSConv(dropout=config['dropout']).to(config['device'])\n",
    "    \n",
    "    # Log model architecture and config\n",
    "    writer.add_text(\"Model/Type\", f\"EEGDSConv with dropout={config['dropout']}\")\n",
    "    writer.add_text(\"Model/Structure\", str(model))\n",
    "    writer.add_text(\"Training Config\", str(config))\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    # pos_weight = torch.tensor([\n",
    "    #     train_set.class_weights['A'] / train_set.class_weights['P']\n",
    "    # ]).to(config['device'])\n",
    "    criterion = nn.BCEWithLogitsLoss()#pos_weight=pos_weight)\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['lr'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Learning rate schedulers\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lambda epoch: min(1.0, (epoch + 1) / config['warmup_epochs'])\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_metric = float('inf')\n",
    "    \n",
    "    for epoch in tqdm(range(config['epochs'])):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for labels, features in train_loader:\n",
    "            features = features.to(config['device'])\n",
    "            labels = labels.to(config['device'])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), \n",
    "                config['grad_clip']\n",
    "            )\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for labels, features in val_loader:\n",
    "                features = features.to(config['device'])\n",
    "                labels = labels.to(config['device'])\n",
    "                \n",
    "                outputs = model(features)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                \n",
    "                preds = torch.sigmoid(outputs)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        predictions = (np.array(all_preds) > 0.5).astype(int)\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Update schedulers\n",
    "        if epoch < config['warmup_epochs']:\n",
    "            warmup_scheduler.step()\n",
    "        else:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        # Log metrics and learning rate\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(all_labels, predictions),\n",
    "            'precision': precision_score(all_labels, predictions),\n",
    "            'recall': recall_score(all_labels, predictions),\n",
    "            'f1': f1_score(all_labels, predictions)\n",
    "        }\n",
    "        \n",
    "        writer.add_scalar('LR', current_lr, epoch)\n",
    "        writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Val', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy', metrics['accuracy'], epoch)\n",
    "        writer.add_scalar('Precision', metrics['precision'], epoch)\n",
    "        writer.add_scalar('Recall', metrics['recall'], epoch)\n",
    "        writer.add_scalar('F1', metrics['f1'], epoch)\n",
    "        writer.add_scalars('Metrics', metrics, epoch)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_metric:\n",
    "            best_metric = val_loss\n",
    "            torch.save(model.state_dict(), f\"{config['log_dir']}/best_model.pth\")\n",
    "    \n",
    "    writer.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating full dataset...\n"
     ]
    }
   ],
   "source": [
    "#============================================================\n",
    "# Training Pipeline\n",
    "#============================================================\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'data_path': '/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet',\n",
    "    'split_ratios': (0.7, 0.15, 0.15),\n",
    "    'batch_size': 32,          # Increased for better generalization\n",
    "    'dropout': 0.6,            # Reduced from 0.6 for better information flow\n",
    "    'lr': 5e-4,                # Base learning rate (sweet spot between 1e-5 and 3e-3)\n",
    "    'weight_decay': 5e-5,      # Increased regularization\n",
    "    'epochs': 2000,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'log_dir': None,#'./runs/CNN',\n",
    "    'lr_scheduler': {\n",
    "        'mode': 'min',\n",
    "        'factor': 0.1,         # More aggressive LR reduction\n",
    "        'patience': 5,         # Faster response to plateaus\n",
    "        'threshold': 0.001,\n",
    "        'cooldown': 3\n",
    "    },\n",
    "    'grad_clip': 1.0,          # Add gradient clipping\n",
    "    'warmup_epochs': 10        # Linear LR warmup\n",
    "}\n",
    "\n",
    "# Initialize dataset\n",
    "print(\"Creating full dataset...\")\n",
    "full_dataset = EEGDataset(config['data_path'])\n",
    "\n",
    "print(\"Splitting the dataset...\")\n",
    "# Split dataset\n",
    "train_set, val_set, test_set = full_dataset.split_dataset(\n",
    "    ratios=config['split_ratios']\n",
    ")\n",
    "\n",
    "del full_dataset\n",
    "\n",
    "len_dataset = len(train_set)\n",
    "sample = train_set[0]\n",
    "label_shape = sample[0].shape\n",
    "feature_shape = sample[1].shape\n",
    "\n",
    "print(f\"unbalanced train dataset shape: ({len_dataset}, [labels: {label_shape}, features: {list(feature_shape)}])\")\n",
    "\n",
    "# Balance training set\n",
    "print(\"Applying SMOTE to train dataset...\")\n",
    "train_set.apply_smote()\n",
    "\n",
    "print(\"Augmenting train dataset...\")\n",
    "train_set.augment_dataset(\n",
    "    n_times=3,              # Nx dataset expansion\n",
    "    noise_std=0.15,         # Moderate noise\n",
    "    scale_range=(0.7, 1.3), # ±30% amplitude variation\n",
    "    warp_range=(0.85, 1.15),# ±15% time warping\n",
    "    max_shift=15,           # 150ms temporal shifts\n",
    "    freq_shift=3,           # ±3Hz frequency shifts\n",
    "    mask_size=75,           # 750ms masking\n",
    "    drop_prob=0.15,         # 15% channel dropout\n",
    "    mixup_alpha=0.2         # Mixup\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=12,\n",
    "    pin_memory=True,      # For GPU acceleration\n",
    "    persistent_workers=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(val_set, batch_size=config['batch_size'], collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=config['batch_size'], collate_fn=collate_fn)\n",
    "\n",
    "len_dataset = len(train_set)\n",
    "sample = train_set[0]\n",
    "label_shape = sample[0].shape\n",
    "feature_shape = sample[1].shape\n",
    "\n",
    "print(f\"train dataset shape: ({len_dataset}, [labels: {label_shape}, features: {list(feature_shape)}])\")\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "writer = SummaryWriter(log_dir=config['log_dir'])\n",
    "\n",
    "# Start training\n",
    "trained_model = train_model(config, train_set, train_loader, val_loader, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48951/1139620167.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model = torch.load('best_model.torch')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming model, criterion, test_loader, device, writer, and epoch are already defined\u001b[39;00m\n\u001b[1;32m      6\u001b[0m best_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.torch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()\n\u001b[1;32m      8\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      9\u001b[0m all_test_markers \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# pos_weight = torch.tensor([\n",
    "#     train_set.class_weights['A'] / train_set.class_weights['P']\n",
    "# ]).to(config['device'])\n",
    "criterion = nn.BCEWithLogitsLoss()#pos_weight=pos_weight)\n",
    "\n",
    "epoch = 1\n",
    "# Assuming model, criterion, test_loader, device, writer, and epoch are already defined\n",
    "best_model = torch.load('best_model.torch')\n",
    "best_model.eval()\n",
    "test_loss = 0\n",
    "all_test_markers = []\n",
    "all_test_predictions = []\n",
    "with torch.no_grad():\n",
    "    for markers, features in tqdm(test_loader):\n",
    "        features = features.to(config['device'])\n",
    "        markers = markers.unsqueeze(-1).to(config['device'])\n",
    "\n",
    "        outputs = best_model(features)\n",
    "        loss = criterion(outputs, markers)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Collect markers and predictions for metrics calculation\n",
    "        all_test_markers.extend(markers.cpu().numpy().flatten())\n",
    "        all_test_predictions.extend(torch.sigmoid(outputs).cpu().numpy().flatten())\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "# Calculate test metrics\n",
    "test_accuracy = accuracy_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_precision = precision_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_recall = recall_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_f1 = f1_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_roc_auc = roc_auc_score(all_test_markers, all_test_predictions)\n",
    "\n",
    "# Log test metrics to TensorBoard\n",
    "writer.add_scalar('Metrics/test_accuracy', test_accuracy, epoch)\n",
    "writer.add_scalar('Metrics/test_precision', test_precision, epoch)\n",
    "writer.add_scalar('Metrics/test_recall', test_recall, epoch)\n",
    "writer.add_scalar('Metrics/test_f1', test_f1, epoch)\n",
    "writer.add_scalar('Metrics/test_roc_auc', test_roc_auc, epoch)\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "{test_accuracy=}\n",
    "{test_precision=}\n",
    "{test_recall=}\n",
    "{test_f1=}\n",
    "{test_roc_auc=}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "best_threshold = 0.0\n",
    "best_f1 = 0.0\n",
    "thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "\n",
    "for threshold in tqdm(thresholds):\n",
    "    binary_predictions = (all_test_predictions > threshold).astype(int)\n",
    "    current_recall = f1_score(all_test_markers, binary_predictions)\n",
    "\n",
    "    if current_recall > best_f1:\n",
    "        best_f1 = current_recall\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"{best_threshold=}\")\n",
    "print(f\"{best_f1=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "import numpy as np\n",
    "best_threshold = 0.1\n",
    "best_recall = 0.0\n",
    "thresholds = np.arange(0.1, 1.0, 0.01)\n",
    "\n",
    "for threshold in tqdm(thresholds):\n",
    "    binary_predictions = (all_test_predictions > threshold).astype(int)\n",
    "    current_recall = recall_score(all_test_markers, binary_predictions)\n",
    "\n",
    "    if current_recall > best_recall:\n",
    "        best_recall = current_recall\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"{best_threshold=}\")\n",
    "print(f\"{best_recall=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
