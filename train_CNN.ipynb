{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import polars as pl\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "# Set the seed for Python's built-in random module\n",
    "random.seed(69)\n",
    "\n",
    "# Set the seed for NumPy's random number generator\n",
    "np.random.seed(69)\n",
    "\n",
    "# Set the seed for PyTorch's random number generators\n",
    "torch.manual_seed(69)\n",
    "torch.cuda.manual_seed(69)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5_828_008, 67)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>event_id</th><th>marker</th><th>time</th><th>Fp1</th><th>Fpz</th><th>Fp2</th><th>F7</th><th>F3</th><th>Fz</th><th>F4</th><th>F8</th><th>FC5</th><th>FC1</th><th>FC2</th><th>FC6</th><th>M1</th><th>T7</th><th>C3</th><th>Cz</th><th>C4</th><th>T8</th><th>M2</th><th>CP5</th><th>CP1</th><th>CP2</th><th>CP6</th><th>P7</th><th>P3</th><th>Pz</th><th>P4</th><th>P8</th><th>POz</th><th>O1</th><th>O2</th><th>AF7</th><th>AF3</th><th>AF4</th><th>AF8</th><th>F5</th><th>F1</th><th>F2</th><th>F6</th><th>FC3</th><th>FCz</th><th>FC4</th><th>C5</th><th>C1</th><th>C2</th><th>C6</th><th>CP3</th><th>CP4</th><th>P5</th><th>P1</th><th>P2</th><th>P6</th><th>PO5</th><th>PO3</th><th>PO4</th><th>PO6</th><th>FT7</th><th>FT8</th><th>TP7</th><th>TP8</th><th>PO7</th><th>PO8</th><th>Oz</th><th>__null_dask_index__</th></tr><tr><td>i64</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td></tr></thead><tbody><tr><td>0</td><td>&quot;Stimulus/1&quot;</td><td>9.438</td><td>18.26099</td><td>16.979025</td><td>1.769803</td><td>2.441713</td><td>2.818692</td><td>10.922015</td><td>5.462004</td><td>-24.56427</td><td>16.242172</td><td>11.220099</td><td>11.027495</td><td>7.121442</td><td>-46.429313</td><td>-7.675472</td><td>16.945733</td><td>7.783233</td><td>20.514471</td><td>-26.908206</td><td>-47.461382</td><td>-4.487977</td><td>7.371627</td><td>2.944143</td><td>7.080501</td><td>-0.390081</td><td>-3.870911</td><td>16.039242</td><td>11.825064</td><td>0.929866</td><td>-0.176483</td><td>-10.09977</td><td>5.692728</td><td>-9.288844</td><td>3.758388</td><td>4.741033</td><td>-8.940138</td><td>5.21949</td><td>10.749776</td><td>7.172626</td><td>-17.153415</td><td>6.482526</td><td>13.326968</td><td>10.335974</td><td>1.308579</td><td>7.458288</td><td>4.62902</td><td>4.661003</td><td>10.387417</td><td>6.23994</td><td>-4.072595</td><td>10.350186</td><td>21.404569</td><td>5.531532</td><td>-5.54077</td><td>-4.60115</td><td>-10.072189</td><td>5.858149</td><td>-6.076038</td><td>-3.457559</td><td>-10.564211</td><td>-3.465435</td><td>-7.45234</td><td>5.723619</td><td>-10.401716</td><td>0</td></tr><tr><td>0</td><td>&quot;Stimulus/1&quot;</td><td>9.44</td><td>18.279552</td><td>17.167071</td><td>1.736013</td><td>2.509046</td><td>3.320807</td><td>11.62726</td><td>6.56513</td><td>-23.024749</td><td>16.535206</td><td>11.428954</td><td>11.867022</td><td>8.841448</td><td>-45.932599</td><td>-8.458003</td><td>16.839472</td><td>7.635926</td><td>21.321651</td><td>-31.699151</td><td>-46.974629</td><td>-4.697016</td><td>7.125317</td><td>3.220561</td><td>7.522331</td><td>-0.681903</td><td>-4.617159</td><td>15.836581</td><td>11.514513</td><td>-0.039646</td><td>-0.730373</td><td>-10.741345</td><td>3.741726</td><td>-9.673243</td><td>4.203654</td><td>5.521279</td><td>-8.967844</td><td>5.960125</td><td>11.30632</td><td>7.854544</td><td>-16.257055</td><td>6.743387</td><td>13.844415</td><td>11.380757</td><td>1.33881</td><td>7.527234</td><td>4.991126</td><td>7.051428</td><td>10.11894</td><td>6.555342</td><td>-4.722692</td><td>9.938277</td><td>21.260168</td><td>4.689456</td><td>-6.292506</td><td>-5.309328</td><td>-10.643972</td><td>4.269458</td><td>-6.793052</td><td>-2.42154</td><td>-10.742051</td><td>-3.052358</td><td>-7.947506</td><td>4.051073</td><td>-10.757459</td><td>1</td></tr><tr><td>0</td><td>&quot;Stimulus/1&quot;</td><td>9.442</td><td>18.187657</td><td>17.195618</td><td>1.650717</td><td>2.619466</td><td>3.746728</td><td>12.211854</td><td>7.602871</td><td>-21.453501</td><td>16.712602</td><td>11.625626</td><td>12.706299</td><td>10.281649</td><td>-45.249379</td><td>-9.02851</td><td>16.747652</td><td>7.526668</td><td>21.978811</td><td>-34.687629</td><td>-46.088848</td><td>-4.949806</td><td>6.908232</td><td>3.4634</td><td>7.742669</td><td>-0.509808</td><td>-5.343742</td><td>15.644091</td><td>11.098233</td><td>-1.040845</td><td>-1.317047</td><td>-11.424223</td><td>1.463857</td><td>-10.139782</td><td>4.563526</td><td>6.382203</td><td>-8.837637</td><td>6.864058</td><td>11.892868</td><td>8.451628</td><td>-15.265568</td><td>6.959286</td><td>14.266943</td><td>12.201225</td><td>1.263118</td><td>7.552298</td><td>5.227928</td><td>8.960626</td><td>9.860875</td><td>6.811119</td><td>-5.368829</td><td>9.622294</td><td>21.128975</td><td>3.794791</td><td>-7.027202</td><td>-5.970655</td><td>-11.33015</td><td>2.269126</td><td>-7.381371</td><td>-1.549136</td><td>-10.951647</td><td>-2.573051</td><td>-8.422673</td><td>1.975578</td><td>-11.568488</td><td>2</td></tr><tr><td>0</td><td>&quot;Stimulus/1&quot;</td><td>9.444</td><td>17.988738</td><td>17.064795</td><td>1.536693</td><td>2.780392</td><td>4.108368</td><td>12.665638</td><td>8.47708</td><td>-19.98386</td><td>16.771917</td><td>11.816801</td><td>13.476138</td><td>11.291504</td><td>-44.341041</td><td>-9.36457</td><td>16.66188</td><td>7.476511</td><td>22.42124</td><td>-35.46427</td><td>-44.869256</td><td>-5.247474</td><td>6.718476</td><td>3.639658</td><td>7.712424</td><td>0.094348</td><td>-6.073221</td><td>15.46495</td><td>10.593378</td><td>-1.921139</td><td>-1.925938</td><td>-12.227119</td><td>-0.95602</td><td>-10.635339</td><td>4.843329</td><td>7.244236</td><td>-8.559468</td><td>7.826785</td><td>12.502754</td><td>8.935521</td><td>-14.259979</td><td>7.13315</td><td>14.590537</td><td>12.723916</td><td>1.071259</td><td>7.535559</td><td>5.311675</td><td>10.193048</td><td>9.607037</td><td>6.978706</td><td>-6.039169</td><td>9.398759</td><td>21.022386</td><td>2.90171</td><td>-7.784325</td><td>-6.628905</td><td>-12.088001</td><td>0.015087</td><td>-7.783466</td><td>-0.885222</td><td>-11.181192</td><td>-2.05307</td><td>-8.959252</td><td>-0.346699</td><td>-12.895379</td><td>3</td></tr><tr><td>0</td><td>&quot;Stimulus/1&quot;</td><td>9.446</td><td>17.697904</td><td>16.795464</td><td>1.420282</td><td>2.984275</td><td>4.421709</td><td>12.995026</td><td>9.112953</td><td>-18.737067</td><td>16.720522</td><td>12.012921</td><td>14.115175</td><td>11.774442</td><td>-43.205799</td><td>-9.5169</td><td>16.576344</td><td>7.49871</td><td>22.614753</td><td>-33.835611</td><td>-43.398607</td><td>-5.583868</td><td>6.552261</td><td>3.723286</td><td>7.438797</td><td>1.040869</td><td>-6.831202</td><td>15.294334</td><td>10.025392</td><td>-2.545744</td><td>-2.555724</td><td>-13.217374</td><td>-3.333748</td><td>-11.107698</td><td>5.055388</td><td>8.033596</td><td>-8.161774</td><td>8.723206</td><td>13.129667</td><td>9.293896</td><td>-13.315702</td><td>7.273473</td><td>14.822615</td><td>12.913112</td><td>0.770779</td><td>7.485243</td><td>5.235299</td><td>10.630651</td><td>9.349522</td><td>7.039159</td><td>-6.762365</td><td>9.247924</td><td>20.94341</td><td>2.056106</td><td>-8.602303</td><td>-7.331279</td><td>-12.87647</td><td>-2.320403</td><td>-7.966721</td><td>-0.441055</td><td>-11.417038</td><td>-1.496988</td><td>-9.632474</td><td>-2.74458</td><td>-14.742146</td><td>4</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1381</td><td>&quot;Stimulus/A&quot;</td><td>928.476</td><td>-24.531312</td><td>-34.040322</td><td>-39.377348</td><td>-26.813586</td><td>-50.33518</td><td>-60.351025</td><td>4.800104</td><td>-8.681692</td><td>-22.494535</td><td>-34.812206</td><td>-23.985204</td><td>-18.962696</td><td>-70.486005</td><td>-15.036631</td><td>-48.358653</td><td>-36.797774</td><td>-37.581036</td><td>-10.546239</td><td>-113.35881</td><td>-13.174225</td><td>-36.488425</td><td>-15.568561</td><td>8.284046</td><td>-25.535649</td><td>-14.563875</td><td>-30.695258</td><td>-80.903069</td><td>-73.376497</td><td>-19.759668</td><td>-16.861085</td><td>-30.762578</td><td>-27.605438</td><td>-26.903408</td><td>-11.824485</td><td>-16.397532</td><td>-31.682237</td><td>-50.844306</td><td>4.70073</td><td>-13.917587</td><td>20.762969</td><td>-43.113357</td><td>-18.824065</td><td>-0.801963</td><td>-51.926852</td><td>-28.117319</td><td>-8.311726</td><td>-33.086039</td><td>-28.030511</td><td>-34.279603</td><td>-30.71269</td><td>-43.739573</td><td>-41.626016</td><td>-23.895303</td><td>-24.468659</td><td>-51.56217</td><td>-33.35497</td><td>-24.890898</td><td>-6.698645</td><td>-0.427032</td><td>2.81708</td><td>-23.857567</td><td>-33.41483</td><td>-28.665009</td><td>275996</td></tr><tr><td>1381</td><td>&quot;Stimulus/A&quot;</td><td>928.478</td><td>-25.037469</td><td>-34.506067</td><td>-39.484176</td><td>-26.978022</td><td>-50.013997</td><td>-59.910298</td><td>5.519787</td><td>-8.487145</td><td>-22.16917</td><td>-34.808108</td><td>-22.969049</td><td>-18.345097</td><td>-70.520443</td><td>-14.521938</td><td>-48.600807</td><td>-36.721548</td><td>-36.762816</td><td>-11.203407</td><td>-113.115348</td><td>-12.976387</td><td>-36.500322</td><td>-15.216633</td><td>8.792874</td><td>-25.335829</td><td>-14.095602</td><td>-30.442555</td><td>-80.504525</td><td>-72.604594</td><td>-19.146617</td><td>-16.718005</td><td>-30.454802</td><td>-27.996757</td><td>-26.544687</td><td>-11.544485</td><td>-16.534841</td><td>-32.275165</td><td>-50.363906</td><td>5.478738</td><td>-13.681101</td><td>21.007286</td><td>-42.716912</td><td>-17.8001</td><td>-0.705913</td><td>-51.988347</td><td>-27.745902</td><td>-7.574538</td><td>-33.135223</td><td>-27.514517</td><td>-33.821243</td><td>-30.579896</td><td>-43.33259</td><td>-40.926079</td><td>-23.535917</td><td>-24.139827</td><td>-51.252728</td><td>-32.873982</td><td>-24.360851</td><td>-6.914038</td><td>0.18291</td><td>3.346472</td><td>-23.528204</td><td>-33.089643</td><td>-28.450162</td><td>275997</td></tr><tr><td>1381</td><td>&quot;Stimulus/A&quot;</td><td>928.48</td><td>-25.620556</td><td>-35.009315</td><td>-39.657826</td><td>-26.904605</td><td>-49.721285</td><td>-59.499187</td><td>6.209261</td><td>-8.234116</td><td>-21.743517</td><td>-34.988863</td><td>-21.930894</td><td>-17.584589</td><td>-70.335446</td><td>-13.765635</td><td>-48.915458</td><td>-36.627762</td><td>-35.817168</td><td>-11.387478</td><td>-112.422118</td><td>-12.591521</td><td>-36.460628</td><td>-14.724547</td><td>9.497388</td><td>-24.645177</td><td>-13.541097</td><td>-30.280727</td><td>-80.109167</td><td>-71.66989</td><td>-18.517414</td><td>-16.08701</td><td>-29.877011</td><td>-28.460028</td><td>-26.272782</td><td>-11.284485</td><td>-16.734728</td><td>-32.675661</td><td>-49.959505</td><td>6.218403</td><td>-13.581815</td><td>20.960643</td><td>-42.455285</td><td>-16.764513</td><td>-0.610304</td><td>-52.093277</td><td>-27.316784</td><td>-6.802836</td><td>-33.153469</td><td>-26.83351</td><td>-33.055253</td><td>-30.375729</td><td>-42.902987</td><td>-39.930319</td><td>-22.712706</td><td>-23.350402</td><td>-50.835199</td><td>-32.165625</td><td>-23.772179</td><td>-7.21466</td><td>0.826389</td><td>4.131466</td><td>-22.842623</td><td>-32.538119</td><td>-27.981675</td><td>275998</td></tr><tr><td>1381</td><td>&quot;Stimulus/A&quot;</td><td>928.482</td><td>-26.238952</td><td>-35.513318</td><td>-39.901309</td><td>-26.594463</td><td>-49.483993</td><td>-59.151322</td><td>6.795769</td><td>-7.967241</td><td>-21.255433</td><td>-35.329352</td><td>-20.969524</td><td>-16.78674</td><td>-69.982398</td><td>-12.855594</td><td>-49.282959</td><td>-36.526949</td><td>-34.840783</td><td>-11.073768</td><td>-111.400716</td><td>-12.068991</td><td>-36.399162</td><td>-14.130256</td><td>10.335449</td><td>-23.548158</td><td>-12.943701</td><td>-30.205676</td><td>-79.746777</td><td>-70.648907</td><td>-17.896494</td><td>-14.986795</td><td>-29.04357</td><td>-28.964238</td><td>-26.107885</td><td>-11.073393</td><td>-17.012841</td><td>-32.852155</td><td>-49.660259</td><td>6.842462</td><td>-13.651779</td><td>20.64128</td><td>-42.328665</td><td>-15.820084</td><td>-0.52463</td><td>-52.230964</td><td>-26.872209</td><td>-6.109631</td><td>-33.145934</td><td>-26.047849</td><td>-32.062022</td><td>-30.112813</td><td>-42.472899</td><td>-38.711372</td><td>-21.477316</td><td>-22.149748</td><td>-50.330373</td><td>-31.281584</td><td>-23.187269</td><td>-7.57299</td><td>1.419604</td><td>5.113047</td><td>-21.838447</td><td>-31.800604</td><td>-27.246317</td><td>275999</td></tr><tr><td>1381</td><td>&quot;Stimulus/A&quot;</td><td>928.484</td><td>-26.874588</td><td>-36.008758</td><td>-40.23387</td><td>-26.101266</td><td>-49.334218</td><td>-58.906511</td><td>7.204739</td><td>-7.74635</td><td>-20.765517</td><td>-35.790732</td><td>-20.185299</td><td>-16.08212</td><td>-69.546499</td><td>-11.908876</td><td>-49.680933</td><td>-36.432498</td><td>-33.951982</td><td>-10.311512</td><td>-110.205475</td><td>-11.481999</td><td>-36.348371</td><td>-13.491591</td><td>11.214009</td><td>-22.180664</td><td>-12.355926</td><td>-30.207402</td><td>-79.446623</td><td>-69.632709</td><td>-17.310454</td><td>-13.506752</td><td>-27.999419</td><td>-29.500396</td><td>-26.075559</td><td>-10.945396</td><td>-17.390696</td><td>-32.821619</td><td>-49.493424</td><td>7.274364</td><td>-13.921328</td><td>20.093928</td><td>-42.328779</td><td>-15.069552</td><td>-0.467267</td><td>-52.388849</td><td>-26.463104</td><td>-5.614215</td><td>-33.122897</td><td>-25.243646</td><td>-30.955288</td><td>-29.814831</td><td>-42.071956</td><td>-37.376735</td><td>-19.938908</td><td>-20.64321</td><td>-49.773965</td><td>-30.297274</td><td>-22.686364</td><td>-7.963246</td><td>1.87742</td><td>6.19621</td><td>-20.597579</td><td>-30.941484</td><td>-26.271603</td><td>276000</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5_828_008, 67)\n",
       "┌──────────┬────────────┬─────────┬────────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ event_id ┆ marker     ┆ time    ┆ Fp1        ┆ … ┆ PO7       ┆ PO8       ┆ Oz        ┆ __null_da │\n",
       "│ ---      ┆ ---        ┆ ---     ┆ ---        ┆   ┆ ---       ┆ ---       ┆ ---       ┆ sk_index_ │\n",
       "│ i64      ┆ str        ┆ f64     ┆ f64        ┆   ┆ f64       ┆ f64       ┆ f64       ┆ _         │\n",
       "│          ┆            ┆         ┆            ┆   ┆           ┆           ┆           ┆ ---       │\n",
       "│          ┆            ┆         ┆            ┆   ┆           ┆           ┆           ┆ i64       │\n",
       "╞══════════╪════════════╪═════════╪════════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 0        ┆ Stimulus/1 ┆ 9.438   ┆ 18.26099   ┆ … ┆ -7.45234  ┆ 5.723619  ┆ -10.40171 ┆ 0         │\n",
       "│          ┆            ┆         ┆            ┆   ┆           ┆           ┆ 6         ┆           │\n",
       "│ 0        ┆ Stimulus/1 ┆ 9.44    ┆ 18.279552  ┆ … ┆ -7.947506 ┆ 4.051073  ┆ -10.75745 ┆ 1         │\n",
       "│          ┆            ┆         ┆            ┆   ┆           ┆           ┆ 9         ┆           │\n",
       "│ 0        ┆ Stimulus/1 ┆ 9.442   ┆ 18.187657  ┆ … ┆ -8.422673 ┆ 1.975578  ┆ -11.56848 ┆ 2         │\n",
       "│          ┆            ┆         ┆            ┆   ┆           ┆           ┆ 8         ┆           │\n",
       "│ 0        ┆ Stimulus/1 ┆ 9.444   ┆ 17.988738  ┆ … ┆ -8.959252 ┆ -0.346699 ┆ -12.89537 ┆ 3         │\n",
       "│          ┆            ┆         ┆            ┆   ┆           ┆           ┆ 9         ┆           │\n",
       "│ 0        ┆ Stimulus/1 ┆ 9.446   ┆ 17.697904  ┆ … ┆ -9.632474 ┆ -2.74458  ┆ -14.74214 ┆ 4         │\n",
       "│          ┆            ┆         ┆            ┆   ┆           ┆           ┆ 6         ┆           │\n",
       "│ …        ┆ …          ┆ …       ┆ …          ┆ … ┆ …         ┆ …         ┆ …         ┆ …         │\n",
       "│ 1381     ┆ Stimulus/A ┆ 928.476 ┆ -24.531312 ┆ … ┆ -23.85756 ┆ -33.41483 ┆ -28.66500 ┆ 275996    │\n",
       "│          ┆            ┆         ┆            ┆   ┆ 7         ┆           ┆ 9         ┆           │\n",
       "│ 1381     ┆ Stimulus/A ┆ 928.478 ┆ -25.037469 ┆ … ┆ -23.52820 ┆ -33.08964 ┆ -28.45016 ┆ 275997    │\n",
       "│          ┆            ┆         ┆            ┆   ┆ 4         ┆ 3         ┆ 2         ┆           │\n",
       "│ 1381     ┆ Stimulus/A ┆ 928.48  ┆ -25.620556 ┆ … ┆ -22.84262 ┆ -32.53811 ┆ -27.98167 ┆ 275998    │\n",
       "│          ┆            ┆         ┆            ┆   ┆ 3         ┆ 9         ┆ 5         ┆           │\n",
       "│ 1381     ┆ Stimulus/A ┆ 928.482 ┆ -26.238952 ┆ … ┆ -21.83844 ┆ -31.80060 ┆ -27.24631 ┆ 275999    │\n",
       "│          ┆            ┆         ┆            ┆   ┆ 7         ┆ 4         ┆ 7         ┆           │\n",
       "│ 1381     ┆ Stimulus/A ┆ 928.484 ┆ -26.874588 ┆ … ┆ -20.59757 ┆ -30.94148 ┆ -26.27160 ┆ 276000    │\n",
       "│          ┆            ┆         ┆            ┆   ┆ 9         ┆ 4         ┆ 3         ┆           │\n",
       "└──────────┴────────────┴─────────┴────────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.read_parquet('/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet')\\\n",
    "    # .drop(['__null_dask_index__'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #============================================================\n",
    "# # Model Architecture\n",
    "# #============================================================\n",
    "# class EEGDSConv(nn.Module):\n",
    "#     def __init__(self, dropout=0.5):\n",
    "#         super().__init__()\n",
    "#         self.block = nn.Sequential(\n",
    "#             nn.Conv1d(64, 64, 15, padding='same', groups=64),\n",
    "#             nn.Conv1d(64, 16, 1),\n",
    "#             nn.BatchNorm1d(16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool1d(4),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Conv1d(16, 16, 7, padding='same', groups=16),\n",
    "#             nn.Conv1d(16, 8, 1),\n",
    "#             nn.BatchNorm1d(8),\n",
    "#             nn.ReLU(),\n",
    "#             nn.AdaptiveAvgPool1d(1),\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(8, 1)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = x.permute(0, 2, 1)\n",
    "#         return self.block(x).squeeze(-1)  # Squeeze last dimension to match target shape\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGMobileNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(63, 32, 15, padding=7),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU6(),\n",
    "            \n",
    "            DepthwiseSeparable(32, 64),\n",
    "            DepthwiseSeparable(64, 128),\n",
    "            DepthwiseSeparable(128, 256),\n",
    "            \n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return self.model(x).squeeze(-1)\n",
    "\n",
    "class DepthwiseSeparable(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv1d(in_ch, in_ch, 15, \n",
    "                                 padding=7, groups=in_ch)\n",
    "        self.pointwise = nn.Conv1d(in_ch, out_ch, 1)\n",
    "        self.bn = nn.BatchNorm1d(out_ch)\n",
    "        self.act = nn.ReLU6()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return self.act(self.bn(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tslearn.neighbors import KNeighborsTimeSeries\n",
    "from numba import njit\n",
    "\n",
    "# Standalone JIT-compiled function outside the class\n",
    "@njit(fastmath=True)\n",
    "def fast_interpolate(original, neighbor, alpha):\n",
    "    \"\"\"Numba-accelerated linear interpolation\"\"\"\n",
    "    return (1 - alpha) * original + alpha * neighbor\n",
    "\n",
    "class TSMOTE:\n",
    "    def __init__(self, n_neighbors=3, time_slices=10):  # Keep original 2000 timesteps\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.time_slices = time_slices\n",
    "        self.slice_size = 200  # 2000/10=200\n",
    "\n",
    "    def _slice_time_series(self, X):\n",
    "        \"\"\"Split into time slices while maintaining full series structure\"\"\"\n",
    "        return X.reshape(X.shape[0], self.time_slices, self.slice_size, X.shape[2])\n",
    "\n",
    "    def _reconstruct_full_series(self, synthetic_slices):\n",
    "        \"\"\"Combine synthetic slices into full-length time series\"\"\"\n",
    "        return synthetic_slices.reshape(-1, self.time_slices * self.slice_size, synthetic_slices.shape[-1])\n",
    "\n",
    "    def _generate_synthetic(self, minority_samples):\n",
    "        \"\"\"Generate full-length synthetic samples\"\"\"\n",
    "        sliced_data = self._slice_time_series(minority_samples)  # (N, slices, 200, ch)\n",
    "        syn_samples = []\n",
    "        \n",
    "        # Generate 1 full synthetic sample per minority sample\n",
    "        for sample_idx in tqdm(range(sliced_data.shape[0])):\n",
    "            synthetic_slices = []\n",
    "            \n",
    "            # Process each time slice\n",
    "            for slice_idx in range(self.time_slices):\n",
    "                knn = KNeighborsTimeSeries(n_neighbors=self.n_neighbors, metric='dtw')\n",
    "                knn.fit(sliced_data[:, slice_idx, :, :])\n",
    "                \n",
    "                # Find neighbors for this slice\n",
    "                neighbors = knn.kneighbors(sliced_data[sample_idx, slice_idx][np.newaxis], \n",
    "                                         return_distance=False)[0]\n",
    "                neighbor_idx = np.random.choice(neighbors)\n",
    "                \n",
    "                # Interpolate within slice\n",
    "                alpha = np.random.uniform(0.2, 0.8)\n",
    "                synthetic_slice = (1 - alpha) * sliced_data[sample_idx, slice_idx] + \\\n",
    "                                 alpha * sliced_data[neighbor_idx, slice_idx]\n",
    "                synthetic_slices.append(synthetic_slice)\n",
    "            \n",
    "            # Combine slices into full series\n",
    "            full_series = np.concatenate(synthetic_slices, axis=0)\n",
    "            syn_samples.append(full_series)\n",
    "        \n",
    "        return np.array(syn_samples)\n",
    "\n",
    "    def fit_resample(self, X, y):\n",
    "        y_int = y.astype(int)\n",
    "        class_counts = np.bincount(y_int)\n",
    "        minority_class = np.argmin(class_counts)\n",
    "        n_needed = class_counts[1 - minority_class] - class_counts[minority_class]\n",
    "        \n",
    "        if n_needed <= 0:\n",
    "            return X, y\n",
    "        \n",
    "        minority_samples = X[y_int == minority_class]\n",
    "        synthetic = self._generate_synthetic(minority_samples)\n",
    "        \n",
    "        # Ensure matching dimensions\n",
    "        assert X.shape[1:] == synthetic.shape[1:], \\\n",
    "            f\"Dimension mismatch: Original {X.shape[1:]}, Synthetic {synthetic.shape[1:]}\"\n",
    "        \n",
    "        return (np.concatenate([X, synthetic[:n_needed]], axis=0),\n",
    "                np.concatenate([y, [minority_class] * n_needed]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# Enhanced Dataset Class with Proper Encapsulation\n",
    "#============================================================\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, source, max_length=2000):\n",
    "        self.df = self._load_and_filter(source)\n",
    "        self.event_ids = self.df['event_id'].unique().to_list()\n",
    "        self.max_length = max_length\n",
    "        # Keep time for sorting but exclude from features\n",
    "        self.feature_cols = [c for c in self.df.columns \n",
    "                           if c not in {'event_id', 'marker', 'time'}]\n",
    "        self._precompute_samples()\n",
    "        self._class_weights = self.compute_class_weights()\n",
    "    \n",
    "    @property\n",
    "    def class_weights(self):\n",
    "        # Expose the computed weights as a property.\n",
    "        return self._class_weights \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.event_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "    \n",
    "    def _precompute_samples(self):\n",
    "        \"\"\"Cache time-ordered samples with revolutionary discipline\"\"\"\n",
    "        self.samples = []\n",
    "        for event_id in self.event_ids:\n",
    "            # Sort by time within each event!\n",
    "            event_data = self.df.filter(pl.col(\"event_id\") == event_id).sort(\"time\")\n",
    "            \n",
    "            features = torch.tensor(\n",
    "                event_data.select(self.feature_cols).to_numpy(),\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "            features = self._pad_sequence(features)\n",
    "            \n",
    "            label = 1.0 if event_data['marker'][0] == \"Stimulus/P\" else 0.0\n",
    "            self.samples.append((\n",
    "                torch.tensor(label, dtype=torch.float32), \n",
    "                features\n",
    "            ))\n",
    "    \n",
    "    def augment_dataset(self, n_times=5, **kwargs):\n",
    "        new_event_id = self.df[\"event_id\"].max() + 1\n",
    "        original_count = len(self.event_ids)\n",
    "        \n",
    "        # Store original samples for mixup\n",
    "        original_samples = [s[1].numpy() for s in self.samples]\n",
    "        original_labels = [s[0].item() for s in self.samples]\n",
    "\n",
    "        new_events = []\n",
    "        for idx in range(original_count):\n",
    "            base_features = original_samples[idx]\n",
    "            base_label = original_labels[idx]\n",
    "            \n",
    "            # Generate N-1 augmented versions\n",
    "            for _ in range(n_times-1):\n",
    "                # Apply augmentation\n",
    "                aug_features = self._apply_random_augmentation(\n",
    "                    base_features, \n",
    "                    mixup_samples=original_samples,\n",
    "                    mixup_labels=original_labels,\n",
    "                    **kwargs\n",
    "                )\n",
    "                \n",
    "                # Create event with unique ID\n",
    "                event_data = self._create_augmented_event(\n",
    "                    aug_features, new_event_id, base_label\n",
    "                )\n",
    "                new_events.append(event_data)\n",
    "                new_event_id += 1 \n",
    "\n",
    "        # Add mixup combinations\n",
    "        mixup_events = self._generate_mixup_combinations(\n",
    "            original_samples, original_labels,\n",
    "            n_combinations=original_count//2,\n",
    "            n_times=n_times,\n",
    "            **kwargs\n",
    "        )\n",
    "        new_events += mixup_events\n",
    "\n",
    "        self.df = pl.concat([self.df, *new_events])\n",
    "        self.event_ids = self.df['event_id'].unique().to_list()\n",
    "        self._precompute_samples()\n",
    "        return self\n",
    "\n",
    "    def _apply_random_augmentation(self, features, **kwargs):\n",
    "        aug_type = np.random.choice([\n",
    "            lambda x: self._gaussian_noise(x, kwargs['noise_std']),\n",
    "            lambda x: self._amplitude_scale(x, kwargs['scale_range']),\n",
    "            lambda x: self._time_warp(x, kwargs['warp_range']),\n",
    "            lambda x: self._channel_shift(x, kwargs['max_shift']),\n",
    "            lambda x: self._frequency_warp(x, kwargs['freq_shift']),\n",
    "            lambda x: self._time_mask(x, kwargs['mask_size']),\n",
    "            lambda x: self._channel_dropout(x, kwargs['drop_prob']),\n",
    "            lambda x: self._mixup(x, kwargs['mixup_samples'], \n",
    "                                kwargs['mixup_labels'], \n",
    "                                kwargs['mixup_alpha'])\n",
    "        ])\n",
    "        return aug_type(features)\n",
    "\n",
    "    # === Core Augmentations ===\n",
    "    def _gaussian_noise(self, features, noise_std=0.1, **kwargs):\n",
    "        noise = np.random.normal(0, noise_std*np.std(features), features.shape)\n",
    "        return features + noise\n",
    "\n",
    "    def _amplitude_scale(self, features, scale_range=(0.8, 1.2), **kwargs):\n",
    "        return features * np.random.uniform(*scale_range)\n",
    "\n",
    "    def _time_warp(self, features, warp_range=(0.8, 1.2)):\n",
    "\n",
    "        orig_length = features.shape[0]\n",
    "        warp_factor = np.random.uniform(*warp_range)\n",
    "        \n",
    "        # People's interpolation ensuring max_length compliance\n",
    "        x_original = np.linspace(0, 1, orig_length)\n",
    "        x_warped = np.linspace(0, 1, self.max_length)  # Always output max_length\n",
    "        \n",
    "        # Proletarian cubic spline interpolation\n",
    "        warped_features = np.array([\n",
    "            CubicSpline(x_original, channel)(x_warped)\n",
    "            for channel in features.T\n",
    "        ]).T\n",
    "        \n",
    "        return warped_features\n",
    "\n",
    "    def _channel_shift(self, features, max_shift=10, **kwargs):\n",
    "        return np.roll(features, np.random.randint(-max_shift, max_shift), axis=0)\n",
    "\n",
    "    def _frequency_warp(self, features, freq_shift=2, **kwargs):\n",
    "        f = np.fft.fft(features, axis=0)\n",
    "        shifted = np.roll(f, np.random.randint(-freq_shift, freq_shift), axis=0)\n",
    "        return np.real(np.fft.ifft(shifted))\n",
    "\n",
    "    def _time_mask(self, features, mask_size=50, **kwargs):\n",
    "        start = np.random.randint(0, len(features)-mask_size)\n",
    "        features[start:start+mask_size] *= np.hanning(mask_size)[:,None]\n",
    "        return features\n",
    "\n",
    "    def _channel_dropout(self, features, drop_prob=0.1, **kwargs):\n",
    "        mask = np.random.rand(features.shape[1]) > drop_prob\n",
    "        return features * mask\n",
    "\n",
    "    def _mixup(self, features, all_samples, all_labels, alpha=0.4, **kwargs):\n",
    "        idx = np.random.randint(0, len(all_samples))\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        return lam*features + (1-lam)*all_samples[idx]\n",
    "\n",
    "    def _generate_mixup_combinations(self, samples, labels, n_times, n_combinations=1000, alpha=0.4,**kwargs):\n",
    "        mixup_events = []\n",
    "        new_event_id = self.df['event_id'].max() + 1 + len(samples)*(n_times-1)\n",
    "        \n",
    "        for _ in range(n_combinations):\n",
    "            idx1, idx2 = np.random.choice(len(samples), 2, replace=False)\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "            \n",
    "            mixed_features = lam*samples[idx1] + (1-lam)*samples[idx2]\n",
    "            mixed_label = lam*labels[idx1] + (1-lam)*labels[idx2]\n",
    "            \n",
    "            event_data = self._create_augmented_event(\n",
    "                mixed_features,\n",
    "                new_event_id,\n",
    "                mixed_label\n",
    "            )\n",
    "            mixup_events.append(event_data)\n",
    "            new_event_id += 1\n",
    "            \n",
    "        return mixup_events\n",
    "\n",
    "    def _create_augmented_event(self, features, event_id, label):\n",
    "        # Ensure features match max_length\n",
    "        features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "        padded_features = self._pad_sequence(features_tensor).numpy()\n",
    "\n",
    "        event_data = {\n",
    "            \"event_id\": [event_id] * self.max_length,\n",
    "            \"marker\": [\"Stimulus/P\" if label else \"Stimulus/A\"] * self.max_length,\n",
    "            \"time\": np.arange(self.max_length)  # This creates Int64 values by default\n",
    "        }\n",
    "\n",
    "        # Revolutionary dtype conversion map\n",
    "        dtype_map = {\n",
    "            pl.Float64: np.float64,\n",
    "            pl.Float32: np.float32,\n",
    "            pl.Int64: np.int64,\n",
    "            pl.Int32: np.int32,\n",
    "            pl.Utf8: str\n",
    "        }\n",
    "\n",
    "        # Convert non-feature columns to the expected types\n",
    "        for key in [\"event_id\", \"marker\", \"time\"]:\n",
    "            expected_dtype = self.df.schema[key]\n",
    "            # Look up the target type based on the expected polars dtype.\n",
    "            target_type = dtype_map.get(type(expected_dtype), np.float64)\n",
    "            event_data[key] = np.array(event_data[key]).astype(target_type)\n",
    "\n",
    "        # Now handle the feature columns using the same logic as before.\n",
    "        for col_idx, col in enumerate(self.feature_cols):\n",
    "            dtype = self.df.schema[col]\n",
    "            if isinstance(dtype, pl.List):\n",
    "                base_type = dtype.inner\n",
    "                target_type = dtype_map.get(type(base_type), np.float64)\n",
    "            else:\n",
    "                target_type = dtype_map.get(type(dtype), np.float64)\n",
    "            event_data[col] = padded_features[:, col_idx].astype(target_type)\n",
    "\n",
    "        return pl.DataFrame(event_data)\n",
    "\n",
    "    \n",
    "    def compute_class_weights(self):\n",
    "        \"\"\"\n",
    "        Compute inverse frequency weights based on the 'marker' column.\n",
    "        Assumes markers are \"Stimulus/A\" and \"Stimulus/P\".\n",
    "        \"\"\"\n",
    "        # Get unique combinations of event_id and marker.\n",
    "        unique_events = self.df.select([\"event_id\", \"marker\"]).unique()\n",
    "        \n",
    "        # Use value_counts on the \"marker\" column.\n",
    "        counts_df = unique_events[\"marker\"].value_counts()\n",
    "\n",
    "        # We'll use 'values' if it exists, otherwise 'marker'.\n",
    "        d = { (row.get(\"values\") or row.get(\"marker\")): row[\"count\"] \n",
    "            for row in counts_df.to_dicts() }\n",
    "        \n",
    "        weight_A = 1.0 / d.get(\"Stimulus/A\", 1)\n",
    "        weight_P = 1.0 / d.get(\"Stimulus/P\", 1)\n",
    "        return {\"A\": weight_A, \"P\": weight_P}\n",
    "   \n",
    "    def split_dataset(self, ratios=(0.7, 0.15, 0.15), seed=None):\n",
    "        \"\"\"\n",
    "        Splits the dataset into three EEGDataset instances for train, val, and test.\n",
    "        This method shuffles the event_ids and then partitions them based on the given ratios.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Copy and shuffle the event_ids\n",
    "        event_ids = self.event_ids.copy()\n",
    "        np.random.shuffle(event_ids)\n",
    "        total = len(event_ids)\n",
    "        \n",
    "        n_train = int(ratios[0] * total)\n",
    "        n_val   = int(ratios[1] * total)\n",
    "        \n",
    "        train_ids = event_ids[:n_train]\n",
    "        val_ids   = event_ids[n_train:n_train+n_val]\n",
    "        test_ids  = event_ids[n_train+n_val:]\n",
    "        \n",
    "        # Filter self.df for the selected event_ids\n",
    "        train_df = self.df.filter(pl.col(\"event_id\").is_in(train_ids))\n",
    "        val_df   = self.df.filter(pl.col(\"event_id\").is_in(val_ids))\n",
    "        test_df  = self.df.filter(pl.col(\"event_id\").is_in(test_ids))\n",
    "        \n",
    "        # Create new EEGDataset instances using the filtered data\n",
    "        train_set = EEGDataset(train_df, self.max_length)\n",
    "        val_set   = EEGDataset(val_df, self.max_length)\n",
    "        test_set  = EEGDataset(test_df, self.max_length)\n",
    "        \n",
    "        return train_set, val_set, test_set\n",
    "    \n",
    "    def _load_and_filter(self, source):\n",
    "        df = pl.read_parquet(source) if isinstance(source, str) else source\n",
    "        df = df.filter(pl.col('marker').is_in([\"Stimulus/A\", \"Stimulus/P\"]))\n",
    "        if '__null_dask_index__' in df.columns:\n",
    "            df = df.drop('__null_dask_index__')\n",
    "        return df\n",
    "\n",
    "    def _pad_sequence(self, tensor):\n",
    "        # Pre-allocate tensor for maximum efficiency\n",
    "        padded = torch.zeros((self.max_length, tensor.size(1)), dtype=tensor.dtype)\n",
    "        length = min(tensor.size(0), self.max_length)\n",
    "        padded[:length] = tensor[:length]\n",
    "        return padded\n",
    "    \n",
    "    def rebalance_by_tsmote(self):\n",
    "        \"\"\"TSMOTE implementation for temporal EEG data\"\"\"\n",
    "        # Extract time-ordered features as 3D array (samples, timesteps, features)\n",
    "        X = np.stack([features.numpy() for _, features in self.samples])\n",
    "        y = np.array([label.item() for label, _ in self.samples])\n",
    "        \n",
    "        # Apply TSMOTE with temporal awareness\n",
    "        tsmote = TSMOTE()\n",
    "        X_res, y_res = tsmote.fit_resample(X, y)\n",
    "\n",
    "        # Generate synthetic temporal events\n",
    "        new_events = []\n",
    "        new_event_id = self.df['event_id'].max() + 1\n",
    "        time_base = np.arange(self.max_length)\n",
    "        original_schema = self.df.schema\n",
    "\n",
    "        # Create dtype conversion map\n",
    "        dtype_map = {\n",
    "            pl.Float64: np.float64,\n",
    "            pl.Float32: np.float32,\n",
    "            pl.Int64: np.int64,\n",
    "            pl.Int32: np.int32,\n",
    "            pl.Utf8: str,\n",
    "        }\n",
    "\n",
    "        # Process synthetic samples (original samples come first in X_res)\n",
    "        for features_3d, label in zip(X_res[len(self.samples):], y_res[len(self.samples):]):\n",
    "            event_data = {\n",
    "                \"event_id\": [new_event_id] * self.max_length,\n",
    "                \"marker\": [\"Stimulus/P\" if label > 0.5 else \"Stimulus/A\"] * self.max_length,\n",
    "                \"time\": time_base.copy()\n",
    "            }\n",
    "            \n",
    "            # Add features with proper temporal structure\n",
    "            for col_idx, col in enumerate(self.feature_cols):\n",
    "                col_data = features_3d[:, col_idx]\n",
    "                schema_type = original_schema[col]\n",
    "                \n",
    "                # Handle data types\n",
    "                if isinstance(schema_type, pl.List):\n",
    "                    base_type = schema_type.inner\n",
    "                    target_type = dtype_map.get(type(base_type), np.float64)\n",
    "                else:\n",
    "                    target_type = dtype_map.get(type(schema_type), np.float64)\n",
    "                \n",
    "                col_data = col_data.astype(target_type)\n",
    "                \n",
    "                # Maintain integer precision\n",
    "                if schema_type in (pl.Int64, pl.Int32):\n",
    "                    col_data = np.round(col_data).astype(int)\n",
    "                \n",
    "                event_data[col] = col_data\n",
    "\n",
    "            # Create DataFrame with strict schema adherence\n",
    "            event_df = pl.DataFrame(event_data).cast(original_schema)\n",
    "            new_events.append(event_df)\n",
    "            new_event_id += 1\n",
    "\n",
    "        # Update dataset with synthetic temporal events\n",
    "        self.df = pl.concat([self.df, *new_events])\n",
    "        self.event_ids = self.df['event_id'].unique().to_list()\n",
    "        self._precompute_samples()\n",
    "        self._class_weights = self.compute_class_weights()\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for variable-length EEG feature sequences.\n",
    "\n",
    "    Each sample is expected to be a tuple (label, feature), where:\n",
    "    - label is a scalar tensor (or 1D tensor) representing the class/target.\n",
    "    - feature is a tensor of shape (seq_len, num_channels), where seq_len may vary.\n",
    "\n",
    "    This function stacks labels and pads features along the time dimension so that\n",
    "    all sequences in the batch have the same length.\n",
    "    \"\"\"\n",
    "    # Unzip the batch into labels and features\n",
    "    labels, features = zip(*batch)\n",
    "    \n",
    "    labels = torch.stack(labels)\n",
    "    # Optionally: labels = labels.unsqueeze(1)  # Uncomment if required by your loss function\n",
    "    padded_features = pad_sequence(features, batch_first=True)\n",
    "    \n",
    "    return labels, padded_features\n",
    "\n",
    "\n",
    "def train_model(config, train_set, train_loader, val_loader, writer):    \n",
    "    # Model initialization\n",
    "    model = EEGMobileNet().to(config['device'])\n",
    "    #EEGDSConv(dropout=config['dropout']).to(config['device'])\n",
    "    \n",
    "    # Log model architecture and config\n",
    "    writer.add_text(\"Model/Type\", f\"EEGDSConv with dropout={config['dropout']}\")\n",
    "    writer.add_text(\"Model/Structure\", str(model))\n",
    "    writer.add_text(\"Training Config\", str(config))\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    # pos_weight = torch.tensor([\n",
    "    #     train_set.class_weights['A'] / train_set.class_weights['P']\n",
    "    # ]).to(config['device'])\n",
    "    criterion = nn.BCEWithLogitsLoss()#pos_weight=pos_weight)\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['lr'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Learning rate schedulers\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lambda epoch: min(1.0, (epoch + 1) / config['warmup_epochs'])\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_metric = float('inf')\n",
    "    \n",
    "    for epoch in tqdm(range(config['epochs'])):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for labels, features in train_loader:\n",
    "            features = features.to(config['device'])\n",
    "            labels = labels.to(config['device'])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), \n",
    "                config['grad_clip']\n",
    "            )\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for labels, features in val_loader:\n",
    "                features = features.to(config['device'])\n",
    "                labels = labels.to(config['device'])\n",
    "                \n",
    "                outputs = model(features)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                \n",
    "                preds = torch.sigmoid(outputs)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        predictions = (np.array(all_preds) > 0.5).astype(int)\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Update schedulers\n",
    "        if epoch < config['warmup_epochs']:\n",
    "            warmup_scheduler.step()\n",
    "        else:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        # Log metrics and learning rate\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(all_labels, predictions),\n",
    "            'precision': precision_score(all_labels, predictions),\n",
    "            'recall': recall_score(all_labels, predictions),\n",
    "            'f1': f1_score(all_labels, predictions)\n",
    "        }\n",
    "        \n",
    "        writer.add_scalar('LR', current_lr, epoch)\n",
    "        writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Val', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy', metrics['accuracy'], epoch)\n",
    "        writer.add_scalar('Precision', metrics['precision'], epoch)\n",
    "        writer.add_scalar('Recall', metrics['recall'], epoch)\n",
    "        writer.add_scalar('F1', metrics['f1'], epoch)\n",
    "        writer.add_scalars('Metrics', metrics, epoch)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_metric:\n",
    "            best_metric = val_loss\n",
    "            torch.save(model.state_dict(), f\"{config['log_dir']}/best_model.pth\")\n",
    "    \n",
    "    writer.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    'data_path': '/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet',\n",
    "    'split_ratios': (0.7, 0.15, 0.15),\n",
    "    'batch_size': 128,          # Increased for better generalization\n",
    "    'dropout': 0.6,            # Reduced from 0.6 for better information flow\n",
    "    'lr': 7e-5,                # Base learning rate (sweet spot between 1e-5 and 3e-3)\n",
    "    'weight_decay': 1e-5,      # Increased regularization\n",
    "    'epochs': 2000,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'log_dir': './runs/CNN',\n",
    "    'lr_scheduler': {\n",
    "        'mode': 'min',\n",
    "        'factor': 0.1,         # More aggressive LR reduction\n",
    "        'patience': 5,         # Faster response to plateaus\n",
    "        'threshold': 0.001,\n",
    "        'cooldown': 3\n",
    "    },\n",
    "    'grad_clip': 1.0,          # Add gradient clipping\n",
    "    'warmup_epochs': 10        # Linear LR warmup\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #============================================================\n",
    "# # Training Pipeline\n",
    "# #============================================================\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# # Initialize dataset\n",
    "# print(\"Creating full dataset...\")\n",
    "# full_dataset = EEGDataset(config['data_path'])\n",
    "\n",
    "# print(\"Splitting the dataset...\")\n",
    "# # Split dataset\n",
    "# train_set, val_set, test_set = full_dataset.split_dataset(\n",
    "#     ratios=config['split_ratios']\n",
    "# )\n",
    "\n",
    "# del full_dataset\n",
    "\n",
    "# len_dataset = len(train_set)\n",
    "# sample = train_set[0]\n",
    "# label_shape = sample[0].shape\n",
    "# feature_shape = sample[1].shape\n",
    "\n",
    "# print(f\"unbalanced train dataset shape: ({len_dataset}, [labels: {label_shape}, features: {list(feature_shape)}])\")\n",
    "\n",
    "# # Balance training set\n",
    "# print(\"Applying SMOTE to train dataset...\")\n",
    "# train_set.rebalance_by_tsmote()\n",
    "\n",
    "# # print(\"Augmenting train dataset...\")\n",
    "# # train_set.augment_dataset(\n",
    "# #     n_times=3,              # Nx dataset expansion\n",
    "# #     noise_std=0.15,         # Moderate noise\n",
    "# #     scale_range=(0.7, 1.3), # ±30% amplitude variation\n",
    "# #     warp_range=(0.85, 1.15),# ±15% time warping\n",
    "# #     max_shift=15,           # 150ms temporal shifts\n",
    "# #     freq_shift=3,           # ±3Hz frequency shifts\n",
    "# #     mask_size=75,           # 750ms masking\n",
    "# #     drop_prob=0.15,         # 15% channel dropout\n",
    "# #     mixup_alpha=0.2         # Mixup\n",
    "# # )\n",
    "\n",
    "# torch.save(train_set, 'train_set.pt')\n",
    "# torch.save(val_set, 'val_set.pt')\n",
    "# torch.save(test_set, 'test_set.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6487/2544576015.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_set = torch.load('train_set.pt')\n",
      "/tmp/ipykernel_6487/2544576015.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_set = torch.load('val_set.pt')\n",
      "/tmp/ipykernel_6487/2544576015.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_set = torch.load('test_set.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset shape: (2549, [labels: torch.Size([]), features: [2000, 63]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owner/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5b72d71fe44195b52a9f513cae28dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(log_dir\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_dir\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 80\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(config, train_set, train_loader, val_loader, writer)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m     75\u001b[0m         model\u001b[38;5;241m.\u001b[39mparameters(), \n\u001b[1;32m     76\u001b[0m         config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad_clip\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     77\u001b[0m     )\n\u001b[1;32m     79\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 80\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Validation phase\u001b[39;00m\n\u001b[1;32m     83\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_set = torch.load('train_set.pt')\n",
    "val_set = torch.load('val_set.pt')\n",
    "test_set = torch.load('test_set.pt')\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=12,\n",
    "    pin_memory=True,      # For GPU acceleration\n",
    "    persistent_workers=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(val_set, batch_size=config['batch_size'], collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=config['batch_size'], collate_fn=collate_fn)\n",
    "\n",
    "len_dataset = len(train_set)\n",
    "sample = train_set[0]\n",
    "label_shape = sample[0].shape\n",
    "feature_shape = sample[1].shape\n",
    "\n",
    "print(f\"train dataset shape: ({len_dataset}, [labels: {label_shape}, features: {list(feature_shape)}])\")\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "writer = SummaryWriter(log_dir=config['log_dir'])\n",
    "\n",
    "# Start training\n",
    "trained_model = train_model(config, train_set, train_loader, val_loader, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# pos_weight = torch.tensor([\n",
    "#     train_set.class_weights['A'] / train_set.class_weights['P']\n",
    "# ]).to(config['device'])\n",
    "criterion = nn.BCEWithLogitsLoss()#pos_weight=pos_weight)\n",
    "\n",
    "epoch = 1\n",
    "# Assuming model, criterion, test_loader, device, writer, and epoch are already defined\n",
    "best_model = torch.load('best_model.torch')\n",
    "best_model.eval()\n",
    "test_loss = 0\n",
    "all_test_markers = []\n",
    "all_test_predictions = []\n",
    "with torch.no_grad():\n",
    "    for markers, features in tqdm(test_loader):\n",
    "        features = features.to(config['device'])\n",
    "        markers = markers.unsqueeze(-1).to(config['device'])\n",
    "\n",
    "        outputs = best_model(features)\n",
    "        loss = criterion(outputs, markers)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Collect markers and predictions for metrics calculation\n",
    "        all_test_markers.extend(markers.cpu().numpy().flatten())\n",
    "        all_test_predictions.extend(torch.sigmoid(outputs).cpu().numpy().flatten())\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "# Calculate test metrics\n",
    "test_accuracy = accuracy_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_precision = precision_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_recall = recall_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_f1 = f1_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_roc_auc = roc_auc_score(all_test_markers, all_test_predictions)\n",
    "\n",
    "# Log test metrics to TensorBoard\n",
    "writer.add_scalar('Metrics/test_accuracy', test_accuracy, epoch)\n",
    "writer.add_scalar('Metrics/test_precision', test_precision, epoch)\n",
    "writer.add_scalar('Metrics/test_recall', test_recall, epoch)\n",
    "writer.add_scalar('Metrics/test_f1', test_f1, epoch)\n",
    "writer.add_scalar('Metrics/test_roc_auc', test_roc_auc, epoch)\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "{test_accuracy=}\n",
    "{test_precision=}\n",
    "{test_recall=}\n",
    "{test_f1=}\n",
    "{test_roc_auc=}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "best_threshold = 0.0\n",
    "best_f1 = 0.0\n",
    "thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "\n",
    "for threshold in tqdm(thresholds):\n",
    "    binary_predictions = (all_test_predictions > threshold).astype(int)\n",
    "    current_recall = f1_score(all_test_markers, binary_predictions)\n",
    "\n",
    "    if current_recall > best_f1:\n",
    "        best_f1 = current_recall\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"{best_threshold=}\")\n",
    "print(f\"{best_f1=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "import numpy as np\n",
    "best_threshold = 0.1\n",
    "best_recall = 0.0\n",
    "thresholds = np.arange(0.1, 1.0, 0.01)\n",
    "\n",
    "for threshold in tqdm(thresholds):\n",
    "    binary_predictions = (all_test_predictions > threshold).astype(int)\n",
    "    current_recall = recall_score(all_test_markers, binary_predictions)\n",
    "\n",
    "    if current_recall > best_recall:\n",
    "        best_recall = current_recall\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"{best_threshold=}\")\n",
    "print(f\"{best_recall=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
