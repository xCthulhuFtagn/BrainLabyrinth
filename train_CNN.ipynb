{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for Python's built-in random module\n",
    "random.seed(69)\n",
    "\n",
    "# Set the seed for NumPy's random number generator\n",
    "np.random.seed(69)\n",
    "\n",
    "# Set the seed for PyTorch's random number generators\n",
    "torch.manual_seed(69)\n",
    "torch.cuda.manual_seed(69)\n",
    "torch.cuda.manual_seed_all(69)  # if you are using multi-GPU.\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5_828_008, 68)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>event_id</th><th>marker</th><th>time</th><th>Fp1</th><th>Fpz</th><th>Fp2</th><th>F7</th><th>F3</th><th>Fz</th><th>F4</th><th>F8</th><th>FC5</th><th>FC1</th><th>FC2</th><th>FC6</th><th>M1</th><th>T7</th><th>C3</th><th>Cz</th><th>C4</th><th>T8</th><th>M2</th><th>CP5</th><th>CP1</th><th>CP2</th><th>CP6</th><th>P7</th><th>P3</th><th>Pz</th><th>P4</th><th>P8</th><th>POz</th><th>O1</th><th>O2</th><th>EOG</th><th>AF7</th><th>AF3</th><th>AF4</th><th>AF8</th><th>F5</th><th>F1</th><th>F2</th><th>F6</th><th>FC3</th><th>FCz</th><th>FC4</th><th>C5</th><th>C1</th><th>C2</th><th>C6</th><th>CP3</th><th>CP4</th><th>P5</th><th>P1</th><th>P2</th><th>P6</th><th>PO5</th><th>PO3</th><th>PO4</th><th>PO6</th><th>FT7</th><th>FT8</th><th>TP7</th><th>TP8</th><th>PO7</th><th>PO8</th><th>Oz</th><th>__null_dask_index__</th></tr><tr><td>i64</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td></tr></thead><tbody><tr><td>0</td><td>&quot;Stimulus/1&quot;</td><td>9.438</td><td>9.53724</td><td>7.803913</td><td>-6.957125</td><td>-3.662101</td><td>-1.831665</td><td>6.678428</td><td>0.643271</td><td>-28.855688</td><td>17.219198</td><td>10.443791</td><td>8.557194</td><td>5.307324</td><td>-41.916786</td><td>-20.510318</td><td>20.105794</td><td>7.447749</td><td>18.204764</td><td>-21.123294</td><td>-54.043086</td><td>-0.779434</td><td>9.156651</td><td>3.35639</td><td>7.050375</td><td>8.437749</td><td>10.035689</td><td>18.781002</td><td>12.663772</td><td>-1.516957</td><td>5.923493</td><td>9.755382</td><td>5.507086</td><td>56.784839</td><td>-16.524688</td><td>0.995008</td><td>1.210651</td><td>-16.334868</td><td>5.347515</td><td>7.849705</td><td>4.708359</td><td>-21.316867</td><td>2.927581</td><td>9.449216</td><td>5.787111</td><td>3.851742</td><td>5.561468</td><td>1.623157</td><td>0.986564</td><td>13.195735</td><td>5.790764</td><td>9.944564</td><td>14.604585</td><td>21.713373</td><td>4.75506</td><td>10.76817</td><td>11.458283</td><td>-8.927917</td><td>3.754084</td><td>0.357387</td><td>-4.394041</td><td>-8.618822</td><td>-5.172546</td><td>11.513064</td><td>4.785936</td><td>5.749947</td><td>0</td></tr><tr><td>0</td><td>&quot;Stimulus/1&quot;</td><td>9.44</td><td>13.396619</td><td>13.868635</td><td>-2.922562</td><td>3.933463</td><td>1.559054</td><td>9.798244</td><td>3.592579</td><td>-24.953035</td><td>15.061343</td><td>8.709912</td><td>9.965191</td><td>5.720247</td><td>-70.75158</td><td>-41.867714</td><td>15.554878</td><td>6.088562</td><td>21.294248</td><td>-0.120773</td><td>-50.053209</td><td>-8.458973</td><td>6.922121</td><td>3.178088</td><td>16.021653</td><td>-9.844435</td><td>-2.435621</td><td>18.337207</td><td>18.720653</td><td>21.930048</td><td>5.166919</td><td>-11.485745</td><td>27.682245</td><td>62.681269</td><td>-15.060192</td><td>0.757932</td><td>5.598742</td><td>-17.990556</td><td>2.588855</td><td>7.099604</td><td>5.117679</td><td>-16.808443</td><td>5.047086</td><td>12.311436</td><td>11.37285</td><td>3.025393</td><td>8.060817</td><td>5.687988</td><td>5.582753</td><td>10.034344</td><td>11.733741</td><td>-3.211322</td><td>11.257885</td><td>25.065433</td><td>21.465142</td><td>-6.313013</td><td>-5.096768</td><td>-1.385707</td><td>27.073425</td><td>-1.288571</td><td>-4.394788</td><td>-19.987022</td><td>14.889966</td><td>-10.593543</td><td>27.311613</td><td>-6.368204</td><td>1</td></tr><tr><td>0</td><td>&quot;Stimulus/1&quot;</td><td>9.442</td><td>14.850883</td><td>14.096507</td><td>-0.078051</td><td>-0.246867</td><td>1.38473</td><td>11.079935</td><td>4.932076</td><td>-24.924988</td><td>14.634668</td><td>9.740824</td><td>11.113594</td><td>5.932303</td><td>-66.520317</td><td>-25.757618</td><td>14.687831</td><td>6.78713</td><td>22.50476</td><td>-9.39821</td><td>-44.082786</td><td>-6.577221</td><td>8.424037</td><td>4.776816</td><td>11.782222</td><td>-2.206932</td><td>-3.447845</td><td>17.317284</td><td>15.465552</td><td>7.507261</td><td>4.749701</td><td>-4.053521</td><td>20.096318</td><td>56.423504</td><td>-16.052469</td><td>0.542124</td><td>3.936092</td><td>-14.449495</td><td>2.010495</td><td>9.100914</td><td>5.373109</td><td>-16.629689</td><td>4.217228</td><td>12.391046</td><td>10.807673</td><td>2.673265</td><td>6.832248</td><td>5.108662</td><td>5.212829</td><td>9.382513</td><td>9.761762</td><td>-3.64864</td><td>9.981714</td><td>22.28212</td><td>13.84102</td><td>-1.795933</td><td>-1.438725</td><td>-4.064985</td><td>22.191193</td><td>-9.739858</td><td>-5.565701</td><td>-9.834637</td><td>8.252451</td><td>-1.853507</td><td>20.793088</td><td>3.175448</td><td>2</td></tr><tr><td>0</td><td>&quot;Stimulus/1&quot;</td><td>9.444</td><td>21.293703</td><td>18.676068</td><td>2.396635</td><td>5.01313</td><td>5.984928</td><td>13.828105</td><td>6.36971</td><td>-23.503992</td><td>20.354059</td><td>13.549383</td><td>12.007385</td><td>2.86629</td><td>-32.600562</td><td>6.407807</td><td>19.207374</td><td>8.063451</td><td>18.753907</td><td>-36.384114</td><td>-46.5438</td><td>0.755845</td><td>9.151167</td><td>1.696862</td><td>-2.183605</td><td>9.926201</td><td>2.112486</td><td>14.258738</td><td>2.697055</td><td>-30.135514</td><td>-6.710183</td><td>0.557059</td><td>-28.83586</td><td>66.307082</td><td>-10.671446</td><td>5.90164</td><td>6.894815</td><td>-11.230897</td><td>8.280238</td><td>14.231695</td><td>8.181461</td><td>-18.193031</td><td>10.290102</td><td>16.717396</td><td>9.37912</td><td>4.499651</td><td>9.509004</td><td>5.286035</td><td>-0.039267</td><td>11.469463</td><td>0.325132</td><td>2.030729</td><td>9.968108</td><td>16.758211</td><td>-15.392703</td><td>2.525013</td><td>3.591888</td><td>-23.756455</td><td>-26.819381</td><td>-14.263371</td><td>-7.832686</td><td>1.443246</td><td>-6.503959</td><td>3.155976</td><td>-28.110124</td><td>-16.510437</td><td>3</td></tr><tr><td>0</td><td>&quot;Stimulus/1&quot;</td><td>9.446</td><td>20.054007</td><td>18.219546</td><td>3.120409</td><td>0.285677</td><td>6.550377</td><td>17.138199</td><td>15.234075</td><td>-13.355245</td><td>18.774967</td><td>14.254335</td><td>17.074288</td><td>12.947393</td><td>-34.977934</td><td>5.612512</td><td>15.83888</td><td>9.753912</td><td>22.196893</td><td>-37.742914</td><td>-36.697359</td><td>-3.596704</td><td>5.289391</td><td>1.301041</td><td>4.294134</td><td>1.056083</td><td>-4.711626</td><td>13.018475</td><td>5.3802</td><td>-11.755816</td><td>-9.147353</td><td>-10.122114</td><td>-23.32428</td><td>64.653867</td><td>-5.254813</td><td>8.025885</td><td>10.051126</td><td>-3.915474</td><td>10.896383</td><td>16.240127</td><td>13.117371</td><td>-7.493371</td><td>10.223294</td><td>18.24204</td><td>15.672042</td><td>-0.109563</td><td>9.109484</td><td>6.398258</td><td>10.450412</td><td>8.586929</td><td>4.823086</td><td>-5.016421</td><td>7.561401</td><td>18.00294</td><td>-12.701589</td><td>-6.145933</td><td>-6.488521</td><td>-21.213362</td><td>-21.120105</td><td>-17.366758</td><td>9.093167</td><td>-11.152638</td><td>1.806184</td><td>-7.890925</td><td>-21.705784</td><td>-25.354465</td><td>4</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1381</td><td>&quot;Stimulus/A&quot;</td><td>928.476</td><td>9.591813</td><td>-41.557977</td><td>-60.77412</td><td>23.760604</td><td>-41.33637</td><td>-66.987031</td><td>-11.20374</td><td>-90.704749</td><td>3.790776</td><td>-30.919723</td><td>-37.216313</td><td>-61.566037</td><td>-52.382623</td><td>-9.286577</td><td>-42.072383</td><td>-38.344749</td><td>-53.684958</td><td>-44.898705</td><td>-49.312139</td><td>-0.965291</td><td>-33.180112</td><td>-17.891398</td><td>-10.890542</td><td>-19.449326</td><td>-9.946805</td><td>-33.991378</td><td>-89.135265</td><td>-85.339557</td><td>-19.949215</td><td>-13.598605</td><td>-35.072145</td><td>379.205543</td><td>10.027611</td><td>-13.869941</td><td>-38.946625</td><td>-80.264638</td><td>-1.0685</td><td>-43.047086</td><td>-10.514064</td><td>-57.963668</td><td>31.12568</td><td>-46.874566</td><td>-32.352284</td><td>18.198025</td><td>-51.127463</td><td>-36.473882</td><td>-43.249089</td><td>-26.761444</td><td>-42.808292</td><td>-27.528111</td><td>-29.581585</td><td>-52.231929</td><td>-52.319126</td><td>-18.26109</td><td>-19.034752</td><td>-54.394882</td><td>-40.090876</td><td>1.734976</td><td>-61.119441</td><td>6.031929</td><td>-18.942257</td><td>-19.640234</td><td>-39.629149</td><td>-28.380572</td><td>275996</td></tr><tr><td>1381</td><td>&quot;Stimulus/A&quot;</td><td>928.478</td><td>10.05211</td><td>-39.285851</td><td>-57.304729</td><td>24.489219</td><td>-41.405448</td><td>-65.223565</td><td>-7.581582</td><td>-85.183478</td><td>7.284242</td><td>-32.467265</td><td>-36.44456</td><td>-58.760747</td><td>-51.05454</td><td>-0.406084</td><td>-41.466138</td><td>-37.820387</td><td>-53.660458</td><td>-51.282983</td><td>-47.950155</td><td>-3.073491</td><td>-34.703956</td><td>-18.293676</td><td>-11.464762</td><td>-24.24428</td><td>-11.731681</td><td>-33.641051</td><td>-89.751296</td><td>-86.665381</td><td>-21.44362</td><td>-17.000163</td><td>-35.684019</td><td>385.31099</td><td>13.91835</td><td>-10.738223</td><td>-30.392338</td><td>-74.628701</td><td>2.910838</td><td>-41.721126</td><td>-7.360083</td><td>-49.079759</td><td>30.052943</td><td>-45.777716</td><td>-31.916799</td><td>19.00681</td><td>-50.908402</td><td>-34.346295</td><td>-36.57473</td><td>-26.347318</td><td>-41.537574</td><td>-29.77699</td><td>-30.339979</td><td>-52.22948</td><td>-53.980062</td><td>-21.867465</td><td>-21.754061</td><td>-57.051582</td><td>-40.913533</td><td>3.709452</td><td>-60.064794</td><td>4.821594</td><td>-19.524317</td><td>-20.815015</td><td>-40.568466</td><td>-29.987083</td><td>275997</td></tr><tr><td>1381</td><td>&quot;Stimulus/A&quot;</td><td>928.48</td><td>12.065095</td><td>-40.534576</td><td>-57.783633</td><td>29.010773</td><td>-43.466559</td><td>-65.123134</td><td>-6.594105</td><td>-84.364808</td><td>11.379481</td><td>-32.393301</td><td>-35.587802</td><td>-55.748424</td><td>-54.481948</td><td>9.565403</td><td>-42.304131</td><td>-38.425967</td><td>-52.91369</td><td>-47.393221</td><td>-47.164644</td><td>-2.847837</td><td>-34.77862</td><td>-17.350552</td><td>-9.44111</td><td>-21.020738</td><td>-8.33714</td><td>-32.512395</td><td>-87.232313</td><td>-83.327347</td><td>-20.088512</td><td>-11.905468</td><td>-34.854007</td><td>402.957563</td><td>16.298075</td><td>-10.369621</td><td>-30.212791</td><td>-73.874753</td><td>5.971585</td><td>-39.426622</td><td>-7.338018</td><td>-49.622007</td><td>30.432534</td><td>-45.860797</td><td>-31.460015</td><td>18.195945</td><td>-51.894871</td><td>-34.708639</td><td>-30.659673</td><td>-27.710193</td><td>-41.63766</td><td>-27.134769</td><td>-30.320361</td><td>-51.321614</td><td>-52.401906</td><td>-19.546804</td><td>-20.653188</td><td>-55.703298</td><td>-39.288035</td><td>12.232324</td><td>-56.132915</td><td>10.525082</td><td>-15.37765</td><td>-18.393694</td><td>-40.166692</td><td>-28.50722</td><td>275998</td></tr><tr><td>1381</td><td>&quot;Stimulus/A&quot;</td><td>928.482</td><td>14.72505</td><td>-38.596546</td><td>-53.248571</td><td>31.472895</td><td>-39.738697</td><td>-64.958224</td><td>-6.417419</td><td>-83.385095</td><td>11.147532</td><td>-30.443848</td><td>-35.688861</td><td>-55.982359</td><td>-46.464062</td><td>18.694285</td><td>-45.26478</td><td>-38.533711</td><td>-50.081167</td><td>-42.353148</td><td>-42.931054</td><td>-3.614683</td><td>-32.516321</td><td>-16.236222</td><td>-6.832051</td><td>-17.149998</td><td>-11.510808</td><td>-35.3496</td><td>-88.705435</td><td>-85.313381</td><td>-21.334178</td><td>-14.573576</td><td>-37.156088</td><td>394.311109</td><td>17.552649</td><td>-8.430964</td><td>-33.06964</td><td>-70.96307</td><td>3.752907</td><td>-39.039754</td><td>-5.491022</td><td>-53.773884</td><td>29.962905</td><td>-46.757481</td><td>-30.543767</td><td>14.668657</td><td>-51.72058</td><td>-34.076067</td><td>-32.300195</td><td>-29.405876</td><td>-39.132014</td><td>-25.195138</td><td>-29.335414</td><td>-51.595348</td><td>-49.798575</td><td>-17.942612</td><td>-18.328749</td><td>-55.256379</td><td>-41.800521</td><td>12.451331</td><td>-55.866528</td><td>9.126756</td><td>-17.300153</td><td>-20.074211</td><td>-42.179936</td><td>-31.332407</td><td>275999</td></tr><tr><td>1381</td><td>&quot;Stimulus/A&quot;</td><td>928.484</td><td>15.808375</td><td>-36.266224</td><td>-53.110752</td><td>32.924337</td><td>-37.36348</td><td>-64.090116</td><td>-5.231033</td><td>-82.415774</td><td>13.220155</td><td>-30.305264</td><td>-33.865274</td><td>-54.286609</td><td>-46.264271</td><td>17.130059</td><td>-43.916018</td><td>-36.523518</td><td>-50.787514</td><td>-40.772671</td><td>-41.895675</td><td>-2.720411</td><td>-34.069455</td><td>-14.845942</td><td>-6.451268</td><td>-17.932889</td><td>-10.319097</td><td>-35.023904</td><td>-88.797509</td><td>-85.17417</td><td>-21.106819</td><td>-13.415075</td><td>-34.868671</td><td>405.91546</td><td>19.267502</td><td>-6.658455</td><td>-34.919759</td><td>-70.498651</td><td>3.814254</td><td>-37.306107</td><td>-4.04167</td><td>-51.11805</td><td>30.894601</td><td>-45.19893</td><td>-30.159779</td><td>13.853158</td><td>-51.240877</td><td>-34.016793</td><td>-30.1696</td><td>-29.843134</td><td>-39.165263</td><td>-27.946339</td><td>-29.823856</td><td>-51.218703</td><td>-50.641914</td><td>-18.087194</td><td>-19.294669</td><td>-53.564813</td><td>-40.085109</td><td>12.634602</td><td>-55.407809</td><td>9.125513</td><td>-17.057724</td><td>-19.120281</td><td>-39.526945</td><td>-30.902496</td><td>276000</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5_828_008, 68)\n",
       "┌──────────┬────────────┬─────────┬───────────┬───┬────────────┬───────────┬───────────┬───────────┐\n",
       "│ event_id ┆ marker     ┆ time    ┆ Fp1       ┆ … ┆ PO7        ┆ PO8       ┆ Oz        ┆ __null_da │\n",
       "│ ---      ┆ ---        ┆ ---     ┆ ---       ┆   ┆ ---        ┆ ---       ┆ ---       ┆ sk_index_ │\n",
       "│ i64      ┆ str        ┆ f64     ┆ f64       ┆   ┆ f64        ┆ f64       ┆ f64       ┆ _         │\n",
       "│          ┆            ┆         ┆           ┆   ┆            ┆           ┆           ┆ ---       │\n",
       "│          ┆            ┆         ┆           ┆   ┆            ┆           ┆           ┆ i64       │\n",
       "╞══════════╪════════════╪═════════╪═══════════╪═══╪════════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 0        ┆ Stimulus/1 ┆ 9.438   ┆ 9.53724   ┆ … ┆ 11.513064  ┆ 4.785936  ┆ 5.749947  ┆ 0         │\n",
       "│ 0        ┆ Stimulus/1 ┆ 9.44    ┆ 13.396619 ┆ … ┆ -10.593543 ┆ 27.311613 ┆ -6.368204 ┆ 1         │\n",
       "│ 0        ┆ Stimulus/1 ┆ 9.442   ┆ 14.850883 ┆ … ┆ -1.853507  ┆ 20.793088 ┆ 3.175448  ┆ 2         │\n",
       "│ 0        ┆ Stimulus/1 ┆ 9.444   ┆ 21.293703 ┆ … ┆ 3.155976   ┆ -28.11012 ┆ -16.51043 ┆ 3         │\n",
       "│          ┆            ┆         ┆           ┆   ┆            ┆ 4         ┆ 7         ┆           │\n",
       "│ 0        ┆ Stimulus/1 ┆ 9.446   ┆ 20.054007 ┆ … ┆ -7.890925  ┆ -21.70578 ┆ -25.35446 ┆ 4         │\n",
       "│          ┆            ┆         ┆           ┆   ┆            ┆ 4         ┆ 5         ┆           │\n",
       "│ …        ┆ …          ┆ …       ┆ …         ┆ … ┆ …          ┆ …         ┆ …         ┆ …         │\n",
       "│ 1381     ┆ Stimulus/A ┆ 928.476 ┆ 9.591813  ┆ … ┆ -19.640234 ┆ -39.62914 ┆ -28.38057 ┆ 275996    │\n",
       "│          ┆            ┆         ┆           ┆   ┆            ┆ 9         ┆ 2         ┆           │\n",
       "│ 1381     ┆ Stimulus/A ┆ 928.478 ┆ 10.05211  ┆ … ┆ -20.815015 ┆ -40.56846 ┆ -29.98708 ┆ 275997    │\n",
       "│          ┆            ┆         ┆           ┆   ┆            ┆ 6         ┆ 3         ┆           │\n",
       "│ 1381     ┆ Stimulus/A ┆ 928.48  ┆ 12.065095 ┆ … ┆ -18.393694 ┆ -40.16669 ┆ -28.50722 ┆ 275998    │\n",
       "│          ┆            ┆         ┆           ┆   ┆            ┆ 2         ┆           ┆           │\n",
       "│ 1381     ┆ Stimulus/A ┆ 928.482 ┆ 14.72505  ┆ … ┆ -20.074211 ┆ -42.17993 ┆ -31.33240 ┆ 275999    │\n",
       "│          ┆            ┆         ┆           ┆   ┆            ┆ 6         ┆ 7         ┆           │\n",
       "│ 1381     ┆ Stimulus/A ┆ 928.484 ┆ 15.808375 ┆ … ┆ -19.120281 ┆ -39.52694 ┆ -30.90249 ┆ 276000    │\n",
       "│          ┆            ┆         ┆           ┆   ┆            ┆ 5         ┆ 6         ┆           │\n",
       "└──────────┴────────────┴─────────┴───────────┴───┴────────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.read_parquet('/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unbalanced train dataset shape: (1926, [labels: torch.Size([]), features: [2000, 64]])\n",
      "train dataset shape: (2606, [labels: torch.Size([]), features: [2000, 64]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owner/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158c227a0aca4eeb9d594f9cb0c5d066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "#============================================================\n",
    "# Model Architecture (Unchanged)\n",
    "#============================================================\n",
    "class EEGDSConv(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(64, 64, 15, padding='same', groups=64),\n",
    "            nn.Conv1d(64, 16, 1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(4),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(16, 16, 7, padding='same', groups=16),\n",
    "            nn.Conv1d(16, 8, 1),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return self.block(x).squeeze(-1)  # Squeeze last dimension to match target shape\n",
    "\n",
    "\n",
    "#============================================================\n",
    "# Enhanced Dataset Class with Proper Encapsulation\n",
    "#============================================================\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, source, max_length=2000):\n",
    "        self.df = self._load_and_filter(source)\n",
    "        self.event_ids = self.df['event_id'].unique().to_list()\n",
    "        self.max_length = max_length\n",
    "        # Compute class weights after filtering, based on the marker distribution.\n",
    "        self._class_weights = self.compute_class_weights()\n",
    "        # Use all columns except event_id and marker as features.\n",
    "        self.feature_cols = [c for c in self.df.columns if c not in {'event_id', 'marker'}]\n",
    "        self._precompute_samples()\n",
    "    \n",
    "    @property\n",
    "    def class_weights(self):\n",
    "        # Expose the computed weights as a property.\n",
    "        return self._class_weights \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.event_ids)\n",
    "    \n",
    "    def _precompute_samples(self):\n",
    "        \"\"\"Cache all samples in memory during initialization\"\"\"\n",
    "        self.samples = []\n",
    "        for event_id in self.event_ids:\n",
    "            event_data = self.df.filter(pl.col(\"event_id\") == event_id)\n",
    "            features = torch.tensor(event_data.select(self.feature_cols).to_numpy(), \n",
    "                                  dtype=torch.float32)\n",
    "            features = self._pad_sequence(features)\n",
    "            label = 1.0 if event_data['marker'][0] == \"Stimulus/P\" else 0.0\n",
    "            self.samples.append( (torch.tensor(label, dtype=torch.float32), features) )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]  # Direct access now!\n",
    "    \n",
    "    def compute_class_weights(self):\n",
    "        \"\"\"\n",
    "        Compute inverse frequency weights based on the 'marker' column.\n",
    "        Assumes markers are \"Stimulus/A\" and \"Stimulus/P\".\n",
    "        \"\"\"\n",
    "        # Get unique combinations of event_id and marker.\n",
    "        unique_events = self.df.select([\"event_id\", \"marker\"]).unique()\n",
    "        \n",
    "        # Use value_counts on the \"marker\" column.\n",
    "        counts_df = unique_events[\"marker\"].value_counts()\n",
    "\n",
    "        # We'll use 'values' if it exists, otherwise 'marker'.\n",
    "        d = { (row.get(\"values\") or row.get(\"marker\")): row[\"count\"] \n",
    "            for row in counts_df.to_dicts() }\n",
    "        \n",
    "        weight_A = 1.0 / d.get(\"Stimulus/A\", 1)\n",
    "        weight_P = 1.0 / d.get(\"Stimulus/P\", 1)\n",
    "        return {\"A\": weight_A, \"P\": weight_P}\n",
    "   \n",
    "    def split_dataset(self, ratios=(0.7, 0.15, 0.15), seed=None):\n",
    "        \"\"\"\n",
    "        Splits the dataset into three EEGDataset instances for train, val, and test.\n",
    "        This method shuffles the event_ids and then partitions them based on the given ratios.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Copy and shuffle the event_ids\n",
    "        event_ids = self.event_ids.copy()\n",
    "        np.random.shuffle(event_ids)\n",
    "        total = len(event_ids)\n",
    "        \n",
    "        n_train = int(ratios[0] * total)\n",
    "        n_val   = int(ratios[1] * total)\n",
    "        \n",
    "        train_ids = event_ids[:n_train]\n",
    "        val_ids   = event_ids[n_train:n_train+n_val]\n",
    "        test_ids  = event_ids[n_train+n_val:]\n",
    "        \n",
    "        # Filter self.df for the selected event_ids\n",
    "        train_df = self.df.filter(pl.col(\"event_id\").is_in(train_ids))\n",
    "        val_df   = self.df.filter(pl.col(\"event_id\").is_in(val_ids))\n",
    "        test_df  = self.df.filter(pl.col(\"event_id\").is_in(test_ids))\n",
    "        \n",
    "        # Create new EEGDataset instances using the filtered data\n",
    "        train_set = EEGDataset(train_df, self.max_length)\n",
    "        val_set   = EEGDataset(val_df, self.max_length)\n",
    "        test_set  = EEGDataset(test_df, self.max_length)\n",
    "        \n",
    "        return train_set, val_set, test_set\n",
    "    \n",
    "    def _load_and_filter(self, source):\n",
    "        if isinstance(source, str):\n",
    "            df = pl.read_parquet(source)\n",
    "        elif isinstance(source, pl.DataFrame):\n",
    "            df = source\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported source type\")\n",
    "        \n",
    "        # Keep only events with these markers and drop unneeded columns.\n",
    "        df = df.filter(pl.col('marker').is_in([\"Stimulus/A\", \"Stimulus/P\"]))\n",
    "        for col in ['time', '__null_dask_index__']:\n",
    "            if col in df.columns:\n",
    "                df = df.drop(col)\n",
    "        return df\n",
    "\n",
    "    def _pad_sequence(self, tensor):\n",
    "        # Pre-allocate tensor for maximum efficiency\n",
    "        padded = torch.zeros((self.max_length, tensor.size(1)), dtype=tensor.dtype)\n",
    "        length = min(tensor.size(0), self.max_length)\n",
    "        padded[:length] = tensor[:length]\n",
    "        return padded\n",
    "    \n",
    "    def apply_smote(self):\n",
    "        # Extract from precomputed PADDED samples (2000×64 guaranteed)\n",
    "        X = np.stack([features.numpy().flatten() for _, features in self.samples])\n",
    "        y = np.array([label.item() for label, _ in self.samples])\n",
    "        \n",
    "        # Apply SMOTE\n",
    "        smote = SMOTE()\n",
    "        X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "        # Create synthetic events\n",
    "        new_events = []\n",
    "        new_event_id = self.df['event_id'].max() + 1\n",
    "        feature_columns = self.feature_cols\n",
    "        \n",
    "        # Create dtype conversion map (Polars → NumPy)\n",
    "        dtype_map = {\n",
    "            pl.Float64: np.float64,\n",
    "            pl.Float32: np.float32,\n",
    "            pl.Int64: np.int64,\n",
    "            pl.Int32: np.int32\n",
    "        }\n",
    "\n",
    "        # Process only new synthetic samples\n",
    "        for features_flat, label in zip(X_res[len(self):], y_res[len(self):]):\n",
    "            # Reshape to (2000, 64)\n",
    "            features_2d = features_flat.reshape(self.max_length, len(feature_columns))\n",
    "            \n",
    "            # Create event DataFrame\n",
    "            event_data = {\n",
    "                \"event_id\": [new_event_id] * self.max_length,\n",
    "                \"marker\": [\"Stimulus/P\" if label else \"Stimulus/A\"] * self.max_length\n",
    "            }\n",
    "            \n",
    "            # Add features with original dtypes\n",
    "            for col_idx, col in enumerate(feature_columns):\n",
    "                pl_dtype = self.df.schema[col]\n",
    "                # Handle list types if necessary\n",
    "                if isinstance(pl_dtype, pl.List):\n",
    "                    np_dtype = dtype_map.get(pl_dtype.inner, np.float64)\n",
    "                else:\n",
    "                    np_dtype = dtype_map.get(pl_dtype, np.float64)\n",
    "                \n",
    "                event_data[col] = features_2d[:, col_idx].astype(np_dtype)\n",
    "\n",
    "            new_events.append(pl.DataFrame(event_data))\n",
    "            new_event_id += 1\n",
    "\n",
    "        # Update dataset\n",
    "        self.df = pl.concat([self.df, *new_events])\n",
    "        self.event_ids = self.df['event_id'].unique().to_list()\n",
    "        self._precompute_samples()\n",
    "        self._class_weights = self.compute_class_weights()\n",
    "        \n",
    "        return self\n",
    "\n",
    "#============================================================\n",
    "# Training Pipeline\n",
    "#============================================================\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'data_path': '/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet',\n",
    "    'split_ratios': (0.7, 0.15, 0.15),\n",
    "    'batch_size': 32,\n",
    "    'dropout': 0.6,\n",
    "    'lr': 1e-5,\n",
    "    'weight_decay': 5e-5,\n",
    "    'epochs': 200,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'log_dir': './runs/CNN'\n",
    "}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for variable-length EEG feature sequences.\n",
    "\n",
    "    Each sample is expected to be a tuple (label, feature), where:\n",
    "    - label is a scalar tensor (or 1D tensor) representing the class/target.\n",
    "    - feature is a tensor of shape (seq_len, num_channels), where seq_len may vary.\n",
    "\n",
    "    This function stacks labels and pads features along the time dimension so that\n",
    "    all sequences in the batch have the same length.\n",
    "    \"\"\"\n",
    "    # Unzip the batch into labels and features\n",
    "    labels, features = zip(*batch)\n",
    "    \n",
    "    labels = torch.stack(labels)\n",
    "    # Optionally: labels = labels.unsqueeze(1)  # Uncomment if required by your loss function\n",
    "    padded_features = pad_sequence(features, batch_first=True)\n",
    "    \n",
    "    return labels, padded_features\n",
    "\n",
    "# Initialize dataset\n",
    "full_dataset = EEGDataset(config['data_path'])\n",
    "\n",
    "# Split dataset\n",
    "train_set, val_set, test_set = full_dataset.split_dataset(\n",
    "    ratios=config['split_ratios']\n",
    ")\n",
    "\n",
    "len_dataset = len(train_set)\n",
    "sample = train_set[0]\n",
    "label_shape = sample[0].shape\n",
    "feature_shape = sample[1].shape\n",
    "\n",
    "print(f\"unbalanced train dataset shape: ({len_dataset}, [labels: {label_shape}, features: {list(feature_shape)}])\")\n",
    "\n",
    "# Balance training set\n",
    "train_set.apply_smote()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=8,        # Use class-conscious workers\n",
    "    pin_memory=True,      # For GPU acceleration\n",
    "    persistent_workers=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(val_set, batch_size=config['batch_size'], collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=config['batch_size'], collate_fn=collate_fn)\n",
    "\n",
    "len_dataset = len(train_set)\n",
    "sample = train_set[0]\n",
    "label_shape = sample[0].shape\n",
    "feature_shape = sample[1].shape\n",
    "\n",
    "print(f\"train dataset shape: ({len_dataset}, [labels: {label_shape}, features: {list(feature_shape)}])\")\n",
    "\n",
    "def train_model(config):    \n",
    "    # Model initialization\n",
    "    model = EEGDSConv(dropout=config['dropout']).to(config['device'])\n",
    "    \n",
    "    # Loss function with automatic class weighting\n",
    "    pos_weight = torch.tensor([\n",
    "        train_set.class_weights['A'] / train_set.class_weights['P']\n",
    "    ]).to(config['device'])\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['lr'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    writer = SummaryWriter(log_dir=config['log_dir'])\n",
    "    best_metric = float('inf')\n",
    "    \n",
    "    for epoch in tqdm(range(config['epochs'])):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for labels, features in train_loader:\n",
    "            features = features.to(config['device'])\n",
    "            labels = labels.to(config['device'])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for labels, features in val_loader:\n",
    "                features = features.to(config['device'])\n",
    "                labels = labels.to(config['device'])\n",
    "                \n",
    "                outputs = model(features)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                \n",
    "                preds = torch.sigmoid(outputs)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        predictions = (np.array(all_preds) > 0.5).astype(int)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Log metrics\n",
    "        writer.add_scalars('Loss', {'train': train_loss, 'val': val_loss}, epoch)\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(all_labels, predictions),\n",
    "            'precision': precision_score(all_labels, predictions),\n",
    "            'recall': recall_score(all_labels, predictions),\n",
    "            'f1': f1_score(all_labels, predictions)\n",
    "        }\n",
    "        writer.add_scalars('Metrics', metrics, epoch)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_metric:\n",
    "            best_metric = val_loss\n",
    "            torch.save(model.state_dict(), f\"{config['log_dir']}/best_model.pth\")\n",
    "    \n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "# Start training\n",
    "trained_model = train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_85623/1139620167.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model = torch.load('best_model.torch')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming model, criterion, test_loader, device, writer, and epoch are already defined\u001b[39;00m\n\u001b[1;32m      6\u001b[0m best_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.torch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()\n\u001b[1;32m      8\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      9\u001b[0m all_test_markers \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Assuming model, criterion, test_loader, device, writer, and epoch are already defined\n",
    "best_model = torch.load('best_model.torch')\n",
    "best_model.eval()\n",
    "test_loss = 0\n",
    "all_test_markers = []\n",
    "all_test_predictions = []\n",
    "with torch.no_grad():\n",
    "    for markers, features in tqdm(test_loader):\n",
    "        features = features.to(device)\n",
    "        markers = markers.unsqueeze(-1).to(device)\n",
    "\n",
    "        outputs = best_model(features)\n",
    "        loss = criterion(outputs, markers)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Collect markers and predictions for metrics calculation\n",
    "        all_test_markers.extend(markers.cpu().numpy().flatten())\n",
    "        all_test_predictions.extend(torch.sigmoid(outputs).cpu().numpy().flatten())\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "# Calculate test metrics\n",
    "test_accuracy = accuracy_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_precision = precision_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_recall = recall_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_f1 = f1_score(all_test_markers, [1 if p > 0.5 else 0 for p in all_test_predictions])\n",
    "test_roc_auc = roc_auc_score(all_test_markers, all_test_predictions)\n",
    "\n",
    "# Log test metrics to TensorBoard\n",
    "writer.add_scalar('Metrics/test_accuracy', test_accuracy, epoch)\n",
    "writer.add_scalar('Metrics/test_precision', test_precision, epoch)\n",
    "writer.add_scalar('Metrics/test_recall', test_recall, epoch)\n",
    "writer.add_scalar('Metrics/test_f1', test_f1, epoch)\n",
    "writer.add_scalar('Metrics/test_roc_auc', test_roc_auc, epoch)\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "{test_accuracy=}\n",
    "{test_precision=}\n",
    "{test_recall=}\n",
    "{test_f1=}\n",
    "{test_roc_auc=}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "best_threshold = 0.0\n",
    "best_f1 = 0.0\n",
    "thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "\n",
    "for threshold in tqdm(thresholds):\n",
    "    binary_predictions = (all_test_predictions > threshold).astype(int)\n",
    "    current_recall = f1_score(all_test_markers, binary_predictions)\n",
    "\n",
    "    if current_recall > best_f1:\n",
    "        best_f1 = current_recall\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"{best_threshold=}\")\n",
    "print(f\"{best_f1=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "import numpy as np\n",
    "best_threshold = 0.1\n",
    "best_recall = 0.0\n",
    "thresholds = np.arange(0.1, 1.0, 0.01)\n",
    "\n",
    "for threshold in tqdm(thresholds):\n",
    "    binary_predictions = (all_test_predictions > threshold).astype(int)\n",
    "    current_recall = recall_score(all_test_markers, binary_predictions)\n",
    "\n",
    "    if current_recall > best_recall:\n",
    "        best_recall = current_recall\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"{best_threshold=}\")\n",
    "print(f\"{best_recall=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
