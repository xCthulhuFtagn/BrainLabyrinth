{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import janitor\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "data_path = '/home/owner/Documents/DEV/BrainLabyrinth/data/Ymaze_exp'\n",
    "final_dataset_path = '/home/owner/Documents/DEV/BrainLabyrinth/data/final_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sklearn.decomposition import PCA as sklearnPCA\n",
    "# import scipy.signal as signal\n",
    "# from mne.preprocessing import ICA\n",
    "\n",
    "\n",
    "# for user_id in tqdm(os.listdir(data_path)):\n",
    "#     person_dir = os.path.join(data_path, user_id)\n",
    "#     for file in os.listdir(person_dir):\n",
    "#         file_path = os.path.join(person_dir, file)\n",
    "\n",
    "#         if file.endswith(\".vhdr\"):\n",
    "#             # Read the EEG data\n",
    "#             raw = mne.io.read_raw_brainvision(file_path, preload=True, ignore_marker_types=True, verbose=False)\n",
    "            \n",
    "#             # Apply preprocessing steps\n",
    "\n",
    "#             # 1. Resample to 500 Hz\n",
    "#             raw.resample(500, npad=\"auto\")  # Resample to 500 Hz\n",
    "            \n",
    "#             # 2. Filter high-pass at 0.1 Hz (for low-frequency noise removal)\n",
    "#             raw.filter(l_freq=0.1, h_freq=30)  # High-pass filter (low-frequency cutoff at 0.1 Hz)\n",
    "            \n",
    "#             # 3. Notch filter at 50 Hz (to remove power line noise)\n",
    "#             raw.notch_filter(freqs=50, picks='all')  # Notch filter for 50 Hz\n",
    "\n",
    "#             # 4. ICA for blink removal\n",
    "#             ica = ICA(n_components=20, method='fastica', max_iter=2000, random_state=97)\n",
    "#             ica.fit(raw, picks='eeg')  # Fit to EEG channels excluding EOG\n",
    "            \n",
    "#             # Detect EOG artifacts\n",
    "#             eog_indices, eog_scores = ica.find_bads_eog(raw, ch_name='EOG', threshold=2.0)\n",
    "#             ica.exclude = eog_indices\n",
    "            \n",
    "#             # Apply ICA cleaning\n",
    "#             raw = ica.apply(raw)\n",
    "#             raw.drop_channels(['EOG'])  # Remove auxiliary EOG channel after cleaning\n",
    "\n",
    "#             # 4. Detrend the data using scipy.signal.detrend\n",
    "#             raw._data = signal.detrend(raw._data, axis=1)  # Detrend along the time axis (axis=1)\n",
    "            \n",
    "#             # Continue with the rest of your pipeline as you have it\n",
    "#             # Find the corresponding .vmrk file\n",
    "#             vmrk_file = file.replace('.vhdr', '.vmrk')\n",
    "#             vmrk_file_path = os.path.join(person_dir, vmrk_file)\n",
    "\n",
    "#             if os.path.exists(vmrk_file_path):\n",
    "#                 # Read the annotations (markers) from the .vmrk file\n",
    "#                 annotations = mne.annotations.read_annotations(vmrk_file_path)\n",
    "#             # Convert raw data to DataFrame\n",
    "#             time_series = raw.to_data_frame()\n",
    "\n",
    "#             # Extract markers (annotations)\n",
    "#             marker_times = annotations.onset  # In seconds\n",
    "#             marker_labels = annotations.description  # The marker labels\n",
    "\n",
    "#             # Create a DataFrame for the markers\n",
    "#             markers_df = pd.DataFrame({\n",
    "#                 'event_id': np.arange(len(marker_labels)), \n",
    "#                 'start': marker_times - 3.0,\n",
    "#                 'end': marker_times + 1.0,\n",
    "#                 'marker': marker_labels\n",
    "#             })\n",
    "            \n",
    "#             markers_df = markers_df[\n",
    "#                 ~markers_df.marker.isin([\n",
    "#                     'Marker/Impedance', 'New Segment/'#, 'Stimulus/2'\n",
    "#                 ])\n",
    "#             ]\n",
    "#             # markers_df.marker = markers_df.marker.replace({'Stimulus/2': 'Stimulus/P'})\n",
    "\n",
    "#             # Display the first few rows of both DataFrames\n",
    "#             # Merge markers with EEG data\n",
    "#             time_series['time'] = time_series['time'].round(3)  # Round times to 3 decimal places for matching\n",
    "\n",
    "#             merged_df = janitor.conditional_join(\n",
    "#                 markers_df, \n",
    "#                 time_series,\n",
    "#                 ('start', 'time', '<='),\n",
    "#                 ('end', 'time', '>='),\n",
    "#                 how='left',\n",
    "#                 df_columns=['event_id', 'marker']\n",
    "#             )\n",
    "#             # display(merged_df.groupby('event_id').count())\n",
    "#             merged_path = os.path.join(final_dataset_path, f\"{user_id}.parquet\")\n",
    "#             merged_df.to_parquet(merged_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ok = os.listdir('/home/owner/Documents/DEV/BrainLabyrinth/data/final_dataset copy')\n",
    "\n",
    "cleaned_dir = '/home/owner/Documents/DEV/BrainLabyrinth/data/final_dataset'\n",
    "for file in os.listdir(cleaned_dir):\n",
    "    if file not in ok:\n",
    "        os.remove(os.path.join(cleaned_dir, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce99f21fe534b949015569f98cfc726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: /home/owner/Documents/DEV/BrainLabyrinth/data/combined_prev_prev_2.parquet\n"
     ]
    }
   ],
   "source": [
    "parquet_directory = \"/home/owner/Documents/DEV/BrainLabyrinth/data/final_dataset\"\n",
    "\n",
    "parquet_files = [\n",
    "    os.path.join(parquet_directory, f)\n",
    "    for f in os.listdir(parquet_directory)\n",
    "    if f.endswith(\".parquet\")\n",
    "]\n",
    "\n",
    "def next_direction(marker: str, last_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the new direction (\"Left\" or \"Right\" or None) based on:\n",
    "      - Stimulus/1 => \"Left\"\n",
    "      - Stimulus/2 => \"Right\"\n",
    "      - Stimulus/A => flip last_dir (None -> None)\n",
    "      - Stimulus/P => same as last_dir (None -> None)\n",
    "      - else => keep last_dir\n",
    "    \"\"\"\n",
    "    base_map = {\n",
    "        \"Stimulus/1\": \"Left\",\n",
    "        \"Stimulus/2\": \"Right\",\n",
    "    }\n",
    "\n",
    "    if marker in base_map:\n",
    "        return base_map[marker]\n",
    "    if marker == \"Stimulus/A\":\n",
    "        if last_dir is None:\n",
    "            return None\n",
    "        return \"Right\" if last_dir == \"Left\" else \"Left\"\n",
    "    if marker == \"Stimulus/P\":\n",
    "        return last_dir  # could be None\n",
    "\n",
    "    return last_dir\n",
    "\n",
    "def label_whole_block(pdf, event_col=\"event_id\", marker_col=\"orig_marker\", time_col=\"time\"):\n",
    "    \"\"\"\n",
    "    1) Remove 'Stimulus/s16' rows from 'orig_marker'.\n",
    "    2) Sort by 'time'.\n",
    "    3) Sort blocks by earliest time in each event_id.\n",
    "    4) For each block, determine new direction from 'orig_marker'.\n",
    "       Save the 'prev_marker'.\n",
    "    5) Drop first-turn events (prev_marker == None).\n",
    "    6) Rename 'direction' -> 'marker'.\n",
    "    \"\"\"\n",
    "    # 1) Remove 'Stimulus/s16'\n",
    "    pdf = pdf.loc[pdf[marker_col] != \"Stimulus/s16\"].copy()\n",
    "\n",
    "    # 2) Sort by time\n",
    "    pdf = pdf.sort_values(time_col)\n",
    "\n",
    "    # 3) Sort blocks by earliest time\n",
    "    earliest_times = pdf.groupby(event_col)[time_col].min().sort_values()\n",
    "    sorted_event_ids = earliest_times.index.tolist()\n",
    "\n",
    "    # Single marker per block\n",
    "    block_marker_map = (\n",
    "        pdf.groupby(event_col)[marker_col]\n",
    "           .first()\n",
    "           .to_dict()\n",
    "    )\n",
    "\n",
    "    block_dir_map = {}\n",
    "    block_prev_map = {}\n",
    "    block_prev_prev_map = {}\n",
    "\n",
    "    prev_direction = None\n",
    "    prev_prev_direction = None\n",
    "\n",
    "    for e_id in sorted_event_ids:\n",
    "        event_marker = block_marker_map[e_id]\n",
    "        block_prev_map[e_id] = prev_direction\n",
    "        block_prev_prev_map[e_id] = prev_prev_direction\n",
    "\n",
    "        direction = next_direction(event_marker, prev_direction)\n",
    "        block_dir_map[e_id] = direction\n",
    "\n",
    "        prev_prev_direction = prev_direction\n",
    "        prev_direction = direction\n",
    "\n",
    "    pdf[\"prev_prev_marker\"] = pdf[event_col].map(block_prev_prev_map)\n",
    "    pdf[\"prev_marker\"] = pdf[event_col].map(block_prev_map)\n",
    "    pdf[\"direction\"] = pdf[event_col].map(block_dir_map)\n",
    "\n",
    "    # Drop first-turn events\n",
    "    pdf = pdf.loc[pdf[\"prev_marker\"].notna() & pdf[\"prev_prev_marker\"].notna()].copy()\n",
    "\n",
    "    # Rename 'direction' -> 'marker'\n",
    "    pdf.rename(columns={\"direction\": \"marker\"}, inplace=True)\n",
    "\n",
    "    return pdf\n",
    "\n",
    "\n",
    "all_dfs = []\n",
    "for file_path in tqdm(parquet_files):\n",
    "    # Read the Parquet file\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    # RENAME original 'marker' => 'orig_marker' so we don't conflict later\n",
    "    df.rename(columns={\"marker\": \"orig_marker\"}, inplace=True)\n",
    "\n",
    "    # Label each block\n",
    "    labeled_df = label_whole_block(df)\n",
    "  \n",
    "    # df.drop(columns=[\"orig_marker\"], inplace=True)\n",
    "\n",
    "    # Make event_id unique per file\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    labeled_df[\"event_id\"] = labeled_df[\"event_id\"].astype(str) + \"_\" + file_name\n",
    "\n",
    "    all_dfs.append(labeled_df)\n",
    "\n",
    "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "# if \"orig_marker\" in combined_df.columns:\n",
    "#    combined_df.drop(columns=[\"orig_marker\"], inplace=True)\n",
    "\n",
    "# Encode event_id\n",
    "unique_events = combined_df[\"event_id\"].unique()\n",
    "event_map = {ev: i for i, ev in enumerate(unique_events)}\n",
    "combined_df[\"event_id\"] = combined_df[\"event_id\"].map(event_map).astype(\"int64\")\n",
    "\n",
    "# Write to Parquet with no duplicates\n",
    "output_file = \"/home/owner/Documents/DEV/BrainLabyrinth/data/combined_prev_prev_2.parquet\"\n",
    "combined_df.to_parquet(output_file)\n",
    "print(\"Saved to:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['event_id', 'orig_marker', 'time', 'Fp1', 'Fpz', 'Fp2', 'F7', 'F3',\n",
       "       'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6', 'M1', 'T7', 'C3', 'Cz',\n",
       "       'C4', 'T8', 'M2', 'CP5', 'CP1', 'CP2', 'CP6', 'P7', 'P3', 'Pz', 'P4',\n",
       "       'P8', 'POz', 'O1', 'O2', 'AF7', 'AF3', 'AF4', 'AF8', 'F5', 'F1', 'F2',\n",
       "       'F6', 'FC3', 'FCz', 'FC4', 'C5', 'C1', 'C2', 'C6', 'CP3', 'CP4', 'P5',\n",
       "       'P1', 'P2', 'P6', 'PO5', 'PO3', 'PO4', 'PO6', 'FT7', 'FT8', 'TP7',\n",
       "       'TP8', 'PO7', 'PO8', 'Oz', 'prev_prev_marker', 'prev_marker', 'marker'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m parquet_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Read the Parquet files into a Dask DataFrame\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparquet_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Compute the shape of the Dask DataFrame\u001b[39;00m\n\u001b[1;32m      8\u001b[0m rows, cols \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/pandas/io/parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/pandas/io/parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet'"
     ]
    }
   ],
   "source": [
    "# Define the directory containing the Parquet files\n",
    "parquet_directory = '/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet'\n",
    "\n",
    "# Read the Parquet files into a Dask DataFrame\n",
    "df = pd.read_parquet(parquet_directory)\n",
    "\n",
    "# Compute the shape of the Dask DataFrame\n",
    "rows, cols = df.shape\n",
    "rows_computed = rows\n",
    "\n",
    "# Print the shape\n",
    "print(f\"Shape of the final dataset: ({rows_computed}, {cols})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
