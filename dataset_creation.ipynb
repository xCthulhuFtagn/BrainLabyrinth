{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import janitor\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "data_path = '/home/owner/Documents/DEV/BrainLabyrinth/data/Ymaze_exp'\n",
    "final_dataset_path = '/home/owner/Documents/DEV/BrainLabyrinth/data/final_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sklearn.decomposition import PCA as sklearnPCA\n",
    "# import scipy.signal as signal\n",
    "# from mne.preprocessing import ICA\n",
    "\n",
    "\n",
    "# for user_id in tqdm(os.listdir(data_path)):\n",
    "#     person_dir = os.path.join(data_path, user_id)\n",
    "#     for file in os.listdir(person_dir):\n",
    "#         file_path = os.path.join(person_dir, file)\n",
    "\n",
    "#         if file.endswith(\".vhdr\"):\n",
    "#             # Read the EEG data\n",
    "#             raw = mne.io.read_raw_brainvision(file_path, preload=True, ignore_marker_types=True, verbose=False)\n",
    "            \n",
    "#             # Apply preprocessing steps\n",
    "\n",
    "#             # 1. Resample to 500 Hz\n",
    "#             raw.resample(500, npad=\"auto\")  # Resample to 500 Hz\n",
    "            \n",
    "#             # 2. Filter high-pass at 0.1 Hz (for low-frequency noise removal)\n",
    "#             raw.filter(l_freq=0.1, h_freq=30)  # High-pass filter (low-frequency cutoff at 0.1 Hz)\n",
    "            \n",
    "#             # 3. Notch filter at 50 Hz (to remove power line noise)\n",
    "#             raw.notch_filter(freqs=50, picks='all')  # Notch filter for 50 Hz\n",
    "\n",
    "#             # 4. ICA for blink removal\n",
    "#             ica = ICA(n_components=20, method='fastica', max_iter=2000, random_state=97)\n",
    "#             ica.fit(raw, picks='eeg')  # Fit to EEG channels excluding EOG\n",
    "            \n",
    "#             # Detect EOG artifacts\n",
    "#             eog_indices, eog_scores = ica.find_bads_eog(raw, ch_name='EOG', threshold=2.0)\n",
    "#             ica.exclude = eog_indices\n",
    "            \n",
    "#             # Apply ICA cleaning\n",
    "#             raw = ica.apply(raw)\n",
    "#             raw.drop_channels(['EOG'])  # Remove auxiliary EOG channel after cleaning\n",
    "\n",
    "#             # 4. Detrend the data using scipy.signal.detrend\n",
    "#             raw._data = signal.detrend(raw._data, axis=1)  # Detrend along the time axis (axis=1)\n",
    "            \n",
    "#             # Continue with the rest of your pipeline as you have it\n",
    "#             # Find the corresponding .vmrk file\n",
    "#             vmrk_file = file.replace('.vhdr', '.vmrk')\n",
    "#             vmrk_file_path = os.path.join(person_dir, vmrk_file)\n",
    "\n",
    "#             if os.path.exists(vmrk_file_path):\n",
    "#                 # Read the annotations (markers) from the .vmrk file\n",
    "#                 annotations = mne.annotations.read_annotations(vmrk_file_path)\n",
    "#             # Convert raw data to DataFrame\n",
    "#             time_series = raw.to_data_frame()\n",
    "\n",
    "#             # Extract markers (annotations)\n",
    "#             marker_times = annotations.onset  # In seconds\n",
    "#             marker_labels = annotations.description  # The marker labels\n",
    "\n",
    "#             # Create a DataFrame for the markers\n",
    "#             markers_df = pd.DataFrame({\n",
    "#                 'event_id': np.arange(len(marker_labels)), \n",
    "#                 'start': marker_times - 3.0,\n",
    "#                 'end': marker_times + 1.0,\n",
    "#                 'marker': marker_labels\n",
    "#             })\n",
    "            \n",
    "#             markers_df = markers_df[\n",
    "#                 ~markers_df.marker.isin([\n",
    "#                     'Marker/Impedance', 'New Segment/'#, 'Stimulus/2'\n",
    "#                 ])\n",
    "#             ]\n",
    "#             # markers_df.marker = markers_df.marker.replace({'Stimulus/2': 'Stimulus/P'})\n",
    "\n",
    "#             # Display the first few rows of both DataFrames\n",
    "#             # Merge markers with EEG data\n",
    "#             time_series['time'] = time_series['time'].round(3)  # Round times to 3 decimal places for matching\n",
    "\n",
    "#             merged_df = janitor.conditional_join(\n",
    "#                 markers_df, \n",
    "#                 time_series,\n",
    "#                 ('start', 'time', '<='),\n",
    "#                 ('end', 'time', '>='),\n",
    "#                 how='left',\n",
    "#                 df_columns=['event_id', 'marker']\n",
    "#             )\n",
    "#             # display(merged_df.groupby('event_id').count())\n",
    "#             merged_path = os.path.join(final_dataset_path, f\"{user_id}.parquet\")\n",
    "#             merged_df.to_parquet(merged_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ok = os.listdir('/home/owner/Documents/DEV/BrainLabyrinth/data/final_dataset copy')\n",
    "\n",
    "cleaned_dir = '/home/owner/Documents/DEV/BrainLabyrinth/data/final_dataset'\n",
    "for file in os.listdir(cleaned_dir):\n",
    "    if file not in ok:\n",
    "        os.remove(os.path.join(cleaned_dir, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563b6f9f02354073ab3490b862c7c251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: /home/owner/Documents/DEV/BrainLabyrinth/data/combined_prev_prev.parquet\n"
     ]
    }
   ],
   "source": [
    "parquet_directory = \"/home/owner/Documents/DEV/BrainLabyrinth/data/final_dataset\"\n",
    "\n",
    "parquet_files = [\n",
    "    os.path.join(parquet_directory, f)\n",
    "    for f in os.listdir(parquet_directory)\n",
    "    if f.endswith(\".parquet\")\n",
    "]\n",
    "\n",
    "def next_direction(marker: str, last_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the new direction (\"Left\" or \"Right\" or None) based on:\n",
    "      - Stimulus/1 => \"Left\"\n",
    "      - Stimulus/2 => \"Right\"\n",
    "      - Stimulus/A => flip last_dir (None -> None)\n",
    "      - Stimulus/P => same as last_dir (None -> None)\n",
    "      - else => keep last_dir\n",
    "    \"\"\"\n",
    "    base_map = {\n",
    "        \"Stimulus/1\": \"Left\",\n",
    "        \"Stimulus/2\": \"Right\",\n",
    "    }\n",
    "\n",
    "    if marker in base_map:\n",
    "        return base_map[marker]\n",
    "    if marker == \"Stimulus/A\":\n",
    "        if last_dir is None:\n",
    "            return None\n",
    "        return \"Right\" if last_dir == \"Left\" else \"Left\"\n",
    "    if marker == \"Stimulus/P\":\n",
    "        return last_dir  # could be None\n",
    "\n",
    "    return last_dir\n",
    "\n",
    "def label_whole_block(pdf, event_col=\"event_id\", marker_col=\"orig_marker\", time_col=\"time\"):\n",
    "    \"\"\"\n",
    "    1) Remove 'Stimulus/s16' rows from 'orig_marker'.\n",
    "    2) Sort by 'time'.\n",
    "    3) Sort blocks by earliest time in each event_id.\n",
    "    4) For each block, determine new direction from 'orig_marker'.\n",
    "       Save the 'prev_marker'.\n",
    "    5) Drop first-turn events (prev_marker == None).\n",
    "    6) Rename 'direction' -> 'marker'.\n",
    "    \"\"\"\n",
    "    # 1) Remove 'Stimulus/s16'\n",
    "    pdf = pdf.loc[pdf[marker_col] != \"Stimulus/s16\"].copy()\n",
    "\n",
    "    # 2) Sort by time\n",
    "    pdf = pdf.sort_values(time_col)\n",
    "\n",
    "    # 3) Sort blocks by earliest time\n",
    "    earliest_times = pdf.groupby(event_col)[time_col].min().sort_values()\n",
    "    sorted_event_ids = earliest_times.index.tolist()\n",
    "\n",
    "    # Single marker per block\n",
    "    block_marker_map = (\n",
    "        pdf.groupby(event_col)[marker_col]\n",
    "           .first()\n",
    "           .to_dict()\n",
    "    )\n",
    "\n",
    "    block_dir_map = {}\n",
    "    block_prev_map = {}\n",
    "    block_prev_prev_map = {}\n",
    "\n",
    "    prev_direction = None\n",
    "    prev_prev_direction = None\n",
    "\n",
    "    for e_id in sorted_event_ids:\n",
    "        event_marker = block_marker_map[e_id]\n",
    "        block_prev_map[e_id] = prev_direction\n",
    "        block_prev_prev_map[e_id] = prev_prev_direction\n",
    "\n",
    "        direction = next_direction(event_marker, prev_direction)\n",
    "        block_dir_map[e_id] = direction\n",
    "\n",
    "        prev_prev_direction = prev_direction\n",
    "        prev_direction = direction\n",
    "\n",
    "    pdf[\"prev_prev_marker\"] = pdf[event_col].map(block_prev_prev_map)\n",
    "    pdf[\"prev_marker\"] = pdf[event_col].map(block_prev_map)\n",
    "    pdf[\"direction\"] = pdf[event_col].map(block_dir_map)\n",
    "\n",
    "    # Drop first-turn events\n",
    "    pdf = pdf.loc[pdf[\"prev_marker\"].notna() & pdf[\"prev_prev_marker\"].notna()].copy()\n",
    "\n",
    "    # Rename 'direction' -> 'marker'\n",
    "    pdf.rename(columns={\"direction\": \"marker\"}, inplace=True)\n",
    "\n",
    "    return pdf\n",
    "\n",
    "\n",
    "all_dfs = []\n",
    "for file_path in tqdm(parquet_files):\n",
    "    # Read the Parquet file\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    # RENAME original 'marker' => 'orig_marker' so we don't conflict later\n",
    "    df.rename(columns={\"marker\": \"orig_marker\"}, inplace=True)\n",
    "\n",
    "    # Label each block\n",
    "    labeled_df = label_whole_block(df)\n",
    "  \n",
    "    # df.drop(columns=[\"orig_marker\"], inplace=True)\n",
    "\n",
    "    # Make event_id unique per file\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    labeled_df[\"event_id\"] = labeled_df[\"event_id\"].astype(str) + \"_\" + file_name\n",
    "\n",
    "    all_dfs.append(labeled_df)\n",
    "\n",
    "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "if \"orig_marker\" in combined_df.columns:\n",
    "   combined_df.drop(columns=[\"orig_marker\"], inplace=True)\n",
    "\n",
    "# Encode event_id\n",
    "unique_events = combined_df[\"event_id\"].unique()\n",
    "event_map = {ev: i for i, ev in enumerate(unique_events)}\n",
    "combined_df[\"event_id\"] = combined_df[\"event_id\"].map(event_map).astype(\"int64\")\n",
    "\n",
    "# Write to Parquet with no duplicates\n",
    "output_file = \"/home/owner/Documents/DEV/BrainLabyrinth/data/combined_prev_prev.parquet\"\n",
    "combined_df.to_parquet(output_file)\n",
    "print(\"Saved to:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['event_id', 'time', 'Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8',\n",
       "       'FC5', 'FC1', 'FC2', 'FC6', 'M1', 'T7', 'C3', 'Cz', 'C4', 'T8', 'M2',\n",
       "       'CP5', 'CP1', 'CP2', 'CP6', 'P7', 'P3', 'Pz', 'P4', 'P8', 'POz', 'O1',\n",
       "       'O2', 'AF7', 'AF3', 'AF4', 'AF8', 'F5', 'F1', 'F2', 'F6', 'FC3', 'FCz',\n",
       "       'FC4', 'C5', 'C1', 'C2', 'C6', 'CP3', 'CP4', 'P5', 'P1', 'P2', 'P6',\n",
       "       'PO5', 'PO3', 'PO4', 'PO6', 'FT7', 'FT8', 'TP7', 'TP8', 'PO7', 'PO8',\n",
       "       'Oz', 'prev_prev_marker', 'prev_marker', 'marker'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "combined_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the final dataset: (5544008, 68)\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing the Parquet files\n",
    "parquet_directory = '/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet'\n",
    "\n",
    "# Read the Parquet files into a Dask DataFrame\n",
    "df = pd.read_parquet(parquet_directory)\n",
    "\n",
    "# Compute the shape of the Dask DataFrame\n",
    "rows, cols = df.shape\n",
    "rows_computed = rows\n",
    "\n",
    "# Print the shape\n",
    "print(f\"Shape of the final dataset: ({rows_computed}, {cols})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
