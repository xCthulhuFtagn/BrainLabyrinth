{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import janitor\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "data_path = '/home/owner/Documents/DEV/BrainLabyrinth/data/Ymaze_exp'\n",
    "final_dataset_path = '/home/owner/Documents/DEV/BrainLabyrinth/data/final_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA as sklearnPCA\n",
    "import scipy.signal as signal\n",
    "from mne.preprocessing import ICA\n",
    "\n",
    "\n",
    "for user_id in tqdm(os.listdir(data_path)):\n",
    "    person_dir = os.path.join(data_path, user_id)\n",
    "    for file in os.listdir(person_dir):\n",
    "        file_path = os.path.join(person_dir, file)\n",
    "\n",
    "        if file.endswith(\".vhdr\"):\n",
    "            # Read the EEG data\n",
    "            raw = mne.io.read_raw_brainvision(file_path, preload=True, ignore_marker_types=True, verbose=False)\n",
    "            \n",
    "            # Apply preprocessing steps\n",
    "\n",
    "            # 1. Resample to 500 Hz\n",
    "            raw.resample(500, npad=\"auto\")  # Resample to 500 Hz\n",
    "            \n",
    "            # 2. Filter high-pass at 0.1 Hz (for low-frequency noise removal)\n",
    "            raw.filter(l_freq=0.1, h_freq=30)  # High-pass filter (low-frequency cutoff at 0.1 Hz)\n",
    "            \n",
    "            # 3. Notch filter at 50 Hz (to remove power line noise)\n",
    "            raw.notch_filter(freqs=50, picks='all')  # Notch filter for 50 Hz\n",
    "\n",
    "            # 4. ICA for blink removal\n",
    "            ica = ICA(n_components=20, method='fastica', max_iter=2000, random_state=97)\n",
    "            ica.fit(raw, picks='eeg')  # Fit to EEG channels excluding EOG\n",
    "            \n",
    "            # Detect EOG artifacts\n",
    "            eog_indices, eog_scores = ica.find_bads_eog(raw, ch_name='EOG', threshold=2.0)\n",
    "            ica.exclude = eog_indices\n",
    "            \n",
    "            # Apply ICA cleaning\n",
    "            raw = ica.apply(raw)\n",
    "            raw.drop_channels(['EOG'])  # Remove auxiliary EOG channel after cleaning\n",
    "\n",
    "            # 4. Detrend the data using scipy.signal.detrend\n",
    "            raw._data = signal.detrend(raw._data, axis=1)  # Detrend along the time axis (axis=1)\n",
    "\n",
    "            # 5. Apply PCA for dimensionality reduction (if necessary)\n",
    "            # n_components = 20\n",
    "            # pca = sklearnPCA(n_components=n_components)\n",
    "            # data_pca = pca.fit_transform(raw.get_data())  # Apply PCA to the EEG data\n",
    "            # raw._data = data_pca  # Replace the original data with the PCA-transformed data\n",
    "            \n",
    "            # Continue with the rest of your pipeline as you have it\n",
    "            # Find the corresponding .vmrk file\n",
    "            vmrk_file = file.replace('.vhdr', '.vmrk')\n",
    "            vmrk_file_path = os.path.join(person_dir, vmrk_file)\n",
    "\n",
    "            if os.path.exists(vmrk_file_path):\n",
    "                # Read the annotations (markers) from the .vmrk file\n",
    "                annotations = mne.annotations.read_annotations(vmrk_file_path)\n",
    "            # Convert raw data to DataFrame\n",
    "            time_series = raw.to_data_frame()\n",
    "\n",
    "            # Extract markers (annotations)\n",
    "            marker_times = annotations.onset  # In seconds\n",
    "            marker_labels = annotations.description  # The marker labels\n",
    "\n",
    "            # Create a DataFrame for the markers\n",
    "            markers_df = pd.DataFrame({\n",
    "                'event_id': np.arange(len(marker_labels)), \n",
    "                'start': marker_times - 3.0,\n",
    "                'end': marker_times + 1.0,\n",
    "                'marker': marker_labels\n",
    "            })\n",
    "            \n",
    "            markers_df = markers_df[\n",
    "                ~markers_df.marker.isin([\n",
    "                    'Marker/Impedance', 'New Segment/'#, 'Stimulus/2'\n",
    "                ])\n",
    "            ]\n",
    "            # markers_df.marker = markers_df.marker.replace({'Stimulus/2': 'Stimulus/P'})\n",
    "\n",
    "            # Display the first few rows of both DataFrames\n",
    "            # Merge markers with EEG data\n",
    "            time_series['time'] = time_series['time'].round(3)  # Round times to 3 decimal places for matching\n",
    "\n",
    "            merged_df = janitor.conditional_join(\n",
    "                markers_df, \n",
    "                time_series,\n",
    "                ('start', 'time', '<='),\n",
    "                ('end', 'time', '>='),\n",
    "                how='left',\n",
    "                df_columns=['event_id', 'marker']\n",
    "            )\n",
    "            # display(merged_df.groupby('event_id').count())\n",
    "            merged_path = os.path.join(final_dataset_path, f\"{user_id}.parquet\")\n",
    "            merged_df.to_parquet(merged_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ok = os.listdir('/home/owner/Documents/DEV/BrainLabyrinth/data/final_dataset copy')\n",
    "\n",
    "cleaned_dir = '/home/owner/Documents/DEV/BrainLabyrinth/data/final_dataset'\n",
    "for file in os.listdir(cleaned_dir):\n",
    "    if file not in ok:\n",
    "        os.remove(os.path.join(cleaned_dir, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac3184028524de4a12fd1279d6e0c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the directory containing the Parquet files\n",
    "parquet_directory = '/home/owner/Documents/DEV/BrainLabyrinth/data/final_dataset'\n",
    "\n",
    "# List all Parquet files in the directory\n",
    "parquet_files = [os.path.join(parquet_directory, f) for f in os.listdir(parquet_directory) if f.endswith('.parquet')]\n",
    "\n",
    "\n",
    "def restore_stimulus_direction(pdf, event_col='event_id', marker_col='marker', time_col='time'):\n",
    "    \"\"\"\n",
    "    For each event_id:\n",
    "      1) Sort by time_col.\n",
    "      2) Remove all 'Stimulus/s16' rows.\n",
    "      3) Mark the first row that is Stimulus/1 or Stimulus/2 (is_first_turn=1) but do NOT drop it yet.\n",
    "      4) Determine base_dir (+1 / -1) from that first turn, apply flips for Stimulus/A.\n",
    "      5) Convert all 1/2/A/P to 'Left'/'Right'.\n",
    "      6) Create a 'prev_turn' column: the previous row's turn label ('Left'/'Right') within the same event_id.\n",
    "      7) Drop rows where 'prev_turn' is NaN (i.e., the first turn row) using dropna.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Sort by time\n",
    "    pdf = pdf.sort_values(time_col).copy()\n",
    "\n",
    "    # 2) Remove 'Stimulus/s16' rows\n",
    "    pdf = pdf[pdf[marker_col] != 'Stimulus/s16']\n",
    "\n",
    "    # Make a mask for rows that are Stimulus/1 or Stimulus/2\n",
    "    mask_1_2 = pdf[marker_col].isin(['Stimulus/1','Stimulus/2'])\n",
    "\n",
    "    # Step 3: For each event_id, find the *first* row that is 1 or 2\n",
    "    pdf['temp_1_2'] = mask_1_2.astype(int)\n",
    "    pdf['cumsum_1_2'] = pdf.groupby(event_col)['temp_1_2'].cumsum()\n",
    "    pdf['is_first_turn'] = np.where(pdf['cumsum_1_2'] == 1, 1, 0)\n",
    "\n",
    "    # Build a dict: event_id -> base_dir from the first turn\n",
    "    first_turn_dir = (\n",
    "        pdf.loc[pdf['is_first_turn'] == 1, [event_col, marker_col]]\n",
    "          .groupby(event_col)[marker_col]\n",
    "          .first()  # the first turn in each event_id\n",
    "          .map({'Stimulus/1': 1, 'Stimulus/2': -1})\n",
    "    )\n",
    "    pdf['base_dir'] = pdf[event_col].map(first_turn_dir)\n",
    "\n",
    "    # Step 4: flipping logic\n",
    "    # Stimulus/A => flip=1, everything else => flip=0\n",
    "    pdf['flip'] = 0\n",
    "    pdf.loc[pdf[marker_col] == 'Stimulus/A', 'flip'] = 1\n",
    "    pdf['flip_cum'] = pdf.groupby(event_col)['flip'].cumsum()\n",
    "\n",
    "    # final_dir = base_dir * ((-1)^flip_cum)\n",
    "    pdf['final_dir'] = pdf['base_dir'] * ((-1) ** pdf['flip_cum'])\n",
    "\n",
    "    # Step 5: rewrite rows in {1,2,A,P} => Left/Right if final_dir is not NaN\n",
    "    turn_set = ['Stimulus/1','Stimulus/2','Stimulus/A','Stimulus/P']\n",
    "    mask_turn = pdf[marker_col].isin(turn_set) & pdf['final_dir'].notna()\n",
    "\n",
    "    pdf.loc[mask_turn, marker_col] = np.where(\n",
    "        pdf.loc[mask_turn, 'final_dir'] == 1,\n",
    "        'Left',\n",
    "        'Right'\n",
    "    )\n",
    "\n",
    "    # Step 6: create a 'prev_turn' column to capture the *previous* turn label.\n",
    "    # A \"turn\" row is any row where marker is now 'Left' or 'Right'.\n",
    "    pdf['turn_dir'] = np.where(\n",
    "        pdf[marker_col].isin(['Left','Right']),\n",
    "        pdf[marker_col],\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # For each row, prev_turn is the turn_dir of the *previous* row that is a turn in the same event_id.\n",
    "    pdf['prev_turn'] = pdf.groupby(event_col)['turn_dir'].shift(1)\n",
    "\n",
    "    # Step 7: drop rows where prev_turn is NaN => removes the actual first turn row\n",
    "    pdf.dropna(subset=['prev_turn'], inplace=True)\n",
    "\n",
    "    # Clean up intermediate columns we don't want in final dataset\n",
    "    pdf.drop([\n",
    "        'temp_1_2','cumsum_1_2','is_first_turn',\n",
    "        'base_dir','flip','flip_cum','final_dir','turn_dir'\n",
    "    ], axis=1, inplace=True)\n",
    "\n",
    "    return pdf\n",
    "\n",
    "\n",
    "# Initialize an empty list to hold the Dask DataFrames\n",
    "dask_dfs = []\n",
    "\n",
    "# Process each Parquet file\n",
    "for file in tqdm(parquet_files):\n",
    "    # Read the Parquet file into a Dask DataFrame\n",
    "    df = pd.read_parquet(file)\n",
    "    \n",
    "    df = restore_stimulus_direction(df)\n",
    "    \n",
    "    # Extract the file name without the extension\n",
    "    file_name = os.path.basename(file).replace('.parquet', '')\n",
    "\n",
    "    # Create a unique event_id by combining the original event_id with the file name\n",
    "    df['event_id'] = df['event_id'].astype(str) + '_' + file_name\n",
    "\n",
    "    # Append the DataFrame to the list\n",
    "    dask_dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dask_dfs, ignore_index=True)\n",
    "\n",
    "# Create a mapping dictionary for unique event_id values\n",
    "unique_event_ids = combined_df['event_id'].unique()\n",
    "event_id_mapping = {event_id: idx for idx, event_id in enumerate(unique_event_ids)}\n",
    "\n",
    "# Renumber the event_id column using the mapping dictionary\n",
    "combined_df['event_id'] = combined_df['event_id'].map(event_id_mapping).astype('int64')\n",
    "\n",
    "\n",
    "# Save the combined Dask DataFrame to a new Parquet file\n",
    "combined_df.to_parquet('/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the final dataset: (79960, 67)\n",
      "   event_id marker   time        Fp1        Fpz       Fp2        F7        F3  \\\n",
      "0         0   Left  9.440  18.279552  17.167071  1.736013  2.509046  3.320807   \n",
      "1         0   Left  9.442  18.187657  17.195618  1.650717  2.619466  3.746728   \n",
      "2         0   Left  9.444  17.988738  17.064795  1.536693  2.780392  4.108368   \n",
      "3         0   Left  9.446  17.697904  16.795464  1.420282  2.984275  4.421709   \n",
      "4         0   Left  9.448  17.339646  16.426039  1.325767  3.213150  4.703400   \n",
      "\n",
      "          Fz        F4  ...        PO4       PO6       FT7       FT8  \\\n",
      "0  11.627260  6.565130  ... -10.643972  4.269458 -6.793052 -2.421540   \n",
      "1  12.211854  7.602871  ... -11.330150  2.269126 -7.381371 -1.549136   \n",
      "2  12.665638  8.477080  ... -12.088001  0.015087 -7.783466 -0.885222   \n",
      "3  12.995026  9.112953  ... -12.876470 -2.320403 -7.966721 -0.441055   \n",
      "4  13.219588  9.470200  ... -13.659191 -4.564826 -7.926686 -0.196698   \n",
      "\n",
      "         TP7       TP8        PO7       PO8         Oz  prev_turn  \n",
      "0 -10.742051 -3.052358  -7.947506  4.051073 -10.757459       Left  \n",
      "1 -10.951647 -2.573051  -8.422673  1.975578 -11.568488       Left  \n",
      "2 -11.181192 -2.053070  -8.959252 -0.346699 -12.895379       Left  \n",
      "3 -11.417038 -1.496988  -9.632474 -2.744580 -14.742146       Left  \n",
      "4 -11.648860 -0.889412 -10.494321 -5.045908 -17.039841       Left  \n",
      "\n",
      "[5 rows x 67 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['event_id', 'marker', 'time', 'Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz',\n",
       "       'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6', 'M1', 'T7', 'C3', 'Cz', 'C4',\n",
       "       'T8', 'M2', 'CP5', 'CP1', 'CP2', 'CP6', 'P7', 'P3', 'Pz', 'P4', 'P8',\n",
       "       'POz', 'O1', 'O2', 'AF7', 'AF3', 'AF4', 'AF8', 'F5', 'F1', 'F2', 'F6',\n",
       "       'FC3', 'FCz', 'FC4', 'C5', 'C1', 'C2', 'C6', 'CP3', 'CP4', 'P5', 'P1',\n",
       "       'P2', 'P6', 'PO5', 'PO3', 'PO4', 'PO6', 'FT7', 'FT8', 'TP7', 'TP8',\n",
       "       'PO7', 'PO8', 'Oz', 'prev_turn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the directory containing the Parquet files\n",
    "parquet_directory = '/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet'\n",
    "\n",
    "# Read the Parquet files into a Dask DataFrame\n",
    "df = pd.read_parquet(parquet_directory)\n",
    "\n",
    "# Compute the shape of the Dask DataFrame\n",
    "rows, cols = df.shape\n",
    "rows_computed = rows\n",
    "\n",
    "# Print the shape\n",
    "print(f\"Shape of the final dataset: ({rows_computed}, {cols})\")\n",
    "\n",
    "# Optionally, you can compute the first few rows to verify the data\n",
    "print(df.head())\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
