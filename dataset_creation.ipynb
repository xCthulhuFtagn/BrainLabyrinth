{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owner/Documents/DEV/BrainLabyrinth/.venv/lib/python3.10/site-packages/dask/dataframe/__init__.py:49: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import janitor\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "data_path = '/home/owner/Documents/DEV/BrainLabyrinth/data/Ymaze_exp'\n",
    "final_dataset_path = '/home/owner/Documents/DEV/BrainLabyrinth/data/final_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sklearn.decomposition import PCA as sklearnPCA\n",
    "# import scipy.signal as signal\n",
    "# from mne.preprocessing import ICA\n",
    "\n",
    "\n",
    "# for user_id in tqdm(os.listdir(data_path)):\n",
    "#     person_dir = os.path.join(data_path, user_id)\n",
    "#     for file in os.listdir(person_dir):\n",
    "#         file_path = os.path.join(person_dir, file)\n",
    "\n",
    "#         if file.endswith(\".vhdr\"):\n",
    "#             # Read the EEG data\n",
    "#             raw = mne.io.read_raw_brainvision(file_path, preload=True, ignore_marker_types=True, verbose=False)\n",
    "            \n",
    "#             # Apply preprocessing steps\n",
    "\n",
    "#             # 1. Resample to 500 Hz\n",
    "#             raw.resample(500, npad=\"auto\")  # Resample to 500 Hz\n",
    "            \n",
    "#             # 2. Filter high-pass at 0.1 Hz (for low-frequency noise removal)\n",
    "#             raw.filter(l_freq=0.1, h_freq=30)  # High-pass filter (low-frequency cutoff at 0.1 Hz)\n",
    "            \n",
    "#             # 3. Notch filter at 50 Hz (to remove power line noise)\n",
    "#             raw.notch_filter(freqs=50, picks='all')  # Notch filter for 50 Hz\n",
    "\n",
    "#             # 4. ICA for blink removal\n",
    "#             ica = ICA(n_components=20, method='fastica', max_iter=2000, random_state=97)\n",
    "#             ica.fit(raw, picks='eeg')  # Fit to EEG channels excluding EOG\n",
    "            \n",
    "#             # Detect EOG artifacts\n",
    "#             eog_indices, eog_scores = ica.find_bads_eog(raw, ch_name='EOG', threshold=2.0)\n",
    "#             ica.exclude = eog_indices\n",
    "            \n",
    "#             # Apply ICA cleaning\n",
    "#             raw = ica.apply(raw)\n",
    "#             raw.drop_channels(['EOG'])  # Remove auxiliary EOG channel after cleaning\n",
    "\n",
    "#             # 4. Detrend the data using scipy.signal.detrend\n",
    "#             raw._data = signal.detrend(raw._data, axis=1)  # Detrend along the time axis (axis=1)\n",
    "\n",
    "#             # 5. Apply PCA for dimensionality reduction (if necessary)\n",
    "#             # n_components = 20\n",
    "#             # pca = sklearnPCA(n_components=n_components)\n",
    "#             # data_pca = pca.fit_transform(raw.get_data())  # Apply PCA to the EEG data\n",
    "#             # raw._data = data_pca  # Replace the original data with the PCA-transformed data\n",
    "            \n",
    "#             # Continue with the rest of your pipeline as you have it\n",
    "#             # Find the corresponding .vmrk file\n",
    "#             vmrk_file = file.replace('.vhdr', '.vmrk')\n",
    "#             vmrk_file_path = os.path.join(person_dir, vmrk_file)\n",
    "\n",
    "#             if os.path.exists(vmrk_file_path):\n",
    "#                 # Read the annotations (markers) from the .vmrk file\n",
    "#                 annotations = mne.annotations.read_annotations(vmrk_file_path)\n",
    "#             # Convert raw data to DataFrame\n",
    "#             time_series = raw.to_data_frame()\n",
    "\n",
    "#             # Extract markers (annotations)\n",
    "#             marker_times = annotations.onset  # In seconds\n",
    "#             marker_labels = annotations.description  # The marker labels\n",
    "\n",
    "#             # Create a DataFrame for the markers\n",
    "#             markers_df = pd.DataFrame({\n",
    "#                 'event_id': np.arange(len(marker_labels)), \n",
    "#                 'start': marker_times - 3.0,\n",
    "#                 'end': marker_times + 1.0,\n",
    "#                 'marker': marker_labels\n",
    "#             })\n",
    "            \n",
    "#             markers_df = markers_df[\n",
    "#                 ~markers_df.marker.isin([\n",
    "#                     'Marker/Impedance', 'New Segment/', 'Stimulus/2'\n",
    "#                 ])\n",
    "#             ]\n",
    "#             # markers_df.marker = markers_df.marker.replace({'Stimulus/2': 'Stimulus/P'})\n",
    "\n",
    "#             # Display the first few rows of both DataFrames\n",
    "#             # Merge markers with EEG data\n",
    "#             time_series['time'] = time_series['time'].round(3)  # Round times to 3 decimal places for matching\n",
    "\n",
    "#             merged_df = janitor.conditional_join(\n",
    "#                 markers_df, \n",
    "#                 time_series,\n",
    "#                 ('start', 'time', '<='),\n",
    "#                 ('end', 'time', '>='),\n",
    "#                 how='left',\n",
    "#                 df_columns=['event_id', 'marker']\n",
    "#             )\n",
    "#             # display(merged_df.groupby('event_id').count())\n",
    "#             merged_path = os.path.join(final_dataset_path, f\"{user_id}.parquet\")\n",
    "#             merged_df.to_parquet(merged_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   event_id      marker   time        Fp1        Fpz       Fp2        F7  \\\n",
      "0         0  Stimulus/1  9.438  18.260990  16.979025  1.769803  2.441713   \n",
      "1         0  Stimulus/1  9.440  18.279552  17.167071  1.736013  2.509046   \n",
      "2         0  Stimulus/1  9.442  18.187657  17.195618  1.650717  2.619466   \n",
      "3         0  Stimulus/1  9.444  17.988738  17.064795  1.536693  2.780392   \n",
      "4         0  Stimulus/1  9.446  17.697904  16.795464  1.420282  2.984275   \n",
      "\n",
      "         F3         Fz        F4  ...       PO3        PO4       PO6  \\\n",
      "0  2.818692  10.922015  5.462004  ... -4.601150 -10.072189  5.858149   \n",
      "1  3.320807  11.627260  6.565130  ... -5.309328 -10.643972  4.269458   \n",
      "2  3.746728  12.211854  7.602871  ... -5.970655 -11.330150  2.269126   \n",
      "3  4.108368  12.665638  8.477080  ... -6.628905 -12.088001  0.015087   \n",
      "4  4.421709  12.995026  9.112953  ... -7.331279 -12.876470 -2.320403   \n",
      "\n",
      "        FT7       FT8        TP7       TP8       PO7       PO8         Oz  \n",
      "0 -6.076038 -3.457559 -10.564211 -3.465435 -7.452340  5.723619 -10.401716  \n",
      "1 -6.793052 -2.421540 -10.742051 -3.052358 -7.947506  4.051073 -10.757459  \n",
      "2 -7.381371 -1.549136 -10.951647 -2.573051 -8.422673  1.975578 -11.568488  \n",
      "3 -7.783466 -0.885222 -11.181192 -2.053070 -8.959252 -0.346699 -12.895379  \n",
      "4 -7.966721 -0.441055 -11.417038 -1.496988 -9.632474 -2.744580 -14.742146  \n",
      "\n",
      "[5 rows x 66 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing the Parquet files\n",
    "parquet_directory = '/home/owner/Documents/DEV/BrainLabyrinth/data/final_dataset copy'\n",
    "\n",
    "# List all Parquet files in the directory\n",
    "parquet_files = [os.path.join(parquet_directory, f) for f in os.listdir(parquet_directory) if f.endswith('.parquet')]\n",
    "\n",
    "# Initialize an empty list to hold the Dask DataFrames\n",
    "dask_dfs = []\n",
    "\n",
    "# Process each Parquet file\n",
    "for file in parquet_files:\n",
    "    # Read the Parquet file into a Dask DataFrame\n",
    "    df = dd.read_parquet(file)\n",
    "\n",
    "    # Extract the file name without the extension\n",
    "    file_name = os.path.basename(file).replace('.parquet', '')\n",
    "\n",
    "    # Create a unique event_id by combining the original event_id with the file name\n",
    "    df['event_id'] = df['event_id'].astype(str) + '_' + file_name\n",
    "\n",
    "    # Append the DataFrame to the list\n",
    "    dask_dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single Dask DataFrame\n",
    "combined_df = dd.concat(dask_dfs, ignore_index=True)\n",
    "\n",
    "# Create a mapping dictionary for unique event_id values\n",
    "unique_event_ids = combined_df['event_id'].unique().compute()\n",
    "event_id_mapping = {event_id: idx for idx, event_id in enumerate(unique_event_ids)}\n",
    "\n",
    "# Renumber the event_id column using the mapping dictionary\n",
    "combined_df['event_id'] = combined_df['event_id'].map(event_id_mapping, meta=('event_id', 'int64'))\n",
    "\n",
    "# Optionally, you can perform operations on the combined Dask DataFrame\n",
    "# For example, you can compute the first few rows to verify the data\n",
    "print(combined_df.head())\n",
    "\n",
    "# Save the combined Dask DataFrame to a new Parquet file\n",
    "combined_df.to_parquet('/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the final dataset: (5828008, 66)\n",
      "   event_id      marker   time        Fp1        Fpz       Fp2        F7  \\\n",
      "0         0  Stimulus/1  9.438  18.260990  16.979025  1.769803  2.441713   \n",
      "1         0  Stimulus/1  9.440  18.279552  17.167071  1.736013  2.509046   \n",
      "2         0  Stimulus/1  9.442  18.187657  17.195618  1.650717  2.619466   \n",
      "3         0  Stimulus/1  9.444  17.988738  17.064795  1.536693  2.780392   \n",
      "4         0  Stimulus/1  9.446  17.697904  16.795464  1.420282  2.984275   \n",
      "\n",
      "         F3         Fz        F4  ...       PO3        PO4       PO6  \\\n",
      "0  2.818692  10.922015  5.462004  ... -4.601150 -10.072189  5.858149   \n",
      "1  3.320807  11.627260  6.565130  ... -5.309328 -10.643972  4.269458   \n",
      "2  3.746728  12.211854  7.602871  ... -5.970655 -11.330150  2.269126   \n",
      "3  4.108368  12.665638  8.477080  ... -6.628905 -12.088001  0.015087   \n",
      "4  4.421709  12.995026  9.112953  ... -7.331279 -12.876470 -2.320403   \n",
      "\n",
      "        FT7       FT8        TP7       TP8       PO7       PO8         Oz  \n",
      "0 -6.076038 -3.457559 -10.564211 -3.465435 -7.452340  5.723619 -10.401716  \n",
      "1 -6.793052 -2.421540 -10.742051 -3.052358 -7.947506  4.051073 -10.757459  \n",
      "2 -7.381371 -1.549136 -10.951647 -2.573051 -8.422673  1.975578 -11.568488  \n",
      "3 -7.783466 -0.885222 -11.181192 -2.053070 -8.959252 -0.346699 -12.895379  \n",
      "4 -7.966721 -0.441055 -11.417038 -1.496988 -9.632474 -2.744580 -14.742146  \n",
      "\n",
      "[5 rows x 66 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['event_id', 'marker', 'time', 'Fp1', 'Fpz', 'Fp2', 'F7', 'F3', 'Fz',\n",
       "       'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6', 'M1', 'T7', 'C3', 'Cz', 'C4',\n",
       "       'T8', 'M2', 'CP5', 'CP1', 'CP2', 'CP6', 'P7', 'P3', 'Pz', 'P4', 'P8',\n",
       "       'POz', 'O1', 'O2', 'AF7', 'AF3', 'AF4', 'AF8', 'F5', 'F1', 'F2', 'F6',\n",
       "       'FC3', 'FCz', 'FC4', 'C5', 'C1', 'C2', 'C6', 'CP3', 'CP4', 'P5', 'P1',\n",
       "       'P2', 'P6', 'PO5', 'PO3', 'PO4', 'PO6', 'FT7', 'FT8', 'TP7', 'TP8',\n",
       "       'PO7', 'PO8', 'Oz'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Define the directory containing the Parquet files\n",
    "parquet_directory = '/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet'\n",
    "\n",
    "# Read the Parquet files into a Dask DataFrame\n",
    "dask_df = dd.read_parquet(parquet_directory)\n",
    "\n",
    "# Compute the shape of the Dask DataFrame\n",
    "rows, cols = dask_df.shape\n",
    "rows_computed = rows.compute()\n",
    "\n",
    "# Print the shape\n",
    "print(f\"Shape of the final dataset: ({rows_computed}, {cols})\")\n",
    "\n",
    "# Optionally, you can compute the first few rows to verify the data\n",
    "print(dask_df.head())\n",
    "dask_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# ok = os.listdir('/home/owner/Documents/DEV/BrainLabyrinth/data/final_dataset (copy)')\n",
    "\n",
    "# cleaned_dir = '/home/owner/Documents/DEV/BrainLabyrinth/data/final_dataset copy'\n",
    "# for file in os.listdir(cleaned_dir):\n",
    "#     if file not in ok:\n",
    "#         os.remove(os.path.join(cleaned_dir, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
