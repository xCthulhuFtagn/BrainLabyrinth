{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from numpy.lib.stride_tricks import sliding_window_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import polars as pl\n",
    "# import numpy as np\n",
    "# from scipy import stats\n",
    "# from scipy.signal import welch\n",
    "# from tqdm import tqdm\n",
    "# import os\n",
    "# from numpy.lib.stride_tricks import sliding_window_view\n",
    "# import math\n",
    "# import gc\n",
    "# import joblib\n",
    "# # ... other imports ...\n",
    "\n",
    "# # --- Feature Extraction with Larger Overlapping Windows ---\n",
    "# def extract_features_large_windows(group: pl.DataFrame) -> pl.DataFrame:\n",
    "#     # Get constant features\n",
    "#     event_id = group['event_id'][0]\n",
    "#     prev_marker = group['prev_marker'][0]\n",
    "#     marker = group['marker'][0] # Target\n",
    "\n",
    "#     eeg_cols = [col for col in group.columns if col not in\n",
    "#                 ['event_id', 'time', 'marker', 'prev_marker', 'orig_marker']]\n",
    "\n",
    "#     # --- Configuration for Large Windows ---\n",
    "#     fs = 500 # Sampling Frequency\n",
    "#     target_len = 2000 # Samples per epoch (4 seconds)\n",
    "#     window_size = 500 # Samples (2 seconds) <-- ADJUSTABLE\n",
    "#     step_size = 250    # Samples (0.5 seconds for 75% overlap) <-- ADJUSTABLE\n",
    "#     # Calculate expected number of windows\n",
    "#     num_windows = math.floor((target_len - window_size) / step_size) + 1 if target_len >= window_size else 0\n",
    "#     # --- End Configuration ---\n",
    "\n",
    "#     features_dict = {\n",
    "#         'event_id': event_id,\n",
    "#         'prev_marker': prev_marker,\n",
    "#         'marker': marker\n",
    "#     }\n",
    "\n",
    "#     bands = { 'delta': (1, 4), 'theta': (4, 8), 'alpha': (8, 13), 'beta': (13, 30) }\n",
    "#     nperseg_welch = min(500, window_size) # Welch window, e.g., 1 sec or window_size if smaller\n",
    "#     noverlap_welch = nperseg_welch // 2\n",
    "\n",
    "#     # Process each EEG channel\n",
    "#     for col in eeg_cols:\n",
    "#         signal = group[col].cast(pl.Float64).to_numpy()\n",
    "#         signal = np.nan_to_num(signal, nan=0.0)\n",
    "#         if len(signal) < target_len:\n",
    "#             signal = np.pad(signal, (0, target_len - len(signal)), constant_values=0.0)\n",
    "#         elif len(signal) > target_len:\n",
    "#             signal = signal[:target_len]\n",
    "\n",
    "#         if len(signal) < window_size: # If epoch too short even for one window\n",
    "#              for window_idx in range(num_windows): # Add NaNs for all expected features/windows\n",
    "#                  # Time domain NaNs\n",
    "#                  features_dict[f\"{col}_mean_w{window_idx}\"] = np.nan\n",
    "#                  features_dict[f\"{col}_std_w{window_idx}\"] = np.nan\n",
    "#                  # ... add NaNs for min, max, skew, kurtosis, mobility, complexity ...\n",
    "#                  # Freq domain NaNs\n",
    "#                  for band in bands: features_dict[f\"{col}_{band}_power_w{window_idx}\"] = np.nan\n",
    "#              continue\n",
    "\n",
    "#         # Create sliding windows view\n",
    "#         windows = sliding_window_view(signal, window_shape=window_size)[::step_size]\n",
    "\n",
    "#         # Calculate features for each window\n",
    "#         for window_idx, window_signal in enumerate(windows):\n",
    "#             # Ensure we don't process more windows than expected (safety)\n",
    "#             if window_idx >= num_windows: break\n",
    "\n",
    "#             with np.errstate(divide='ignore', invalid='ignore'):\n",
    "#                 # Basic Stats\n",
    "#                 features_dict[f\"{col}_mean_w{window_idx}\"] = np.mean(window_signal)\n",
    "#                 features_dict[f\"{col}_std_w{window_idx}\"] = np.std(window_signal, ddof=1)\n",
    "#                 features_dict[f\"{col}_min_w{window_idx}\"] = np.min(window_signal)\n",
    "#                 features_dict[f\"{col}_max_w{window_idx}\"] = np.max(window_signal)\n",
    "#                 features_dict[f\"{col}_skew_w{window_idx}\"] = stats.skew(window_signal, bias=False)\n",
    "#                 features_dict[f\"{col}_kurtosis_w{window_idx}\"] = stats.kurtosis(window_signal, bias=False)\n",
    "\n",
    "#                 # Hjorth Parameters\n",
    "#                 diff1 = np.diff(window_signal)\n",
    "#                 diff2 = np.diff(diff1)\n",
    "#                 var_signal = np.var(window_signal, ddof=1)\n",
    "#                 var_diff1 = np.var(diff1, ddof=1)\n",
    "#                 var_diff2 = np.var(diff2, ddof=1)\n",
    "#                 mobility = np.sqrt(var_diff1 / var_signal) if var_signal > 1e-9 else 0.0\n",
    "#                 complexity = np.sqrt(var_diff2 / var_diff1) / mobility if mobility > 1e-9 and var_diff1 > 1e-9 else 0.0\n",
    "#                 features_dict[f\"{col}_mobility_w{window_idx}\"] = mobility\n",
    "#                 features_dict[f\"{col}_complexity_w{window_idx}\"] = complexity\n",
    "\n",
    "#                 # Band Power\n",
    "#                 try:\n",
    "#                     freqs, psd = welch(window_signal, fs=fs, nperseg=nperseg_welch, noverlap=noverlap_welch, scaling='density', average='mean')\n",
    "#                     for band, (low_hz, high_hz) in bands.items():\n",
    "#                         idx_band = np.logical_and(freqs >= low_hz, freqs < high_hz)\n",
    "#                         band_power = np.mean(psd[idx_band]) if np.any(idx_band) else 0.0\n",
    "#                         features_dict[f\"{col}_{band}_power_w{window_idx}\"] = band_power\n",
    "#                 except ValueError as e:\n",
    "#                     print(f\"Warning: Welch failed for {col} w{window_idx}, event {event_id}: {e}\")\n",
    "#                     for band in bands: features_dict[f\"{col}_{band}_power_w{window_idx}\"] = np.nan\n",
    "\n",
    "#     return pl.DataFrame([features_dict])\n",
    "\n",
    "\n",
    "# # --- Function to Create Dataset (calls extract_features_1000s) ---\n",
    "# def create_ml_dataset_large_windows(df_path: str, output_path: str = \"ML_dataset_large_window_features.parquet\", batch_size: int = 100):\n",
    "#     # This function remains structurally the same as create_ml_dataset_epoch_features,\n",
    "#     # just make sure it calls extract_features_large_windows instead.\n",
    "#     # Remember to use a different temp directory name like 'temp_large_window_files'\n",
    "\n",
    "#     # Initialize\n",
    "#     temp_counter = 0\n",
    "#     temp_files = []\n",
    "#     processed_batch = []\n",
    "#     temp_dir = \"temp_large_window_files\" # Different temp dir\n",
    "#     os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "#     lf = pl.scan_parquet(df_path).with_columns(\n",
    "#         pl.col(['prev_marker', 'marker']).cast(pl.Utf8),\n",
    "#         pl.all().exclude(['event_id', 'time', 'marker', 'prev_marker', 'orig_marker']).cast(pl.Float64)\n",
    "#     )\n",
    "\n",
    "#     print(\"Fetching unique event IDs...\")\n",
    "#     event_ids = lf.select('event_id').unique(maintain_order=True).collect()['event_id'].to_list()\n",
    "#     total_events = len(event_ids)\n",
    "#     print(f\"Found {total_events} unique events.\")\n",
    "\n",
    "#     with tqdm(total=total_events, desc=\"Processing events (large windows)\") as pbar:\n",
    "#         for event_id in event_ids:\n",
    "#             try:\n",
    "#                 event_group_lf = lf.filter(pl.col('event_id') == event_id)\n",
    "#                 event_group_df = event_group_lf.sort('time').collect()\n",
    "\n",
    "#                 if event_group_df.is_empty():\n",
    "#                      pbar.update(1); continue\n",
    "#                 # Check length, although padding/truncation is handled inside\n",
    "#                 # if event_group_df.height != 2000:\n",
    "#                 #      print(f\"Warning: Event {event_id} has {event_group_df.height} samples\")\n",
    "\n",
    "#                 # *** Call the large window feature extractor ***\n",
    "#                 processed_df = extract_features_large_windows(event_group_df)\n",
    "#                 processed_batch.append(processed_df)\n",
    "\n",
    "#                 if len(processed_batch) >= batch_size:\n",
    "#                     temp_file = os.path.join(temp_dir, f\"temp_{temp_counter}.parquet\")\n",
    "#                     pl.concat(processed_batch).write_parquet(temp_file, compression=\"zstd\")\n",
    "#                     temp_files.append(temp_file)\n",
    "#                     processed_batch = []\n",
    "#                     temp_counter += 1\n",
    "#                     gc.collect()\n",
    "\n",
    "#                 pbar.update(1)\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing event_id {event_id}: {str(e)}\")\n",
    "#                 import traceback\n",
    "#                 traceback.print_exc()\n",
    "#                 pbar.update(1)\n",
    "#                 continue\n",
    "\n",
    "#     # Write remaining batch\n",
    "#     if processed_batch:\n",
    "#         temp_file = os.path.join(temp_dir, f\"temp_{temp_counter}.parquet\")\n",
    "#         pl.concat(processed_batch).write_parquet(temp_file, compression=\"zstd\")\n",
    "#         temp_files.append(temp_file)\n",
    "#         print(f\"Wrote final batch of {len(processed_batch)} events.\")\n",
    "\n",
    "#     # Combine temporary files\n",
    "#     print(f\"Combining {len(temp_files)} temporary batch files...\")\n",
    "#     if temp_files:\n",
    "#         # ... (Combination and cleanup logic is identical to previous function) ...\n",
    "#         try:\n",
    "#             lazy_frames = [pl.scan_parquet(f) for f in temp_files]\n",
    "#             pl.concat(lazy_frames, rechunk=False).sink_parquet(\n",
    "#                 output_path, compression=\"zstd\", statistics=True\n",
    "#             )\n",
    "#             print(f\"Successfully created final dataset: {output_path}\")\n",
    "#         except Exception as e:\n",
    "#              print(f\"ERROR: Failed during final concatenation/writing: {e}\")\n",
    "#              print(f\"Temporary files are kept for inspection in '{temp_dir}'.\")\n",
    "#              return None\n",
    "#         finally:\n",
    "#              if os.path.exists(output_path):\n",
    "#                  print(\"Cleaning up temporary files...\")\n",
    "#                  for f in temp_files:\n",
    "#                      try: os.remove(f)\n",
    "#                      except OSError as e: print(f\"Warning: Could not remove temp file {f}: {e}\")\n",
    "#                  try: os.rmdir(temp_dir); print(f\"Removed temporary directory: {temp_dir}\")\n",
    "#                  except OSError: print(f\"Temporary directory {temp_dir} not empty, not removed.\")\n",
    "#                  print(\"Cleanup complete.\")\n",
    "#              else:\n",
    "#                  print(f\"Final file not created. Keeping temporary files in '{temp_dir}'.\")\n",
    "#         return output_path\n",
    "#     else:\n",
    "#         print(\"No temporary files were generated. No output created.\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # --- Usage Example ---\n",
    "# INPUT_FILE = \"/home/owner/Documents/DEV/BrainLabyrinth/data/combined.parquet\"\n",
    "# OUTPUT_FILE_LARGE_WINDOW = \"ML_dataset_500_features.parquet\" # New name\n",
    "# BATCH_SIZE = 200\n",
    "\n",
    "# final_large_window_file_path = create_ml_dataset_large_windows(INPUT_FILE, OUTPUT_FILE_LARGE_WINDOW, BATCH_SIZE)\n",
    "\n",
    "# if final_large_window_file_path:\n",
    "#     print(f\"\\nLarge window feature dataset creation complete. Output file: {final_large_window_file_path}\")\n",
    "#     # Verify schema or head\n",
    "#     print(\"\\nSchema of the large window feature dataset:\")\n",
    "#     final_lf = pl.scan_parquet(final_large_window_file_path)\n",
    "#     print(final_lf.schema)\n",
    "#     # Estimate feature count more accurately now\n",
    "#     num_features_estimate = len(final_lf.columns) - 3 # Subtract event_id, marker, prev_marker\n",
    "#     print(f\"\\nEstimated number of feature columns: {num_features_estimate}\")\n",
    "\n",
    "# else:\n",
    "#     print(\"\\nLarge window feature dataset creation failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import polars as pl\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# import os\n",
    "# import math\n",
    "# import gc\n",
    "# import joblib\n",
    "# from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "# from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif\n",
    "# import traceback # Import traceback for detailed error printing\n",
    "\n",
    "# # --- Configuration ---\n",
    "# # *** IMPORTANT: Update INPUT_FILE to the correct large window features file ***\n",
    "# INPUT_FILE = \"ML_dataset_500_features.parquet\" # Use the file generated by the previous step\n",
    "# OUTPUT_SCALER_MODEL = \"fs_scaler_500.joblib\" # Use distinct names\n",
    "# OUTPUT_FINAL_FEATURE_LIST = \"final_selected_features_500.joblib\" # Use distinct names\n",
    "\n",
    "# # Feature Selection Parameters\n",
    "# VARIANCE_THRESHOLD = 0.01\n",
    "# MI_SELECT_K = 500\n",
    "# MI_SAMPLE_SIZE = 20000 # Keep or adjust based on memory\n",
    "# BATCH_SIZE = 1000 # For scaler fitting\n",
    "\n",
    "# # --- Identify Feature and Target Columns ---\n",
    "# print(\"Reading schema to identify columns...\")\n",
    "# try:\n",
    "#     # Make sure INPUT_FILE exists\n",
    "#     if not os.path.exists(INPUT_FILE):\n",
    "#         raise FileNotFoundError(f\"Input file not found: {INPUT_FILE}\")\n",
    "#     schema = pl.read_parquet(INPUT_FILE, n_rows=0).schema\n",
    "# except Exception as e:\n",
    "#     print(f\"Error reading schema from {INPUT_FILE}: {e}\")\n",
    "#     print(\"Please ensure the file exists and is a valid Parquet file.\")\n",
    "#     exit(1) # Use non-zero exit code for errors\n",
    "\n",
    "# IDENTIFIER_COLS = ['event_id', 'prev_marker']\n",
    "# TARGET_COLUMN = 'marker'\n",
    "# # Calculate initial feature columns\n",
    "# FEATURE_COLUMNS = [col for col in schema if col not in IDENTIFIER_COLS + [TARGET_COLUMN]]\n",
    "# n_original = len(FEATURE_COLUMNS) # Store original count here\n",
    "# print(f\"Found {n_original} initial feature columns.\")\n",
    "# print(f\"Target column: {TARGET_COLUMN}\")\n",
    "\n",
    "# if not FEATURE_COLUMNS:\n",
    "#     raise ValueError(\"No feature columns identified. Check IDENTIFIER_COLS and TARGET_COLUMN.\")\n",
    "# if TARGET_COLUMN not in schema:\n",
    "#      raise ValueError(f\"Target column '{TARGET_COLUMN}' not found in the dataset.\")\n",
    "\n",
    "# # --- Step 1: Low Variance Threshold Preparation ---\n",
    "\n",
    "# # 1a. Fit StandardScaler Incrementally on ALL original features\n",
    "# print(\"\\n--- Step 1: Low Variance Threshold ---\")\n",
    "# scaler = StandardScaler()\n",
    "# print(f\"Fitting StandardScaler incrementally (batch size: {BATCH_SIZE})...\")\n",
    "# selected_mask_variance = None # Initialize mask variable\n",
    "\n",
    "# try:\n",
    "#     total_rows = pl.scan_parquet(INPUT_FILE).select(pl.len()).collect().item()\n",
    "#     n_batches = math.ceil(total_rows / BATCH_SIZE)\n",
    "#     row_iterator = pl.read_parquet(INPUT_FILE, columns=FEATURE_COLUMNS).iter_slices(n_rows=BATCH_SIZE)\n",
    "\n",
    "#     for i, data_chunk_pl in enumerate(tqdm(row_iterator, total=n_batches, desc=\"Fitting Scaler\")):\n",
    "#         if data_chunk_pl.height > 0:\n",
    "#             features_np = data_chunk_pl.to_numpy()\n",
    "#             if features_np.size > 0:\n",
    "#                 # Check for NaNs/Infs which cause issues in partial_fit\n",
    "#                 if np.any(~np.isfinite(features_np)):\n",
    "#                      print(f\"\\nWarning: Non-finite values found in chunk {i}. Replacing with 0 before scaling.\")\n",
    "#                      features_np = np.nan_to_num(features_np, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "#                 scaler.partial_fit(features_np)\n",
    "#             del features_np\n",
    "#         del data_chunk_pl\n",
    "#         gc.collect()\n",
    "\n",
    "#     print(\"StandardScaler fitting complete.\")\n",
    "#     joblib.dump(scaler, OUTPUT_SCALER_MODEL)\n",
    "#     print(f\"Saved fitted scaler to {OUTPUT_SCALER_MODEL}\")\n",
    "\n",
    "#     # 1b. Determine Variance Threshold Mask (based on the fitted scaler)\n",
    "#     if not hasattr(scaler, 'var_') or scaler.var_ is None:\n",
    "#         raise ValueError(\"Scaler variance (var_) attribute not found or is None. Fitting likely failed.\")\n",
    "\n",
    "#     variances = scaler.var_\n",
    "#     selector_variance = VarianceThreshold(threshold=VARIANCE_THRESHOLD)\n",
    "#     selected_mask_variance = variances > VARIANCE_THRESHOLD # This mask corresponds to ORIGINAL features\n",
    "#     features_after_variance = [\n",
    "#         feature for feature, selected in zip(FEATURE_COLUMNS, selected_mask_variance) if selected\n",
    "#     ]\n",
    "#     n_after_variance = len(features_after_variance)\n",
    "\n",
    "#     print(f\"Applied Variance Threshold > {VARIANCE_THRESHOLD}\")\n",
    "#     print(f\"Features remaining after variance check: {n_after_variance} (removed {n_original - n_after_variance})\")\n",
    "\n",
    "#     if not features_after_variance:\n",
    "#         print(\"Error: No features remaining after variance thresholding. Check threshold or data.\")\n",
    "#         exit(1)\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during StandardScaler fitting or Variance Threshold prep: {e}\")\n",
    "#     traceback.print_exc()\n",
    "#     exit(1)\n",
    "\n",
    "\n",
    "# # --- Step 2: Univariate Selection (Mutual Information) ---\n",
    "# print(\"\\n--- Step 2: Mutual Information Selection ---\")\n",
    "# features_after_mi = [] # Initialize final list\n",
    "# n_after_mi = 0\n",
    "# k_features = 0 # Initialize k_features\n",
    "\n",
    "# # 2a. Sample Data (including ALL original features needed for scaling)\n",
    "# print(f\"Sampling data ({MI_SAMPLE_SIZE} rows) for MI calculation...\")\n",
    "# # We need all original features for scaling, plus the target\n",
    "# cols_to_sample = FEATURE_COLUMNS + [TARGET_COLUMN]\n",
    "\n",
    "# try:\n",
    "#     if total_rows <= MI_SAMPLE_SIZE:\n",
    "#          print(\"Total rows <= sample size, using all data for MI.\")\n",
    "#          sample_df = pl.read_parquet(INPUT_FILE, columns=cols_to_sample)\n",
    "#     else:\n",
    "#         sample_df = pl.scan_parquet(INPUT_FILE, columns=cols_to_sample)\\\n",
    "#                        .sample(n=MI_SAMPLE_SIZE, shuffle=True, seed=42)\\\n",
    "#                        .collect()\n",
    "\n",
    "#     print(f\"Sampled {sample_df.height} rows.\")\n",
    "#     if sample_df.is_empty(): raise ValueError(\"Sampled DataFrame is empty.\")\n",
    "\n",
    "#     # Separate features and target\n",
    "#     X_sample_full = sample_df.select(FEATURE_COLUMNS).to_numpy() # Has n_original columns\n",
    "#     y_sample_raw = sample_df.select(TARGET_COLUMN).to_numpy().ravel()\n",
    "#     del sample_df\n",
    "#     gc.collect()\n",
    "\n",
    "#     # Encode target variable\n",
    "#     le = LabelEncoder()\n",
    "#     y_sample = le.fit_transform(y_sample_raw)\n",
    "#     print(f\"Encoded target variable. Found classes: {le.classes_}\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during data sampling: {e}\")\n",
    "#     traceback.print_exc()\n",
    "#     exit(1)\n",
    "\n",
    "# # 2b. Scale the FULL Sample Data\n",
    "# print(\"Scaling the sample data (using all original features)...\")\n",
    "# try:\n",
    "#     # Check for NaNs/Infs before transform\n",
    "#     if np.any(~np.isfinite(X_sample_full)):\n",
    "#          print(\"Warning: Non-finite values found in sample features. Replacing with 0 before transform.\")\n",
    "#          X_sample_full = np.nan_to_num(X_sample_full, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "#     X_sample_scaled_full = scaler.transform(X_sample_full) # Transform uses the fitted scaler\n",
    "#     del X_sample_full # Free memory\n",
    "#     gc.collect()\n",
    "\n",
    "#     # 2c. Apply Variance Threshold filtering to the SCALED sample data\n",
    "#     print(\"Applying variance threshold filter to scaled sample data...\")\n",
    "#     if selected_mask_variance is None:\n",
    "#          raise ValueError(\"Variance threshold mask was not calculated.\")\n",
    "#     # Use the mask calculated in step 1b to select columns from the scaled data\n",
    "#     X_sample_scaled_variance_filtered = X_sample_scaled_full[:, selected_mask_variance]\n",
    "#     del X_sample_scaled_full # Free memory\n",
    "#     gc.collect()\n",
    "#     print(f\"Shape of data for MI: {X_sample_scaled_variance_filtered.shape}\") # Should have n_after_variance columns\n",
    "\n",
    "# except Exception as e:\n",
    "#      print(f\"Error scaling sample data or applying variance filter: {e}\")\n",
    "#      traceback.print_exc()\n",
    "#      exit(1)\n",
    "\n",
    "\n",
    "# # 2d. Calculate Mutual Information Scores and Select K Best\n",
    "# print(f\"Calculating Mutual Information scores and selecting top {MI_SELECT_K} features...\")\n",
    "# try:\n",
    "#     # Check if input data for MI exists and has expected shape\n",
    "#     if 'X_sample_scaled_variance_filtered' not in locals() or X_sample_scaled_variance_filtered.shape[1] != n_after_variance:\n",
    "#          raise ValueError(\"Data for MI calculation is missing or has incorrect shape.\")\n",
    "\n",
    "#     # Ensure k is not larger than the number of features available AFTER variance thresholding\n",
    "#     num_features_for_mi = X_sample_scaled_variance_filtered.shape[1]\n",
    "#     k_features = min(MI_SELECT_K, num_features_for_mi) # Define k_features here\n",
    "#     if k_features < MI_SELECT_K:\n",
    "#         print(f\"Warning: Requested K={MI_SELECT_K}, but only {k_features} available after variance threshold. Selecting all {k_features}.\")\n",
    "#     if k_features == 0:\n",
    "#         raise ValueError(\"k_features is 0, cannot select features.\")\n",
    "\n",
    "#     # Use random_state for reproducibility if MI uses it\n",
    "#     selector_mi = SelectKBest(lambda X, y: mutual_info_classif(X, y, discrete_features=False, random_state=42), k=k_features)\n",
    "#     selector_mi.fit(X_sample_scaled_variance_filtered, y_sample)\n",
    "\n",
    "#     # Get the mask relative to the variance-filtered features\n",
    "#     selected_mask_mi = selector_mi.get_support()\n",
    "\n",
    "#     # Map this mask back to the original feature names that passed the variance threshold\n",
    "#     features_after_mi = [\n",
    "#         feature for feature, selected in zip(features_after_variance, selected_mask_mi) if selected\n",
    "#     ]\n",
    "#     n_after_mi = len(features_after_mi)\n",
    "\n",
    "#     print(\"Mutual Information selection complete.\")\n",
    "#     print(f\"Final features selected: {n_after_mi} (removed {n_after_variance - n_after_mi} based on MI)\")\n",
    "\n",
    "#     if n_after_mi == 0 :\n",
    "#         print(\"Warning: Mutual information selected 0 features.\")\n",
    "\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during Mutual Information calculation/selection: {e}\")\n",
    "#     traceback.print_exc()\n",
    "#     # No exit here, allow summary to print if possible, but feature list might be empty\n",
    "\n",
    "\n",
    "# # --- Final Results ---\n",
    "# print(\"\\n--- Feature Selection Summary ---\")\n",
    "# print(f\"Initial features: {n_original}\")\n",
    "# print(f\"After Variance Threshold (> {VARIANCE_THRESHOLD}): {n_after_variance}\")\n",
    "# # Check if MI step completed successfully enough to have n_after_mi and k_features\n",
    "# if 'n_after_mi' in locals() and 'k_features' in locals():\n",
    "#     print(f\"After Mutual Information (Top {k_features}): {n_after_mi}\")\n",
    "# else:\n",
    "#     print(\"Mutual Information step did not complete successfully.\")\n",
    "\n",
    "# # Save the final list of selected features\n",
    "# if features_after_mi: # Only save if the list is not empty\n",
    "#     try:\n",
    "#         joblib.dump(features_after_mi, OUTPUT_FINAL_FEATURE_LIST)\n",
    "#         print(f\"\\nSaved the final list of {n_after_mi} selected features to: {OUTPUT_FINAL_FEATURE_LIST}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error saving the final feature list: {e}\")\n",
    "# else:\n",
    "#     print(\"\\nNo features selected by Mutual Information, final list not saved.\")\n",
    "\n",
    "# print(\"\\nFeature selection process finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading schema to identify columns...\n",
      "Found 5292 initial feature columns.\n",
      "Target column: marker\n",
      "\n",
      "Determining all unique target classes...\n",
      "Total rows in dataset: 2772\n",
      "Target encoder fitted. All classes: ['Left' 'Right']\n",
      "\n",
      "--- Step 1: Incremental Fit Scaler & L1 Model (Alpha=0.08) ---\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 L1 Fit: 100%|██████████| 3/3 [00:00<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Incremental Scaler and L1 Model fitting finished.\n",
      "\n",
      "--- Step 2: Identifying features selected by L1 ---\n",
      "\n",
      "Selected 109 features out of 5292 using L1 regularization (alpha=0.08).\n",
      "\n",
      "--- Step 3: Mutual Information Selection on L1 Features ---\n",
      "Sampling data (20000 rows) for MI calculation...\n",
      "Total rows <= sample size, using all data for MI.\n",
      "Sampled 2772 rows.\n",
      "Scaling the sample data (using scaler fitted on all features)...\n",
      "Applying L1 feature filter to scaled sample data...\n",
      "Shape of data for MI: (2772, 109)\n",
      "Calculating Mutual Information scores and selecting top 50 features...\n",
      "Mutual Information selection complete.\n",
      "Final features selected: 50 (removed 59 based on MI)\n",
      "\n",
      "--- Feature Selection Summary ---\n",
      "Initial features: 5292\n",
      "After L1 Regularization (alpha=0.08): 109\n",
      "After Mutual Information (Top 50): 50\n",
      "\n",
      "--- Saving final outputs ---\n",
      "Saved scaler fitted on original features to: fs_scaler_model.joblib\n",
      "Saved the final list of 50 selected features to: final_selected_features.joblib\n",
      "\n",
      "--- L1 -> MI Feature Selection Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "import gc\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "import traceback\n",
    "\n",
    "# Suppress convergence warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='sklearn')\n",
    "\n",
    "# --- Configuration ---\n",
    "# *** Select the INPUT file based on the window size you want to use ***\n",
    "INPUT_FEATURES_FILE = \"ML_dataset_500_features.parquet\"\n",
    "\n",
    "# --- FIXED Output filenames ---\n",
    "OUTPUT_SCALER_MODEL = \"fs_scaler_model.joblib\"\n",
    "OUTPUT_FINAL_FEATURE_LIST = \"final_selected_features.joblib\"\n",
    "# --- End FIXED Output filenames ---\n",
    "\n",
    "TARGET_COLUMN = \"marker\"\n",
    "IDENTIFIER_COLS = ['event_id', 'prev_marker']\n",
    "\n",
    "# Parameters for Selection Pipeline\n",
    "BATCH_SIZE = 1000\n",
    "N_TRAINING_EPOCHS_L1 = 1\n",
    "\n",
    "# L1 Parameters\n",
    "# ** CRUCIAL HYPERPARAMETER TO TUNE **\n",
    "# Increase this significantly based on previous run (e.g., 0.005 or 0.01 might be better starting points now)\n",
    "L1_ALPHA = 0.08 # Keep user's value for now, but add note to increase\n",
    "# print(f\"*** NOTE: L1_ALPHA is set to {L1_ALPHA}. Consider increasing significantly based on previous results (e.g., 0.005, 0.01) to select fewer features before MI. ***\")\n",
    "\n",
    "# MI Parameters\n",
    "MI_SELECT_K = 50\n",
    "MI_SAMPLE_SIZE = 20000\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- Identify Feature and Target Columns ---\n",
    "print(\"Reading schema to identify columns...\")\n",
    "try:\n",
    "    if not os.path.exists(INPUT_FEATURES_FILE):\n",
    "        raise FileNotFoundError(f\"Input file not found: {INPUT_FEATURES_FILE}\")\n",
    "    schema = pl.read_parquet(INPUT_FEATURES_FILE, n_rows=0).schema\n",
    "except Exception as e:\n",
    "    print(f\"Error reading schema from {INPUT_FEATURES_FILE}: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "FEATURE_COLUMNS = [col for col in schema if col not in IDENTIFIER_COLS + [TARGET_COLUMN]]\n",
    "n_original = len(FEATURE_COLUMNS)\n",
    "print(f\"Found {n_original} initial feature columns.\")\n",
    "print(f\"Target column: {TARGET_COLUMN}\")\n",
    "\n",
    "if not FEATURE_COLUMNS:\n",
    "    raise ValueError(\"No feature columns identified.\")\n",
    "if TARGET_COLUMN not in schema:\n",
    "     raise ValueError(f\"Target column '{TARGET_COLUMN}' not found in the dataset.\")\n",
    "\n",
    "# --- Pre-determine all classes and fit LabelEncoder ---\n",
    "print(\"\\nDetermining all unique target classes...\")\n",
    "n_total_rows_global = 0 # Define outside try block\n",
    "try:\n",
    "    # Also get total rows here for later use\n",
    "    lf_scan = pl.scan_parquet(INPUT_FEATURES_FILE)\n",
    "    n_total_rows_global = lf_scan.select(pl.len()).collect().item() # Store total rows globally\n",
    "    print(f\"Total rows in dataset: {n_total_rows_global}\")\n",
    "\n",
    "    all_unique_targets = lf_scan.select(pl.col(TARGET_COLUMN)) \\\n",
    "                           .unique() \\\n",
    "                           .collect() \\\n",
    "                           .get_column(TARGET_COLUMN) \\\n",
    "                           .to_numpy()\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(all_unique_targets)\n",
    "    all_classes = label_encoder.classes_\n",
    "    print(f\"Target encoder fitted. All classes: {all_classes}\")\n",
    "    all_classes_encoded = label_encoder.transform(all_classes)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error determining unique target classes or total rows: {e}\")\n",
    "    traceback.print_exc()\n",
    "    exit(1)\n",
    "\n",
    "# --- Initialize Scaler and L1 Model ---\n",
    "scaler_full = StandardScaler()\n",
    "l1_model = SGDClassifier(\n",
    "    loss='log_loss',\n",
    "    penalty='l1',\n",
    "    alpha=L1_ALPHA,\n",
    "    max_iter=1000,\n",
    "    tol=1e-3,\n",
    "    shuffle=True,\n",
    "    random_state=RANDOM_STATE,\n",
    "    warm_start=(N_TRAINING_EPOCHS_L1 > 1),\n",
    "    learning_rate='optimal'\n",
    ")\n",
    "\n",
    "# --- Step 1: Fit Scaler and L1 Model Incrementally ---\n",
    "print(f\"\\n--- Step 1: Incremental Fit Scaler & L1 Model (Alpha={L1_ALPHA}) ---\")\n",
    "\n",
    "is_first_partial_fit = True\n",
    "\n",
    "for epoch in range(N_TRAINING_EPOCHS_L1):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{N_TRAINING_EPOCHS_L1}\")\n",
    "    try:\n",
    "        columns_to_read = FEATURE_COLUMNS + [TARGET_COLUMN]\n",
    "        row_iterator = pl.read_parquet(INPUT_FEATURES_FILE, columns=columns_to_read).iter_slices(n_rows=BATCH_SIZE)\n",
    "        # n_total_rows calculation moved outside the loop\n",
    "        n_batches = math.ceil(n_total_rows_global / BATCH_SIZE)\n",
    "\n",
    "        batch_pbar = tqdm(row_iterator, total=n_batches, desc=f\"Epoch {epoch + 1} L1 Fit\")\n",
    "        for data_chunk_pl in batch_pbar:\n",
    "            if data_chunk_pl.height == 0: continue\n",
    "\n",
    "            X_chunk = data_chunk_pl.select(FEATURE_COLUMNS).to_numpy()\n",
    "            y_chunk_raw = data_chunk_pl.select(TARGET_COLUMN).to_numpy().ravel()\n",
    "\n",
    "            if np.any(~np.isfinite(X_chunk)):\n",
    "                 batch_pbar.write(f\"Warning: Non-finite values found in batch. Replacing with 0.\")\n",
    "                 X_chunk = np.nan_to_num(X_chunk, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "            # Transform target using the pre-fitted encoder\n",
    "            y_chunk = label_encoder.transform(y_chunk_raw)\n",
    "\n",
    "            # Fit scaler_full incrementally\n",
    "            scaler_full.partial_fit(X_chunk)\n",
    "            # Scale chunk for L1 model fitting\n",
    "            X_chunk_scaled = scaler_full.transform(X_chunk)\n",
    "\n",
    "            # Fit L1 model incrementally\n",
    "            if is_first_partial_fit:\n",
    "                l1_model.partial_fit(X_chunk_scaled, y_chunk, classes=all_classes_encoded)\n",
    "                is_first_partial_fit = False\n",
    "            else:\n",
    "                l1_model.partial_fit(X_chunk_scaled, y_chunk)\n",
    "\n",
    "            del data_chunk_pl, X_chunk, y_chunk_raw, y_chunk, X_chunk_scaled\n",
    "            gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during incremental fitting Epoch {epoch + 1}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        exit(1)\n",
    "\n",
    "print(\"\\nIncremental Scaler and L1 Model fitting finished.\")\n",
    "\n",
    "# --- Step 2: Identify Features Selected by L1 ---\n",
    "print(\"\\n--- Step 2: Identifying features selected by L1 ---\")\n",
    "\n",
    "features_after_l1 = []\n",
    "n_after_l1 = 0\n",
    "selected_mask_l1 = None\n",
    "\n",
    "try:\n",
    "    if not hasattr(l1_model, 'coef_'):\n",
    "        raise ValueError(\"L1 model has not been fitted properly or has no 'coef_' attribute.\")\n",
    "\n",
    "    coefficients = l1_model.coef_\n",
    "    if coefficients.shape[0] == 1:\n",
    "        abs_coef = np.abs(coefficients[0])\n",
    "    else:\n",
    "        print(\"Multi-class coefficients found, summing absolute values.\")\n",
    "        abs_coef = np.sum(np.abs(coefficients), axis=0)\n",
    "\n",
    "    tolerance = 1e-5\n",
    "    selected_mask_l1 = abs_coef > tolerance\n",
    "    features_after_l1 = [\n",
    "        feature for feature, selected in zip(FEATURE_COLUMNS, selected_mask_l1) if selected\n",
    "    ]\n",
    "    n_after_l1 = len(features_after_l1)\n",
    "\n",
    "    print(f\"\\nSelected {n_after_l1} features out of {n_original} using L1 regularization (alpha={L1_ALPHA}).\")\n",
    "\n",
    "    if n_after_l1 == 0:\n",
    "        print(f\"Warning: L1 regularization selected 0 features. Alpha ({L1_ALPHA}) might be too high. Skipping MI step.\")\n",
    "    elif n_after_l1 == n_original:\n",
    "         print(f\"Warning: L1 regularization selected all {n_original} features. Alpha ({L1_ALPHA}) might be too low.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error identifying non-zero L1 coefficients: {e}\")\n",
    "    traceback.print_exc()\n",
    "    exit(1)\n",
    "\n",
    "# --- Step 3: Filter with Mutual Information (on Sample) ---\n",
    "print(\"\\n--- Step 3: Mutual Information Selection on L1 Features ---\")\n",
    "\n",
    "features_after_l1_mi = []\n",
    "n_after_mi = 0\n",
    "k_features_mi = 0\n",
    "X_sample_scaled_l1_filtered = None # Define outside try block for MI\n",
    "\n",
    "# Proceed only if L1 selected some features\n",
    "if n_after_l1 > 0:\n",
    "    # 3a. Sample Data\n",
    "    print(f\"Sampling data ({MI_SAMPLE_SIZE} rows) for MI calculation...\")\n",
    "    cols_to_sample = FEATURE_COLUMNS + [TARGET_COLUMN]\n",
    "    X_sample_full = None # Define outside try block\n",
    "    y_sample = None # Define outside try block\n",
    "\n",
    "    try:\n",
    "        # Use the globally calculated n_total_rows_global\n",
    "        if n_total_rows_global <= MI_SAMPLE_SIZE:\n",
    "             print(\"Total rows <= sample size, using all data for MI.\")\n",
    "             sample_df = pl.read_parquet(INPUT_FEATURES_FILE, columns=cols_to_sample)\n",
    "        else:\n",
    "            sample_df = pl.scan_parquet(INPUT_FEATURES_FILE, columns=cols_to_sample)\\\n",
    "                           .sample(n=MI_SAMPLE_SIZE, shuffle=True, seed=RANDOM_STATE)\\\n",
    "                           .collect()\n",
    "\n",
    "        print(f\"Sampled {sample_df.height} rows.\")\n",
    "        if sample_df.is_empty(): raise ValueError(\"Sampled DataFrame is empty.\")\n",
    "\n",
    "        X_sample_full = sample_df.select(FEATURE_COLUMNS).to_numpy()\n",
    "        y_sample_raw = sample_df.select(TARGET_COLUMN).to_numpy().ravel()\n",
    "        del sample_df\n",
    "        gc.collect()\n",
    "\n",
    "        y_sample = label_encoder.transform(y_sample_raw)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data sampling for MI: {e}\")\n",
    "        traceback.print_exc()\n",
    "        exit(1)\n",
    "\n",
    "    # 3b. Scale the FULL Sample Data\n",
    "    print(\"Scaling the sample data (using scaler fitted on all features)...\")\n",
    "    X_sample_scaled_full = None # Define outside try block\n",
    "    try:\n",
    "        if X_sample_full is None: # Check if sampling succeeded\n",
    "             raise ValueError(\"X_sample_full is None, sampling likely failed.\")\n",
    "\n",
    "        if np.any(~np.isfinite(X_sample_full)):\n",
    "             print(\"Warning: Non-finite values found in sample features. Replacing with 0.\")\n",
    "             X_sample_full = np.nan_to_num(X_sample_full, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        X_sample_scaled_full = scaler_full.transform(X_sample_full)\n",
    "        del X_sample_full\n",
    "        gc.collect()\n",
    "\n",
    "        # 3c. Apply L1 filtering to the SCALED sample data\n",
    "        print(\"Applying L1 feature filter to scaled sample data...\")\n",
    "        if selected_mask_l1 is None:\n",
    "             raise ValueError(\"L1 selection mask was not calculated.\")\n",
    "\n",
    "        X_sample_scaled_l1_filtered = X_sample_scaled_full[:, selected_mask_l1] # Assign here\n",
    "        del X_sample_scaled_full\n",
    "        gc.collect()\n",
    "        print(f\"Shape of data for MI: {X_sample_scaled_l1_filtered.shape}\")\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"Error scaling sample or applying L1 filter: {e}\")\n",
    "         traceback.print_exc()\n",
    "         exit(1)\n",
    "\n",
    "\n",
    "    # 3d. Calculate Mutual Information Scores and Select K Best\n",
    "    print(f\"Calculating Mutual Information scores and selecting top {MI_SELECT_K} features...\")\n",
    "    try:\n",
    "        # Check if input data for MI exists and has expected shape\n",
    "        if X_sample_scaled_l1_filtered is None or X_sample_scaled_l1_filtered.shape[1] != n_after_l1:\n",
    "             raise ValueError(\"Data for MI calculation (X_sample_scaled_l1_filtered) is missing or has incorrect shape.\")\n",
    "        if y_sample is None:\n",
    "            raise ValueError(\"Target data for MI (y_sample) is missing.\")\n",
    "\n",
    "        num_features_for_mi = X_sample_scaled_l1_filtered.shape[1]\n",
    "        k_features_mi = min(MI_SELECT_K, num_features_for_mi)\n",
    "        if k_features_mi < MI_SELECT_K:\n",
    "            print(f\"Warning: Requested K={MI_SELECT_K}, but only {k_features_mi} available after L1 selection. Selecting all {k_features_mi}.\")\n",
    "        if k_features_mi == 0:\n",
    "            print(\"Warning: No features left for MI selection.\")\n",
    "            features_after_l1_mi = []\n",
    "            n_after_mi = 0\n",
    "        else:\n",
    "            selector_mi = SelectKBest(lambda X, y: mutual_info_classif(X, y, discrete_features=False, random_state=RANDOM_STATE), k=k_features_mi)\n",
    "            selector_mi.fit(X_sample_scaled_l1_filtered, y_sample)\n",
    "\n",
    "            selected_mask_mi = selector_mi.get_support()\n",
    "\n",
    "            features_after_l1_mi = [\n",
    "                feature for feature, selected in zip(features_after_l1, selected_mask_mi) if selected\n",
    "            ]\n",
    "            n_after_mi = len(features_after_l1_mi)\n",
    "\n",
    "            print(\"Mutual Information selection complete.\")\n",
    "            print(f\"Final features selected: {n_after_mi} (removed {n_after_l1 - n_after_mi} based on MI)\")\n",
    "\n",
    "            if n_after_mi == 0 :\n",
    "                print(\"Warning: Mutual information selected 0 features from L1 subset.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Mutual Information calculation/selection: {e}\")\n",
    "        traceback.print_exc()\n",
    "        # Allow script to continue to summary\n",
    "\n",
    "else: # Case where L1 selected 0 features\n",
    "    print(\"L1 selected 0 features. Final feature list is empty.\")\n",
    "    features_after_l1_mi = []\n",
    "    n_after_mi = 0\n",
    "    k_features_mi = 0\n",
    "\n",
    "# --- Final Results and Saving ---\n",
    "print(\"\\n--- Feature Selection Summary ---\")\n",
    "print(f\"Initial features: {n_original}\")\n",
    "print(f\"After L1 Regularization (alpha={L1_ALPHA}): {n_after_l1}\")\n",
    "print(f\"After Mutual Information (Top {k_features_mi}): {n_after_mi}\")\n",
    "\n",
    "# --- Save Scaler (fitted on ALL features) and FINAL Feature List ---\n",
    "print(\"\\n--- Saving final outputs ---\")\n",
    "try:\n",
    "    if hasattr(scaler_full, 'n_features_in_'):\n",
    "        joblib.dump(scaler_full, OUTPUT_SCALER_MODEL)\n",
    "        print(f\"Saved scaler fitted on original features to: {OUTPUT_SCALER_MODEL}\")\n",
    "    else:\n",
    "         print(\"Scaler was not fitted. Not saving scaler.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving the scaler: {e}\")\n",
    "\n",
    "# Always save the feature list, even if empty, to avoid FileNotFoundError later\n",
    "try:\n",
    "    joblib.dump(features_after_l1_mi, OUTPUT_FINAL_FEATURE_LIST)\n",
    "    print(f\"Saved the final list of {n_after_mi} selected features to: {OUTPUT_FINAL_FEATURE_LIST}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving the final feature list: {e}\")\n",
    "\n",
    "print(\"\\n--- L1 -> MI Feature Selection Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading selected features from: final_selected_features_500.joblib\n",
      "Loaded 500 selected features.\n",
      "\n",
      "Loading data from: ML_dataset_500_features.parquet (Cols: 501)\n",
      "Loaded DataFrame shape: (2772, 501)\n",
      "Converted data to NumPy arrays.\n",
      "\n",
      "Encoding target variable...\n",
      "Target classes: ['Left' 'Right']\n",
      "Positive class 'Right' encoded as: 1\n",
      "Encoded target shape: (2772,)\n",
      "\n",
      "Splitting data into Training (75%) and Testing (25%)...\n",
      "X_train shape: (2079, 500), y_train shape: (2079,)\n",
      "X_test shape: (693, 500), y_test shape: (693,)\n",
      "\n",
      "Scaling features (fitting scaler on training data only)...\n",
      "Scaling complete.\n",
      "Saved fitted scaler to: trained_models_in_memory/scaler_in_memory.joblib\n",
      "\n",
      "Initializing models...\n",
      "\n",
      "--- Training, Evaluating Models, and Tuning Threshold ---\n",
      "\n",
      "--- Processing Model: LogisticRegression ---\n",
      "Training...\n",
      "Training complete.\n",
      "Evaluating with default threshold (0.5)...\n",
      "Tuning threshold...\n",
      "Best threshold found: 0.2971\n",
      "\n",
      "--- Evaluation Results: LogisticRegression ---\n",
      "Accuracy (Default Threshold 0.5): 0.5873\n",
      "Best Threshold Found            : 0.2971\n",
      "Accuracy (Best Threshold)       : 0.5974\n",
      "\n",
      "Classification Report (Default Threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.58      0.57      0.58       343\n",
      "       Right       0.59      0.60      0.59       350\n",
      "\n",
      "    accuracy                           0.59       693\n",
      "   macro avg       0.59      0.59      0.59       693\n",
      "weighted avg       0.59      0.59      0.59       693\n",
      "\n",
      "Confusion Matrix (Default Threshold):\n",
      "[[197 146]\n",
      " [140 210]]\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing Model: SGDClassifier_Log ---\n",
      "Training...\n",
      "Training complete.\n",
      "Evaluating with default threshold (0.5)...\n",
      "Tuning threshold...\n",
      "Best threshold found: 0.0397\n",
      "\n",
      "--- Evaluation Results: SGDClassifier_Log ---\n",
      "Accuracy (Default Threshold 0.5): 0.5426\n",
      "Best Threshold Found            : 0.0397\n",
      "Accuracy (Best Threshold)       : 0.5498\n",
      "\n",
      "Classification Report (Default Threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.54      0.50      0.52       343\n",
      "       Right       0.54      0.58      0.56       350\n",
      "\n",
      "    accuracy                           0.54       693\n",
      "   macro avg       0.54      0.54      0.54       693\n",
      "weighted avg       0.54      0.54      0.54       693\n",
      "\n",
      "Confusion Matrix (Default Threshold):\n",
      "[[173 170]\n",
      " [147 203]]\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing Model: SVC_Linear ---\n",
      "Training...\n",
      "Training complete.\n",
      "Evaluating with default threshold (0.5)...\n",
      "Tuning threshold...\n",
      "Best threshold found: 0.5059\n",
      "\n",
      "--- Evaluation Results: SVC_Linear ---\n",
      "Accuracy (Default Threshold 0.5): 0.5844\n",
      "Best Threshold Found            : 0.5059\n",
      "Accuracy (Best Threshold)       : 0.5873\n",
      "\n",
      "Classification Report (Default Threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.58      0.56      0.57       343\n",
      "       Right       0.59      0.61      0.60       350\n",
      "\n",
      "    accuracy                           0.58       693\n",
      "   macro avg       0.58      0.58      0.58       693\n",
      "weighted avg       0.58      0.58      0.58       693\n",
      "\n",
      "Confusion Matrix (Default Threshold):\n",
      "[[193 150]\n",
      " [138 212]]\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing Model: SVC_RBF ---\n",
      "Training...\n",
      "Training complete.\n",
      "Evaluating with default threshold (0.5)...\n",
      "Tuning threshold...\n",
      "Best threshold found: 0.5205\n",
      "\n",
      "--- Evaluation Results: SVC_RBF ---\n",
      "Accuracy (Default Threshold 0.5): 0.5628\n",
      "Best Threshold Found            : 0.5205\n",
      "Accuracy (Best Threshold)       : 0.5714\n",
      "\n",
      "Classification Report (Default Threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.56      0.54      0.55       343\n",
      "       Right       0.57      0.58      0.57       350\n",
      "\n",
      "    accuracy                           0.56       693\n",
      "   macro avg       0.56      0.56      0.56       693\n",
      "weighted avg       0.56      0.56      0.56       693\n",
      "\n",
      "Confusion Matrix (Default Threshold):\n",
      "[[186 157]\n",
      " [146 204]]\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing Model: RandomForest ---\n",
      "Training...\n",
      "Training complete.\n",
      "Evaluating with default threshold (0.5)...\n",
      "Tuning threshold...\n",
      "Best threshold found: 0.4771\n",
      "\n",
      "--- Evaluation Results: RandomForest ---\n",
      "Accuracy (Default Threshold 0.5): 0.5498\n",
      "Best Threshold Found            : 0.4771\n",
      "Accuracy (Best Threshold)       : 0.5642\n",
      "\n",
      "Classification Report (Default Threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.55      0.52      0.53       343\n",
      "       Right       0.55      0.58      0.56       350\n",
      "\n",
      "    accuracy                           0.55       693\n",
      "   macro avg       0.55      0.55      0.55       693\n",
      "weighted avg       0.55      0.55      0.55       693\n",
      "\n",
      "Confusion Matrix (Default Threshold):\n",
      "[[179 164]\n",
      " [148 202]]\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing Model: GradientBoosting ---\n",
      "Training...\n",
      "Training complete.\n",
      "Evaluating with default threshold (0.5)...\n",
      "Tuning threshold...\n",
      "Best threshold found: 0.4823\n",
      "\n",
      "--- Evaluation Results: GradientBoosting ---\n",
      "Accuracy (Default Threshold 0.5): 0.5512\n",
      "Best Threshold Found            : 0.4823\n",
      "Accuracy (Best Threshold)       : 0.5570\n",
      "\n",
      "Classification Report (Default Threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.55      0.55      0.55       343\n",
      "       Right       0.56      0.55      0.56       350\n",
      "\n",
      "    accuracy                           0.55       693\n",
      "   macro avg       0.55      0.55      0.55       693\n",
      "weighted avg       0.55      0.55      0.55       693\n",
      "\n",
      "Confusion Matrix (Default Threshold):\n",
      "[[188 155]\n",
      " [156 194]]\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing Model: KNeighbors ---\n",
      "Training...\n",
      "Training complete.\n",
      "Evaluating with default threshold (0.5)...\n",
      "Tuning threshold...\n",
      "Best threshold found: 0.5000\n",
      "\n",
      "--- Evaluation Results: KNeighbors ---\n",
      "Accuracy (Default Threshold 0.5): 0.5411\n",
      "Best Threshold Found            : 0.5000\n",
      "Accuracy (Best Threshold)       : 0.5411\n",
      "\n",
      "Classification Report (Default Threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.54      0.55      0.54       343\n",
      "       Right       0.55      0.54      0.54       350\n",
      "\n",
      "    accuracy                           0.54       693\n",
      "   macro avg       0.54      0.54      0.54       693\n",
      "weighted avg       0.54      0.54      0.54       693\n",
      "\n",
      "Confusion Matrix (Default Threshold):\n",
      "[[187 156]\n",
      " [162 188]]\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing Model: GaussianNB ---\n",
      "Training...\n",
      "Training complete.\n",
      "Evaluating with default threshold (0.5)...\n",
      "Tuning threshold...\n",
      "Best threshold found: 0.9802\n",
      "\n",
      "--- Evaluation Results: GaussianNB ---\n",
      "Accuracy (Default Threshold 0.5): 0.4949\n",
      "Best Threshold Found            : 0.9802\n",
      "Accuracy (Best Threshold)       : 0.5022\n",
      "\n",
      "Classification Report (Default Threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.49      0.85      0.63       343\n",
      "       Right       0.50      0.14      0.22       350\n",
      "\n",
      "    accuracy                           0.49       693\n",
      "   macro avg       0.50      0.50      0.42       693\n",
      "weighted avg       0.50      0.49      0.42       693\n",
      "\n",
      "Confusion Matrix (Default Threshold):\n",
      "[[293  50]\n",
      " [300  50]]\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing Model: DecisionTree ---\n",
      "Training...\n",
      "Training complete.\n",
      "Evaluating with default threshold (0.5)...\n",
      "Tuning threshold...\n",
      "Best threshold found: 0.0100\n",
      "\n",
      "--- Evaluation Results: DecisionTree ---\n",
      "Accuracy (Default Threshold 0.5): 0.5152\n",
      "Best Threshold Found            : 0.0100\n",
      "Accuracy (Best Threshold)       : 0.5382\n",
      "\n",
      "Classification Report (Default Threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.51      0.52      0.51       343\n",
      "       Right       0.52      0.51      0.52       350\n",
      "\n",
      "    accuracy                           0.52       693\n",
      "   macro avg       0.52      0.52      0.52       693\n",
      "weighted avg       0.52      0.52      0.52       693\n",
      "\n",
      "Confusion Matrix (Default Threshold):\n",
      "[[178 165]\n",
      " [171 179]]\n",
      "----------------------------------------\n",
      "\n",
      "--- Processing Model: MLPClassifier ---\n",
      "Training...\n",
      "Training complete.\n",
      "Evaluating with default threshold (0.5)...\n",
      "Tuning threshold...\n",
      "Best threshold found: 0.3375\n",
      "\n",
      "--- Evaluation Results: MLPClassifier ---\n",
      "Accuracy (Default Threshold 0.5): 0.5382\n",
      "Best Threshold Found            : 0.3375\n",
      "Accuracy (Best Threshold)       : 0.5469\n",
      "\n",
      "Classification Report (Default Threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.53      0.55      0.54       343\n",
      "       Right       0.54      0.53      0.53       350\n",
      "\n",
      "    accuracy                           0.54       693\n",
      "   macro avg       0.54      0.54      0.54       693\n",
      "weighted avg       0.54      0.54      0.54       693\n",
      "\n",
      "Confusion Matrix (Default Threshold):\n",
      "[[189 154]\n",
      " [166 184]]\n",
      "----------------------------------------\n",
      "\n",
      "--- Final Accuracy Summary ---\n",
      "Model                     | Acc (Default)   | Best Threshold  | Acc (Tuned)    \n",
      "---------------------------------------------------------------------------\n",
      "LogisticRegression        | 0.5873          | 0.2971          | 0.5974         \n",
      "SGDClassifier_Log         | 0.5426          | 0.0397          | 0.5498         \n",
      "SVC_Linear                | 0.5844          | 0.5059          | 0.5873         \n",
      "SVC_RBF                   | 0.5628          | 0.5205          | 0.5714         \n",
      "RandomForest              | 0.5498          | 0.4771          | 0.5642         \n",
      "GradientBoosting          | 0.5512          | 0.4823          | 0.5570         \n",
      "KNeighbors                | 0.5411          | 0.5000          | 0.5411         \n",
      "GaussianNB                | 0.4949          | 0.9802          | 0.5022         \n",
      "DecisionTree              | 0.5152          | 0.0100          | 0.5382         \n",
      "MLPClassifier             | 0.5382          | 0.3375          | 0.5469         \n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "import gc # Import gc for memory management\n",
    "import traceback # Import traceback for error details\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Import popular classifiers\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Suppress convergence warnings for models like Logistic Regression or MLP\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='sklearn')\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_FEATURES_FILE = \"ML_dataset_500_features.parquet\" # The dataset with ~4k features\n",
    "SELECTED_FEATURES_LIST_FILE = \"final_selected_features_500.joblib\" # List of ~500 feature names\n",
    "TARGET_COLUMN = \"marker\"\n",
    "IDENTIFIER_COLS = ['event_id', 'prev_marker'] # Columns to ignore for features/target\n",
    "\n",
    "TEST_SIZE = 0.25\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "OUTPUT_MODEL_DIR = \"trained_models_in_memory\"\n",
    "os.makedirs(OUTPUT_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# --- 1. Load Selected Features ---\n",
    "print(f\"Loading selected features from: {SELECTED_FEATURES_LIST_FILE}\")\n",
    "try:\n",
    "    selected_features = joblib.load(SELECTED_FEATURES_LIST_FILE)\n",
    "    if not isinstance(selected_features, list) or len(selected_features) == 0:\n",
    "        raise ValueError(\"Loaded features are not a valid non-empty list.\")\n",
    "    print(f\"Loaded {len(selected_features)} selected features.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Selected features file not found at {SELECTED_FEATURES_LIST_FILE}\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading selected features: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "columns_to_load = selected_features + [TARGET_COLUMN]\n",
    "\n",
    "# --- 2. Load Dataset into Memory ---\n",
    "print(f\"\\nLoading data from: {INPUT_FEATURES_FILE} (Cols: {len(columns_to_load)})\")\n",
    "try:\n",
    "    if not os.path.exists(INPUT_FEATURES_FILE):\n",
    "        raise FileNotFoundError(f\"Input file not found: {INPUT_FEATURES_FILE}\")\n",
    "    df_pl = pl.read_parquet(INPUT_FEATURES_FILE, columns=columns_to_load)\n",
    "    print(f\"Loaded DataFrame shape: {df_pl.shape}\")\n",
    "\n",
    "    X = df_pl.select(selected_features).to_numpy()\n",
    "    y_raw = df_pl.select(TARGET_COLUMN).to_numpy().ravel()\n",
    "    del df_pl\n",
    "    gc.collect()\n",
    "    print(\"Converted data to NumPy arrays.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "     print(f\"Error: Input file not found at {INPUT_FEATURES_FILE}\")\n",
    "     exit(1)\n",
    "except pl.exceptions.ColumnNotFoundError as e:\n",
    "     print(f\"Error: One or more selected columns not found in {INPUT_FEATURES_FILE}. Details: {e}\")\n",
    "     print(\"Ensure the feature list file corresponds to the columns in the Parquet file.\")\n",
    "     exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or processing data: {e}\")\n",
    "    traceback.print_exc()\n",
    "    exit(1)\n",
    "\n",
    "# --- 3. Handle Missing Values ---\n",
    "if np.any(~np.isfinite(X)):\n",
    "    print(\"Warning: Non-finite values (NaN/Inf) found in features. Replacing with 0.\")\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=np.finfo(X.dtype).max, neginf=np.finfo(X.dtype).min)\n",
    "\n",
    "# --- 4. Encode Target Variable ---\n",
    "print(\"\\nEncoding target variable...\")\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_raw)\n",
    "print(f\"Target classes: {label_encoder.classes_}\")\n",
    "# Find index of the 'positive' class (e.g., 'Right', assuming it's the second class)\n",
    "positive_class_label = 'Right' # Or 'Left', choose one consistently\n",
    "positive_class_index = np.where(label_encoder.classes_ == positive_class_label)[0][0]\n",
    "print(f\"Positive class '{positive_class_label}' encoded as: {positive_class_index}\")\n",
    "print(f\"Encoded target shape: {y.shape}\")\n",
    "\n",
    "# --- 5. Split Data into Train/Test ---\n",
    "print(f\"\\nSplitting data into Training ({1-TEST_SIZE:.0%}) and Testing ({TEST_SIZE:.0%})...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "del X, y_raw, y\n",
    "gc.collect()\n",
    "\n",
    "# --- 6. Scale Features ---\n",
    "print(\"\\nScaling features (fitting scaler on training data only)...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Scaling complete.\")\n",
    "scaler_path = os.path.join(OUTPUT_MODEL_DIR, \"scaler_in_memory.joblib\")\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Saved fitted scaler to: {scaler_path}\")\n",
    "\n",
    "# --- 7. Initialize Models ---\n",
    "print(\"\\nInitializing models...\")\n",
    "# Ensure models that need probability=True have it set\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(random_state=RANDOM_STATE, max_iter=1000, solver='liblinear'),\n",
    "    \"SGDClassifier_Log\": SGDClassifier(loss='log_loss', random_state=RANDOM_STATE, max_iter=1000, tol=1e-3), # Supports predict_proba\n",
    "    # SGDClassifier_Hinge does NOT support predict_proba directly\n",
    "    \"SVC_Linear\": SVC(kernel='linear', probability=True, random_state=RANDOM_STATE), # Needs probability=True\n",
    "    \"SVC_RBF\": SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE),       # Needs probability=True\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=150, random_state=RANDOM_STATE, n_jobs=-1, max_depth=20, min_samples_leaf=5),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE, learning_rate=0.1, max_depth=3), # Supports predict_proba\n",
    "    \"KNeighbors\": KNeighborsClassifier(n_neighbors=5, n_jobs=-1), # Supports predict_proba\n",
    "    \"GaussianNB\": GaussianNB(), # Supports predict_proba\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=15, min_samples_leaf=10), # Supports predict_proba\n",
    "    \"MLPClassifier\": MLPClassifier(hidden_layer_sizes=(64, 32), random_state=RANDOM_STATE, max_iter=500, early_stopping=True) # Supports predict_proba\n",
    "}\n",
    "\n",
    "# --- Helper Function for Threshold Tuning ---\n",
    "def find_best_threshold(y_true, y_pred_proba, pos_label_index=1, steps=100):\n",
    "    \"\"\"Finds the probability threshold that maximizes accuracy.\"\"\"\n",
    "    best_threshold = 0.5 # Default\n",
    "    best_accuracy = 0.0\n",
    "    min_prob = max(np.min(y_pred_proba), 0.01)\n",
    "    max_prob = np.max(y_pred_proba)\n",
    "    \n",
    "    # Calculate initial accuracy with default threshold\n",
    "    y_pred_default = (y_pred_proba >= 0.5).astype(int)\n",
    "    best_accuracy = accuracy_score(y_true, y_pred_default) # Start with default accuracy\n",
    "\n",
    "    # Iterate through thresholds\n",
    "    thresholds = np.linspace(min_prob, max_prob, steps + 1) # Check edge cases too\n",
    "    for threshold in thresholds:\n",
    "        y_pred_tuned = (y_pred_proba >= threshold).astype(int)\n",
    "        current_accuracy = accuracy_score(y_true, y_pred_tuned)\n",
    "        if current_accuracy > best_accuracy:\n",
    "            best_accuracy = current_accuracy\n",
    "            best_threshold = threshold\n",
    "            \n",
    "    return best_threshold, best_accuracy\n",
    "\n",
    "# --- 8. Train, Evaluate, and Tune Threshold ---\n",
    "print(\"\\n--- Training, Evaluating Models, and Tuning Threshold ---\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- Processing Model: {name} ---\")\n",
    "    try:\n",
    "        # Train the model\n",
    "        print(\"Training...\")\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        print(\"Training complete.\")\n",
    "\n",
    "        # --- Original Evaluation (Default Threshold) ---\n",
    "        print(\"Evaluating with default threshold (0.5)...\")\n",
    "        y_pred_original = model.predict(X_test_scaled)\n",
    "        accuracy_original = accuracy_score(y_test, y_pred_original)\n",
    "        report_original = classification_report(y_test, y_pred_original, target_names=label_encoder.classes_, zero_division=0)\n",
    "        cm_original = confusion_matrix(y_test, y_pred_original)\n",
    "\n",
    "        # --- Threshold Tuning ---\n",
    "        best_threshold = 0.5 # Default\n",
    "        accuracy_tuned = accuracy_original # Start with original\n",
    "\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            print(\"Tuning threshold...\")\n",
    "            try:\n",
    "                # Get probabilities for the positive class\n",
    "                y_pred_proba = model.predict_proba(X_test_scaled)[:, positive_class_index]\n",
    "                best_threshold, accuracy_tuned = find_best_threshold(y_test, y_pred_proba, positive_class_index)\n",
    "                print(f\"Best threshold found: {best_threshold:.4f}\")\n",
    "            except Exception as te:\n",
    "                print(f\"Could not tune threshold for {name}: {te}\")\n",
    "                best_threshold = 'N/A'\n",
    "                accuracy_tuned = accuracy_original # Fallback to original if tuning fails\n",
    "        else:\n",
    "            print(f\"Model {name} does not support predict_proba. Skipping threshold tuning.\")\n",
    "            best_threshold = 'N/A'\n",
    "            accuracy_tuned = accuracy_original\n",
    "\n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'accuracy_original': accuracy_original,\n",
    "            'accuracy_tuned': accuracy_tuned,\n",
    "            'best_threshold': best_threshold,\n",
    "            'report_original': report_original, # Report based on default threshold\n",
    "            'cm_original': cm_original\n",
    "        }\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"\\n--- Evaluation Results: {name} ---\")\n",
    "        print(f\"Accuracy (Default Threshold 0.5): {accuracy_original:.4f}\")\n",
    "        if best_threshold != 'N/A':\n",
    "            print(f\"Best Threshold Found            : {best_threshold:.4f}\")\n",
    "            print(f\"Accuracy (Best Threshold)       : {accuracy_tuned:.4f}\")\n",
    "        print(\"\\nClassification Report (Default Threshold):\")\n",
    "        print(report_original)\n",
    "        print(\"Confusion Matrix (Default Threshold):\")\n",
    "        print(cm_original)\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Save the trained model\n",
    "        model_path = os.path.join(OUTPUT_MODEL_DIR, f\"{name}.joblib\")\n",
    "        joblib.dump(model, model_path)\n",
    "        # print(f\"Saved trained model to: {model_path}\") # Optional: uncomment if needed\n",
    "\n",
    "    except MemoryError:\n",
    "         print(f\"MemoryError occurred while processing {name}.\")\n",
    "         results[name] = {'accuracy_original': 'MemoryError', 'accuracy_tuned': 'MemoryError', 'best_threshold': 'MemoryError', 'report_original': 'MemoryError', 'cm_original': 'MemoryError'}\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {name}: {e}\")\n",
    "        results[name] = {'accuracy_original': 'Error', 'accuracy_tuned': 'Error', 'best_threshold': 'Error', 'report_original': str(e), 'cm_original': 'Error'}\n",
    "        traceback.print_exc()\n",
    "\n",
    "# --- 9. Final Summary ---\n",
    "print(\"\\n--- Final Accuracy Summary ---\")\n",
    "print(f\"{'Model':<25} | {'Acc (Default)':<15} | {'Best Threshold':<15} | {'Acc (Tuned)':<15}\")\n",
    "print(\"-\" * 75)\n",
    "for name, metrics in results.items():\n",
    "    acc_orig_str = f\"{metrics['accuracy_original']:.4f}\" if isinstance(metrics['accuracy_original'], float) else str(metrics['accuracy_original'])\n",
    "    acc_tuned_str = f\"{metrics['accuracy_tuned']:.4f}\" if isinstance(metrics['accuracy_tuned'], float) else str(metrics['accuracy_tuned'])\n",
    "    thresh_str = f\"{metrics['best_threshold']:.4f}\" if isinstance(metrics['best_threshold'], float) else str(metrics['best_threshold'])\n",
    "\n",
    "    print(f\"{name:<25} | {acc_orig_str:<15} | {thresh_str:<15} | {acc_tuned_str:<15}\")\n",
    "print(\"-\" * 75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
